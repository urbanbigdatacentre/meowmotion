{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> MeowMotion Detecting Trips, OD Matrices, and Transport Mode from GPS Data </p> <p> </p>"},{"location":"#welcome-to-meowmotion","title":"Welcome to MeowMotion \ud83d\udc3e","text":"<p>MeowMotion is a Python package designed for processing and analyzing geolocation data to detect trips and infer transport modes. Originally developed at the Urban Big Data Centre (UBDC), MeowMotion provides a complete pipeline from raw GPS data ingestion and preprocessing to machine learning-based travel mode detection. In addition to trip detection and classification, MeowMotion now includes advanced functionality for trip scaling and OD matrix generation. By applying demographic-based and activity-based weighting, it can scale trip data to better represent population-wide mobility. The package produces five types of Origin-Destination (OD) matrices, including peak period flows and residual non-peak movement, making it especially useful for transport modeling, demand analysis, and policy evaluation. Whether you're working with mobile phone app data or other location sources, MeowMotion helps you clean, structure, and analyze the movement patterns of individuals, offering both granular trip-level outputs and high-level aggregated summaries. It's an ideal tool for researchers, analysts, and developers interested in urban mobility, transportation planning, or smart city applications.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p>Data Ingestion   Reads raw geolocation data from supported formats.</p> </li> <li> <p>Filtering and Cleaning   Applies preprocessing to remove noise and prepare the data for analysis.</p> </li> <li> <p>Stay Point Detection   Identifies and saves user stay points based on spatial and temporal thresholds.</p> </li> <li> <p>Trip Generation   Creates trip data using detected stay points.</p> </li> <li> <p>Trip Scaling &amp; OD Matrix Generation Reads generated trips and applies:</p> </li> <li>Demographic-based user weighting using external population/sample profiles.</li> <li>Novel activity-based weighting based on users' activity in the data.</li> </ul> <p>Scales trips accordingly and produces four different types of Origin-Destination (OD) matrices, offering robust representations of mobility flows.</p> <pre><code>- Type 1: AM peak weekdays (7 AM \u2013 10 AM)\n- Type 2: PM peak weekdays (4 PM \u2013 7 PM)\n- Type 3: All-day / All trips\n- Type 4: Type 3 minus (Type 1 + Type 2), i.e. non-peak OD flows\n</code></pre> <p>In addition to OD matrices, the process also outputs:</p> <pre><code>- `trip_points.csv`: Raw GPS trajectories for each trip (with placeholder mode).\n- `non_agg_stay_points.csv`: All GPS points within detected stay clusters.\n- `na_flows.csv`: Enhanced trip flows with user-level context (trips per active day, etc.).\n- `agg_stay_points.csv`: Abstracted stay location data linked to geographic zones.\n</code></pre> <ul> <li> <p>Trajectory Processing   Analyzes raw and trip data to produce:</p> <ul> <li>Granular trip data with trajectory points.</li> <li>Aggregated trip statistics.</li> </ul> </li> <li> <p>Feature Engineering   Performs data engineering on aggregated trip statistics to prepare inputs for machine learning models.</p> </li> <li> <p>Machine Learning for Travel Mode Detection</p> <ul> <li>Trains two built-in ML models: Decision Tree and Random Forest.</li> <li>Provides functionality to save (pickle) trained models.</li> <li>Includes a prediction endpoint to load models and classify the travel mode of unseen trip data.</li> </ul> </li> <li> <p>Custom Model Training Support   While MeowMotion includes built-in models, it also supports retraining with custom datasets.</p> </li> <li> <p>Prediction Output   Generates two output datasets:</p> <ul> <li>Non-aggregated trip data with origin geo code, destination geo code, and predicted travel mode for each trip.</li> <li>Aggregated summary showing trip counts between each origin-destination pair, split by transport modes (Walk, Bicycle, Car, Bus, Train, Metro).</li> </ul> </li> </ul>"},{"location":"#citation","title":"\ud83d\udcd1 Citation","text":"<p>If you use MeowMotion in your research, please cite: <pre><code>@software{MeowMotion,\n  author       = {Faraz M. Awan},\n  title        = {{MeowMotion: Detecting Trips, OD Matrices, and Transport Modes from GPS Data}},\n  affiliation  = {Urban Big Data Centre, University of Glasgow},\n  publisher    = {Zenodo},\n  year         = {2024},\n  doi          = {10.5281/zenodo.15311740},\n  url          = {https://doi.org/10.5281/zenodo.15311740}\n}\n</code></pre></p>"},{"location":"about/","title":"About","text":""},{"location":"about/#about","title":"About","text":"<p>MeowMotion is a Python package developed to process geolocation data from sources such as mobile phone apps. Its primary goal is to detect trips and identify transport modes using raw location data.</p>"},{"location":"about/#development-and-funding","title":"\ud83e\uddea Development and Funding","text":"<p>MeowMotion was developed by Dr. Faraz Awan at the Urban Big Data Centre (UBDC), a research centre dedicated to advancing the use of data and technology to address urban challenges.</p> <p>UBDC is jointly funded by the Economic and Social Research Council (ESRC) and the University of Glasgow, under the following ESRC grants:</p> <ul> <li>ES/L011921/1 \u2013 Urban Big Data Centre</li> <li>ES/S007105/1 \u2013 Urban Big Data Centre</li> </ul> <p>The centre\u2019s mission is to improve social, economic, and environmental well-being in cities by promoting the accessibility and responsible use of data.</p> <p>The development of this package was supported as part of UBDC's broader goal to enable research and innovation through urban data science.</p>"},{"location":"about/#feedback-support","title":"\ud83d\udcec Feedback &amp; Support","text":"<p>For questions, suggestions, or collaborations, feel free to reach out via Issues or contact:</p> <p>\ud83d\udce7 ubdc-dataservice@glasgow.ac.uk</p> <p>\ud83d\udcc4 License This project is licensed under the MIT License.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#installation-guide","title":"\ud83d\udce6 Installation Guide","text":"<p>Welcome to MeowMotion, a Python package for detecting trips and transport modes from GPS data, built with \u2764\ufe0f and structured using Poetry.</p>"},{"location":"getting-started/installation/#prerequisites","title":"\ud83d\udccc Prerequisites","text":"<ul> <li>Python 3.11 is recommended for best compatibility.</li> <li>Poetry (optional, for source installs):  </li> </ul> <pre><code>pip install poetry\n</code></pre>"},{"location":"getting-started/installation/#install-via-pypi-recommended","title":"\ud83c\udf89 Install via PyPI (Recommended)","text":"<p>The easiest way to get started is to install directly from PyPI:</p> <pre><code>poetry new project_name\ncd project_name\npoetry add meowmotion\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"\ud83d\ude80 Install from Source","text":"<p>If you prefer to run the latest code or contribute:</p> <pre><code>git clone https://github.com/faraz-m-awan/meowmotion.git\ncd meowmotion\npoetry install\n</code></pre>"},{"location":"getting-started/installation/#facing-issues-with-other-python-versions","title":"\u26a0\ufe0f Facing Issues with Other Python Versions?","text":"<p>If you're not using Python 3.11 and encounter errors while installing dependencies (especially related to compiled packages or lock file constraints): 1. Delete the existing lock file: <pre><code>rm poetry.lock\n</code></pre> 2. Install dependencies using uv (a fast alternative to pip): <pre><code>uv pip install -r pyproject.toml\n</code></pre> This can help resolve compatibility issues with newer or older Python versions.</p>"},{"location":"getting-started/installation/#troubleshooting-build-errors-eg-gdal-igraph-c-extensions","title":"\ud83d\udee0\ufe0f Troubleshooting Build Errors (e.g., GDAL, igraph, C extensions)","text":"<p>Some dependencies in the geospatial stack require native libraries. If you hit errors related to GDAL, PROJ, or other C extensions, follow the platform-specific steps below.</p>"},{"location":"getting-started/installation/#for-windows","title":"\ud83e\ude9f For Windows:","text":"<ol> <li>Install GDAL (if not already installed by QGIS/OSGeo4W):    <pre><code>choco install gdal -y\nrefreshenv\n</code></pre></li> <li>Install CMake and build tools:</li> <li>CMake (via cmake.org or Chocolatey):    <pre><code>choco install cmake -y\n</code></pre></li> <li>Microsoft C++ Build Tools:</li> <li>Download from: https://visualstudio.microsoft.com/visual-cpp-build-tools/</li> <li>Include C++ build tools in setup</li> </ol> <p>This ensures packages like python-igraph, fiona, or rasterio can compile or find pre-built wheels.</p>"},{"location":"getting-started/installation/#for-linux","title":"\ud83d\udc27 For Linux:","text":"<p>Install system dependencies before installing Python packages: <pre><code>   sudo apt update\n   sudo apt install build-essential cmake gdal-bin libgdal-dev libxml2-dev libglpk-dev libigraph-dev\n</code></pre></p>"},{"location":"getting-started/installation/#youre-ready","title":"\u2705 You're Ready!","text":"<p>You're all set to use MeowMotion! \ud83c\udf89 You can now import MeowMotion and dive into the Quick Start Guide or explore the modules in the meowmotion/ directory.  Happy analyzing! \ud83d\udc3e</p>"},{"location":"getting-started/quick-start/","title":"\ud83d\ude80 Quick Start","text":"<p>Welcome to MeowMotion! This quick start guide walks you through detecting trips and predicting transport modes using sample GPS data, all in just a few lines of code.</p> <p>\u26a0\ufe0f Make sure you've followed the Installation Guide before starting.</p>"},{"location":"getting-started/quick-start/#step-1-prepare-your-data","title":"\ud83d\udcc2 Step 1: Prepare Your Data","text":"<p>Ensure you have a GPS data file (e.g., <code>sample_gps_data.csv</code>) with the following minimum columns:</p> Column Description uid Unique identifier for each user datetime UTC timestamp of the GPS point lat Latitude lng Longitude impression_acc GPS point accuracy in meters <p>\ud83d\udccc Example snippet (Microsoft Research Asia's Geolife GPS Trajectory Dataset):</p> <pre><code>uid,datetime,lat,lng,impression_acc\n000,2008-10-23 02:53:04,39.984702,116.318417,99\n000,2008-10-23 02:53:10,39.984683,116.31845,5\n000,2008-10-23 02:53:15,39.984686,116.318417,99\n000,2008-10-23 02:53:20,39.984688,116.318385,99\n000,2008-10-23 02:53:25,39.984655,116.318263,99\n000,2008-10-23 02:53:30,39.984611,116.318026,5\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-filter-the-data","title":"\ud83e\uddf9 Step 2: Filter the Data","text":"<p><pre><code>from meowmotion.process_data import getFilteredData\n\nraw_df = readData() # Reading raw GPS data\nimpr_acc = 100 # Setting impression accuracy (GPS accuracy) to at least 100m\ncpu_cores = 12 # Using 12 cores of processor\n\n# Filter based on impression accuracy and speed\nraw_df_filtered = getFilteredData(\n    raw_df,\n    impr_acc=impr_acc,\n    cpu_cores=cpu_cores\n)\n</code></pre> This step removes noisy and low-quality points to prepare the data for stop detection.</p>"},{"location":"getting-started/quick-start/#step-3-detect-stop-nodes","title":"\ud83d\uded1 Step 3: Detect Stop Nodes","text":"<p><pre><code>from meowmotion.meowmob import getStopNodes\nfrom meowmotion.process_data import saveFile\n\noutput_dir = 'path/to/output/directory'\n# Detect significant stop locations\nstdf = getStopNodes(\n    tdf=raw_df_filtered,\n    time_th=time_th,\n    radius=radius,\n    cpu_cores=cpu_cores\n)\n\n# Save to disk\nsaveFile(output_dir, 'stop_nodes.csv', stdf)\n</code></pre> This step identifies user stop locations based on temporal and spatial clustering of GPS points.</p>"},{"location":"getting-started/quick-start/#output-stop_nodescsv-schema","title":"\ud83d\udccb Output: <code>stop_nodes.csv</code> Schema","text":"<p>The output file contains one row per detected stay (stop) location. Each row includes:</p> Column Description <code>uid</code> Unique identifier for the user <code>org_lng</code> Longitude of the centroid of the detected stay location <code>org_lat</code> Latitude of the centroid of the detected stay location <code>datetime</code> Arrival time, when the user first arrived at the stay location <code>leaving_datetime</code> Departure time, when the user left the stay location <p>\u2705 These stop locations are later used to generate trip flows between consecutive stops.</p>"},{"location":"getting-started/quick-start/#step-4-generate-trips-from-stop-nodes","title":"\ud83e\udded Step 4: Generate Trips from Stop Nodes","text":"<pre><code>from meowmotion.meowmob import processFlowGenration\n\n# Create trips between stop nodes\ntrip_df = processFlowGenration(\n    stdf=stdf,\n    raw_df=raw_df_filtered,\n    cpu_cores=cpu_cores\n)\n\n# Save trip data\nsaveFile(output_dir, 'trip_data.csv', trip_df)\n</code></pre>"},{"location":"getting-started/quick-start/#output-trip_datacsv-schema","title":"\ud83d\udccb Output: <code>trip_data.csv</code> Schema","text":"<p>The output file contains one row per detected trip between two stay locations. Each row includes:</p> Column Description <code>uid</code> Unique identifier for the user <code>org_lat</code> Latitude of the origin stay location centroid <code>org_lng</code> Longitude of the origin stay location centroid <code>org_arival_time</code> Time when the user arrived at the origin stay location <code>org_leaving_time</code> Time when the user left the origin stay location <code>dest_lat</code> Latitude of the destination stay location centroid <code>dest_lng</code> Longitude of the destination stay location centroid <code>dest_arival_time</code> Time when the user arrived at the destination stay location <code>stay_points</code> All GPS points within the origin stay location cluster <code>trip_points</code> Trajectory points generated during the trip between two stay locations <code>trip_time</code> Total duration of the trip <code>stay_duration</code> Duration the user stayed at the origin location (detected using scikit-mobility) <code>observed_stay_duration</code> Duration inferred based on GPS points within the stay location <p>\ud83e\udded These trips are the basis for later mode classification and OD matrix generation.</p>"},{"location":"getting-started/quick-start/#step-5-calculate-activity-statistics","title":"\ud83d\udcca Step 5: Calculate Activity Statistics","text":"<pre><code>from meowmotion.meowmob import getActivityStats\n\n# Compute user activity summary\nactivity_df = getActivityStats(\n    df=raw_df,\n    output_dir=output_dir,\n    cpu_cores=cpu_cores\n)\n\n# Save to disk\nsaveFile(output_dir, 'activity_stats.csv', activity_df)\n</code></pre>"},{"location":"getting-started/quick-start/#output-activity_statscsv-schema","title":"\ud83d\udccb Output: <code>activity_stats.csv</code> Schema","text":"<p>The output file contains activity statistics per user, aggregated by month. Each row includes:</p> Column Description <code>uid</code> Unique identifier for the user <code>month</code> Month in <code>YYYY-MM</code> format <code>total_active_days</code> Total number of days the user was observed active in that month <p>\ud83d\udcca This information is later used to weight users' trip contributions when generating OD matrices.</p>"},{"location":"getting-started/quick-start/#step-6-generate-od-matrices","title":"\ud83d\uddfa\ufe0f Step 6: Generate OD Matrices","text":"<p><pre><code>from meowmotion.meowmob import generateOD\nimport geopandas as gpd\nimport pandas as pd\n\n# Load supporting data\ncity_shape = gpd.read_file(city_shape_file_path) # Shapefile of the city\nhldf = pd.read_csv(hl_file_path) # Detected home location data of the users in the data\nadult_population_df = pd.read_csv(adult_population_file_path) # Adult population of the city W.R.T. to IMD\n\n# Generate 4 types of OD matrices with scaling\ngenerateOD(\n    trip_df=trip_df,\n    shape=city_shape,\n    active_day_df=activity_df,\n    hldf=hldf,\n    adult_population=adult_population_df,\n    output_dir=output_dir,\n    cpu_cores=cpu_cores,\n)\n</code></pre> This produces four types of OD matrices using demographic and activity-based weights:</p> <ul> <li>Type 1: AM peak (7\u201310am)</li> <li>Type 2: PM peak (4\u20137pm)</li> <li>Type 3: All-day</li> <li>Type 4: Non-peak (Type 3 \u2212 Type 1 &amp; 2)</li> </ul>"},{"location":"getting-started/quick-start/#output-od_matrix_type_xcsv-schema","title":"\ud83d\udccb Output: <code>od_matrix_type_X.csv</code> Schema","text":"<p>The output file contains Origin-Destination (OD) pairs with associated trip counts and scaled values. Each row represents a unique OD pair for a given time window (e.g., AM peak, PM peak, etc.).</p> Column Description <code>origin_geo_code</code> Geographic code of the origin area (e.g., data zone, LSOA, MSOA) <code>destination_geo_code</code> Geographic code of the destination area <code>trips</code> Number of detected trips in the raw GPS data <code>trips_weighted</code> Trips scaled using both activity-based and council-level weights <code>percentage</code> Share of trips for this OD pair relative to all trips in the region <p>\ud83d\udccc Multiple OD matrix files are generated (AM, PM, all-day, non-peak), each following this schema.</p>"},{"location":"getting-started/quick-start/#additional-outputs-from-generateod","title":"\ud83d\udce6 Additional Outputs from <code>generateOD()</code>","text":"<p>In addition to OD matrices, the <code>generateOD()</code> function produces the following five datasets by default:</p>"},{"location":"getting-started/quick-start/#1-trip_pointscsv","title":"1. <code>trip_points.csv</code>","text":"<p>This file contains detailed trajectory points for each detected trip and includes:</p> Column Description <code>uid</code> Unique identifier for the user <code>imd_quintile</code> IMD quintile of the user's home location <code>trip_id</code> Unique identifier for the trip <code>trip_points</code> List of GPS points forming the trajectory between origin and destination <code>total_active_days</code> Number of days the user was active in the dataset <code>travel_mode</code> Placeholder column (mode not yet detected at this stage)"},{"location":"getting-started/quick-start/#2-non_agg_stay_pointscsv","title":"2. <code>non_agg_stay_points.csv</code>","text":"<p>This file lists all GPS points within the detected stay location clusters for each user:</p> Column Description <code>uid</code> Unique identifier for the user <code>imd_quintile</code> IMD quintile of the user's home location <code>stay_points</code> List of GPS points within the stay location cluster <code>stop_node_arival_time</code> Time when the user arrived at the stay location <code>stop_node_leaving_time</code> Time when the user left the stay location <code>stay_duration</code> Duration of stay at the location <code>centroid_lat</code> Latitude of the stay location centroid <code>centroid_lng</code> Longitude of the stay location centroid <code>total_active_days</code> Number of active days for the user"},{"location":"getting-started/quick-start/#3-na_flowscsv","title":"3. <code>na_flows.csv</code>","text":"<p>Unlike the trip flows from Step 4, this dataset includes additional user-level attributes:</p> Column Description <code>uid</code> Unique identifier for the user <code>imd_quintile</code> IMD quintile of the user's home location <code>trip_id</code> Unique trip ID <code>org_lat</code> Latitude of the origin stay location <code>org_lng</code> Longitude of the origin stay location <code>org_arival_time</code> Time of arrival at the origin stay location <code>org_leaving_time</code> Time of departure from the origin stay location <code>dest_lat</code> Latitude of the destination stay location <code>dest_lng</code> Longitude of the destination stay location <code>dest_arival_time</code> Time of arrival at the destination stay location <code>total_trips</code> Total number of trips detected for the user <code>total_active_days</code> Number of active days for the user <code>tpad</code> Trips per active day (<code>total_trips / total_active_days</code>) <code>travel_mode</code> Placeholder column (mode not yet detected)"},{"location":"getting-started/quick-start/#4-agg_stay_pointscsv","title":"4. <code>agg_stay_points.csv</code>","text":"<p>This abstracted version of stay point data assigns each detected stop to a geographic zone (<code>geo_code</code>), making it less sensitive:</p> Column Description <code>imd_quintile</code> IMD quintile of the user's home location <code>stop_node_geo_code</code> Geographic zone code where the stop was detected <code>stop_node_arival_time</code> Time of arrival at the stay location <code>stop_node_leaving_time</code> Time of departure from the stay location <code>stay_duration</code> Duration of stay at the location"},{"location":"getting-started/quick-start/#notes-on-required-input-files","title":"\ud83d\udccc Notes on Required Input Files","text":""},{"location":"getting-started/quick-start/#1-shapefile","title":"\ud83e\udded 1. Shapefile","text":"<p>The shapefile must include the following mandatory columns:</p> Column Description geo_code Unique identifier for each geographic area (e.g., LSOA, MSOA, data zone) name Human-readable name for the geographic area geometry Polygon geometry representing the spatial boundary <p>\ud83d\udccc The coordinate reference system (CRS) must be EPSG:4326 (WGS84).</p> <p>This shapefile defines the spatial resolution for OD matrix generation. You can choose to calculate OD matrices at different geographic levels, including:</p> <ul> <li>Local level: data zones, LSOA</li> <li>Intermediate level: MSOA, intermediate zones</li> <li>Regional level: councils, municipalities</li> </ul> <p>\ud83d\uddc2\ufe0f Sample Shapefile Preview</p> geo_code name geometry (EPSG:4326) S02001902 Garrowhill West POLYGON ((-4.11936 55.85619, ...)) S02001903 Garrowhill East and Swinton POLYGON ((-4.09793 55.85989, ...)) S02001908 Barlanark POLYGON ((-4.13333 55.86491, ...)) S02001907 North Barlanark and Easterhouse South POLYGON ((-4.11959 55.86862, ...)) S02001927 Dennistoun North POLYGON ((-4.21574 55.86692, ...)) <p>\u2705 Ensure geometries are valid and CRS is correctly set to EPSG:4326 (WGS84) for spatial operations to succeed.</p>"},{"location":"getting-started/quick-start/#2-home-location-file","title":"\ud83c\udfe0 2. Home Location File","text":"<p>The Home Location file contains information about the detected home locations of users in the GPS dataset. These locations are identified using a novel home detection method that combines:</p> <ul> <li>Active evening presence thresholds, and</li> <li>UK residential building data</li> </ul> <p>This hybrid approach yields more accurate home location detection compared to traditional methods that rely solely on evening activity.</p> <p>\u2139\ufe0f Note: The current version of MeowMotion does not generate this file. You can request the home location dataset from the UBDC Data Service:</p> <p>\ud83d\udce7 <code>ubdc-dataservice@glasgow.ac.uk</code></p>"},{"location":"getting-started/quick-start/#required-columns-in-the-home-location-file","title":"\ud83d\uddc2\ufe0f Required Columns in the Home Location File","text":"Column Description <code>uid</code> Unique identifier for the user <code>home_datazone</code> / <code>lsoa</code> The data zone or LSOA where the user's home is located <code>msoa</code> / <code>intzone_code</code> MSOA or intermediate zone code <code>msoa_name</code> Name of the MSOA or intermediate zone <code>council_code</code> Unique code for the local authority or council area <code>council_name</code> Name of the local authority or council <code>imd_quintile</code> Index of Multiple Deprivation quintile (1 = most deprived, 5 = least) <p>\u2705 Ensure that the <code>uid</code> column matches the user IDs in your GPS dataset for consistent joining.</p>"},{"location":"getting-started/quick-start/#sample-home-location-data","title":"\ud83d\udccb Sample Home Location Data","text":"uid home_datazone/lsoa msoa/intzone_code msoa/intzone_name council_code council_name imd_quintile 0 001 S01009758 S02001842 Darnley East S12000046 Glasgow City 1 002 S01009758 S02001842 Darnley East S12000046 Glasgow City 2 003 S01009758 S02001842 Darnley East S12000046 Glasgow City 3 004 S01009758 S02001842 Darnley East S12000046 Glasgow City 4 005 S01009759 S02001842 Darnley East S12000046 Glasgow City"},{"location":"getting-started/quick-start/#3-adult-population-file","title":"\ud83e\uddee 3. Adult Population File","text":"<p>The Adult Population file contains information about the total number of adults in each Index of Multiple Deprivation (IMD) quintile within a given council area. The proportional share of each quintile can be calculated as a percentage of the total population within the corresponding city or region.</p> <p>This data is publicly available from:</p> <ul> <li>Office of National Statistics (ONS)</li> <li>National Records of Scotland (NRS)</li> </ul>"},{"location":"getting-started/quick-start/#required-columns-in-the-adult-population-file","title":"\ud83d\uddc2\ufe0f Required Columns in the Adult Population File","text":"Column Description <code>council</code> Name of the local authority or council area <code>imd_quintile</code> IMD quintile (1 = most deprived, 5 = least deprived) <code>Total</code> Total adult population in that IMD quintile within the council <code>Percentage</code> Proportion of total population this IMD quintile represents (e.g., 0.43 = 43%) <p>\u2705 Ensure the <code>council</code> values match those in the Home Location file for accurate merging.</p>"},{"location":"getting-started/quick-start/#sample-adult-population-data","title":"\ud83d\udccb Sample Adult Population Data","text":"council imd_quintile Total Percentage Glasgow City 1 229597 0.43 Glasgow City 2 93635 0.17 Glasgow City 3 73942 0.14 Glasgow City 4 67347 0.13 Glasgow City 5 70641 0.13"},{"location":"getting-started/quick-start/#quick-start-travel-mode-detection","title":"\ud83d\udea6 Quick Start: Travel Mode Detection","text":"<p>MeowMotion supports machine learning\u2013based travel mode detection using features derived from GPS traces, public transport infrastructure, and movement patterns.</p>"},{"location":"getting-started/quick-start/#step-1-prepare-the-trip-data","title":"\ud83e\uddf1 Step 1: Prepare the Trip Data","text":"<pre><code>from meowmotion.data_formatter import processTripData\nfrom datetime import datetime\nimport pandas as pd\n\nprint(f\"{datetime.now()}: Reading raw data\")\nraw_df = readData() # This is the raw GPS data. Read it the way you are most comfortable with\n\nprint(f\"{datetime.now()}: Reading Trip Point Data\")\ntp_df = pd.read_csv(trip_point_data_file) # Trip points data generated by 'generateOD() above.'\n\nprint(f\"{datetime.now()}: Reading NA-flow Data\")\nnaf_df = pd.read_csv(na_flow_data_file) # na_flows data generated by 'generateOD() above.'\n\n# Format the data for modeling\ntrip_df = processTripData(trip_point_df=tp_df, na_flow_df=naf_df, raw_df=raw_df)\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-feature-engineering","title":"\ud83e\udde0 Step 2: Feature Engineering","text":"<pre><code>from meowmotion.data_formatter import featureEngineering\nimport geopandas as gpd\n\n# Read shape files\nbus_stops = gpd.read_file('path/to/bus_stops/shape_file.shp')\ntrain_stops = gpd.read_file('path/to/train_stations/shape_file.shp')\nmetro_stops = gpd.read_file('path/to/metro_stations/shape_file.shp')\ngreen_space_df = gpd.read_file('path/to/green_spaces/shape_file.shp')\n\nshape_files = [bus_stops, train_stops, metro_stops, green_space_df] # Create list of shapefiles. Keep it in the same order. It will be passed as a parameter to featureEngineering\n\n# Enrich trip data with contextual features\ntrip_df = featureEngineering(\n    trip_df=trip_df, shape_files=shape_files, cpu_cores=cpu_cores\n)\n\n# Save enriched data\nsaveFile(f\"{output_dir}/tmd\", \"processed_trip_points_data.csv\", trip_df)\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-generate-trip-statistics","title":"\ud83d\udcca Step 3: Generate Trip Statistics","text":"<pre><code>from meowmotion.data_formatter import generateTrajStats\n\n# Extract movement stats for each trip\ntrip_stats_df = generateTrajStats(trip_df)\n\n# Save the results\nsaveFile(f\"{output_dir}/tmd\", \"trip_stats_data.csv\", trip_stats_df)\n</code></pre>"},{"location":"getting-started/quick-start/#step-4-predict-travel-mode","title":"\ud83e\udd16 Step 4: Predict Travel Mode","text":"<pre><code>from meowmotion.model_tmd import modePredict\n\nop_df, agg_op_df = modePredict(\n    processed_non_agg_data=processed_non_agg_data,       # processed_data dataFrame\n    stats_agg_data=stats_agg_data,                       # stats_data dataFrame\n    artifacts_dir=\"path/to/artifacts\", # Create a folder artifacts and keep model and label encoder in it.\n    model_file_name=\"model.pkl\",\n    le_file_name=\"label_encoder.joblib\",\n    shape_file=\"path/to/shape_file.shp\",\n    output_dir=\"path/to/output_dir\"\n)\n</code></pre> <p>\u26a0\ufe0f Note: The <code>model.pkl</code> and <code>label_encoder.joblib</code> files are not included in this repository. Please request them from the UBDC Data Service: \ud83d\udce7 <code>ubdc-dataservice@glasgow.ac.uk</code></p>"},{"location":"getting-started/quick-start/#output-travel-mode-detection-results","title":"\ud83d\udccb Output: Travel Mode Detection Results","text":"<p>After running the Travel Mode Detection pipeline, two types of outputs are generated:</p>"},{"location":"getting-started/quick-start/#aggregated-output-travel-mode-matrix","title":"\ud83d\uddfa\ufe0f Aggregated Output \u2014 Travel Mode Matrix","text":"<p>Each row represents a unique origin-destination (OD) pair with counts of trips detected by travel mode.</p> Column Description <code>origin_geo_code</code> Geographic code of the trip origin area <code>destination_geo_code</code> Geographic code of the trip destination area <code>bicycle</code> Number of trips detected as Bicycle trips <code>bus</code> Number of trips detected as Bus trips <code>car</code> Number of trips detected as Car trips <code>train</code> Number of trips detected as Train trips <code>walk</code> Number of trips detected as Walk trips <p>\ud83d\udeb2 \ud83d\ude8d \ud83d\ude97 \ud83d\ude86 \ud83d\udeb6 Aggregated results help analyze transport mode distribution between OD pairs across the study area.</p>"},{"location":"getting-started/quick-start/#non-aggregated-output-detected-travel-mode-for-each-trip","title":"\ud83e\udded Non-Aggregated Output \u2014 Detected Travel Mode for Each Trip","text":"<p>This file contains detailed trip-level detection results for each GPS trajectory point associated with a trip.</p> Column Description <code>trip_id</code> Unique identifier for the trip <code>origin_geo_code</code> Geographic code of the trip's origin <code>destination_geo_code</code> Geographic code of the trip's destination <code>tp_lat</code> Latitude of the trajectory point <code>tp_lng</code> Longitude of the trajectory point <code>datetime</code> Timestamp of the trajectory point <code>travel_mode</code> Predicted transport mode at the trajectory point <p>\ud83e\udde0 This detailed file allows fine-grained analysis of mode-switching behavior within trips or validation against high-resolution GPS tracks.</p>"},{"location":"getting-started/quick-start/#notes-on-required-input-files_1","title":"\ud83d\udccc Notes on Required Input Files","text":"<p>The Travel Mode Detection pipeline uses multiple shapefiles to extract spatial characteristics of the trips.</p> <p>Each shapefile must:</p> <ul> <li>Use EPSG:4326 (WGS84) coordinate system.</li> <li>Contain appropriate geometry (<code>POINT</code> buffers or <code>POLYGON</code> areas) for spatial analysis.</li> </ul>"},{"location":"getting-started/quick-start/#bus-stops-shapefile","title":"\ud83d\ude8c Bus Stops Shapefile","text":"stop_id lng lat geometry bs_001 -4.259865 55.857296 POLYGON ((-4.25939 55.85730, ...)) bs_002 -4.258346 55.861953 POLYGON ((-4.25787 55.86196, ...)) <ul> <li><code>lng</code> and <code>lat</code> are the exact coordinates of each bus stop.</li> <li><code>geometry</code> defines a 30-meter buffer polygon around the stop (configurable as needed).</li> </ul>"},{"location":"getting-started/quick-start/#metro-stations-shapefile","title":"\ud83d\ude87 Metro Stations Shapefile","text":"stop_id lng lat geometry ms_001 -4.258553 55.852036 POLYGON ((-4.25807 55.85204, ...)) ms_002 -4.294267 55.852112 POLYGON ((-4.29379 55.85212, ...)) <ul> <li>Similar to bus stops, metro stations use point coordinates and buffered areas.</li> </ul>"},{"location":"getting-started/quick-start/#train-stations-shapefile","title":"\ud83d\ude86 Train Stations Shapefile","text":"stop_id lng lat geometry ts_001 -4.269514 55.864641 POLYGON ((-4.26903 55.86465, ...)) ts_002 -4.283278 55.861438 POLYGON ((-4.28280 55.86145, ...))"},{"location":"getting-started/quick-start/#green-spaces-shapefile","title":"\ud83c\udf33 Green Spaces Shapefile","text":"gsp_id geometry gs_001 POLYGON Z ((219216.711 666579.172 ...)) gs_002 POLYGON Z ((219243.240 666760.324 ...)) <ul> <li>Green space shapefiles do not have <code>lng</code> or <code>lat</code> columns.</li> <li>They consist only of polygon geometries representing parks, fields, or natural areas.</li> <li>Make sure polygons are valid and CRS is correctly set.</li> </ul> <p>\ud83d\udccc These shapefiles help the model understand whether trips interact with transport networks (stops, stations) or land-use features (parks, green spaces).</p>"},{"location":"getting-started/quick-start/#youre-done","title":"\u2705 You're Done!","text":"<p>\ud83c\udf89 Congratulations! You've successfully completed the MeowMotion core pipelines.</p> <p>By now, you have:</p> <ul> <li>\u2705 Cleaned and filtered raw GPS data.</li> <li>\u2705 Detected user stop nodes based on spatial and temporal patterns.</li> <li>\u2705 Generated trip-level flows between detected stop nodes.</li> <li>\u2705 Scaled trips to population levels by producing multiple OD matrices.</li> <li>\u2705 Produced additional datasets such as trip points, stay points, and user activity statistics.</li> <li>\u2705 Performed feature engineering by integrating public transport stops and green space data.</li> <li>\u2705 Built movement statistics and processed trips for modeling.</li> <li>\u2705 Predicted travel modes for each trip using pre-trained ML models.</li> </ul> <p>Your outputs are ready for:</p> <ul> <li>\ud83d\udcca OD-based mobility analysis</li> <li>\ud83d\udee3\ufe0f Transport planning and policy simulations</li> <li>\ud83d\udea6 Travel behavior studies</li> <li>\ud83e\udde0 Further machine learning or custom model training</li> </ul> <p>\ud83d\udcda For deeper details, tutorials, or API references, head over to the full MeowMotion Documentation.</p> <p>\ud83d\udc3e Happy analyzing with MeowMotion!</p>"},{"location":"reference/data_formatter/","title":"Data Formatter","text":""},{"location":"reference/data_formatter/#meowmotion.data_formatter.calculateBearing","title":"<code>calculateBearing(lat1, lon1, lat2, lon2)</code>","text":"<p>Computes the initial bearing (forward azimuth) from one geographic coordinate to another. This bearing is measured clockwise from true north and returned as a value between 0 and 360 degrees.</p> How It Works <ol> <li>Converts both starting (lat1, lon1) and ending (lat2, lon2) coordinates to radians.</li> <li>Uses the difference in longitudes (dlon) and trigonometric functions to calculate    the bearing in radians.</li> <li>Converts the bearing from radians to degrees.</li> <li>Normalizes the result to ensure it falls within the range of [0, 360).</li> </ol> <p>Parameters:</p> Name Type Description Default <code>lat1</code> <code>float</code> <p>Latitude of the start location (in decimal degrees).</p> required <code>lon1</code> <code>float</code> <p>Longitude of the start location (in decimal degrees).</p> required <code>lat2</code> <code>float</code> <p>Latitude of the end location (in decimal degrees).</p> required <code>lon2</code> <code>float</code> <p>Longitude of the end location (in decimal degrees).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The initial bearing in degrees, between 0 and 360.</p> Example <p>bearing = calculateBearing(12.9716, 77.5946, 13.0827, 80.2707) print(bearing) 76.123456789  # Example output</p> Source code in <code>meowmotion/data_formatter.py</code> <pre><code>def calculateBearing(lat1: float, lon1: float, lat2: float, lon2: float) -&gt; float:\n    \"\"\"\n    Computes the initial bearing (forward azimuth) from one geographic coordinate to another.\n    This bearing is measured clockwise from true north and returned as a value between 0 and 360 degrees.\n\n    How It Works:\n        1. Converts both starting (lat1, lon1) and ending (lat2, lon2) coordinates to radians.\n        2. Uses the difference in longitudes (dlon) and trigonometric functions to calculate\n           the bearing in radians.\n        3. Converts the bearing from radians to degrees.\n        4. Normalizes the result to ensure it falls within the range of [0, 360).\n\n    Args:\n        lat1 (float): Latitude of the start location (in decimal degrees).\n        lon1 (float): Longitude of the start location (in decimal degrees).\n        lat2 (float): Latitude of the end location (in decimal degrees).\n        lon2 (float): Longitude of the end location (in decimal degrees).\n\n    Returns:\n        float: The initial bearing in degrees, between 0 and 360.\n\n    Example:\n        &gt;&gt;&gt; bearing = calculateBearing(12.9716, 77.5946, 13.0827, 80.2707)\n        &gt;&gt;&gt; print(bearing)\n        76.123456789  # Example output\n    \"\"\"\n\n    lon1, lat1, lon2, lat2 = map(np.deg2rad, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1\n    y = np.sin(dlon) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n    bearing = np.arctan2(y, x)\n    bearing = np.rad2deg(bearing)\n    bearing = (bearing + 360) % 360\n    return bearing\n</code></pre>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.calculateStraightnessIndex","title":"<code>calculateStraightnessIndex(df)</code>","text":"<p>Calculates a trip\u2019s \"straightness index\" by comparing the total distance traveled to the straight-line distance between the first and last points. The resulting ratio (straight-line distance \u00f7 actual path distance) measures how directly a traveler moved from start to end.</p> How It Works <ol> <li>Summarizes the total distance covered (<code>distance_covered</code>).</li> <li>Calculates the straight-line (haversine) distance between the first and last    coordinates in the trip.</li> <li>Divides the straight-line distance by the actual path length.</li> <li>Returns that value for every row in the DataFrame.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame representing a single trip, containing at least - <code>lat</code>: Latitude coordinates - <code>lng</code>: Longitude coordinates - <code>distance_covered</code>: The distance between consecutive points (in meters)</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: A list (the same length as <code>df</code>) with the same straightness index</p> <code>List[float]</code> <p>repeated for each row. If the path length is 0 or NaN, returns <code>[np.nan] * len(df)</code>.</p> Example <p>import pandas as pd from haversine import haversine df = pd.DataFrame({ ...     \"lat\": [12.9716, 13.0827], ...     \"lng\": [77.5946, 80.2707], ...     \"distance_covered\": [0, 35000]  # for example ... }) result = calculateStraightnessIndex(df) print(result) [0.5, 0.5]  # Indicates the path is half as direct as a straight line</p> Source code in <code>meowmotion/data_formatter.py</code> <pre><code>def calculateStraightnessIndex(df: pd.DataFrame) -&gt; List[float]:\n    \"\"\"\n    Calculates a trip\u2019s \"straightness index\" by comparing the total distance traveled\n    to the straight-line distance between the first and last points. The resulting ratio\n    (straight-line distance \u00f7 actual path distance) measures how directly a traveler\n    moved from start to end.\n\n    How It Works:\n        1. Summarizes the total distance covered (`distance_covered`).\n        2. Calculates the straight-line (haversine) distance between the first and last\n           coordinates in the trip.\n        3. Divides the straight-line distance by the actual path length.\n        4. Returns that value for every row in the DataFrame.\n\n    Args:\n        df (pd.DataFrame): A DataFrame representing a single trip, containing at least\n            - `lat`: Latitude coordinates\n            - `lng`: Longitude coordinates\n            - `distance_covered`: The distance between consecutive points (in meters)\n\n    Returns:\n        List[float]: A list (the same length as `df`) with the same straightness index\n        repeated for each row. If the path length is 0 or NaN, returns `[np.nan] * len(df)`.\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from haversine import haversine\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"lat\": [12.9716, 13.0827],\n        ...     \"lng\": [77.5946, 80.2707],\n        ...     \"distance_covered\": [0, 35000]  # for example\n        ... })\n        &gt;&gt;&gt; result = calculateStraightnessIndex(df)\n        &gt;&gt;&gt; print(result)\n        [0.5, 0.5]  # Indicates the path is half as direct as a straight line\n    \"\"\"\n    # Calculate the length of the actual path\n    path_length = df[\"distance_covered\"].sum()\n    if path_length == 0 or np.isnan(path_length):\n        return [np.nan] * df.shape[0]  # Avoid division error\n    total_points = df.shape[0] - 1\n    first_lat = df.iloc[0][\"lat\"]\n    first_lon = df.iloc[0][\"lng\"]\n    last_lat = df.iloc[total_points][\"lat\"]\n    last_lon = df.iloc[total_points][\"lng\"]\n\n    # Calculate the length of the shortest possible straight line\n    straight_line_length = haversine(\n        (first_lat, first_lon), (last_lat, last_lon), unit=\"m\"\n    )\n\n    # Calculate the straightness index\n    straightness_index = straight_line_length / path_length\n    return [straightness_index] * df.shape[0]\n</code></pre>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.checkIfAtGrrenSpace","title":"<code>checkIfAtGrrenSpace(df, sdf)</code>","text":"<p>Checks whether a trip has at least five data points that fall within the specified green space polygons. If it does, the entire trip is marked with 1 for every row. Otherwise, it returns 0 for each row.</p> Note <ul> <li>This function uses a threshold of five detections by default, but you can   customize this threshold as needed.</li> </ul> How It Works <ol> <li>Iterates through all points in <code>df</code> (each representing a trajectory point in the trip).</li> <li>For each point, checks if it intersects any of the polygons in <code>sdf</code>.</li> <li>If at least five points from the trip are found in a green space, returns a list    of 1s (one per row in <code>df</code>).</li> <li>Otherwise, returns a list of 0s.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing trip data with at least <code>lat</code> and <code>lng</code> columns for each point in the trip.</p> required <code>sdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame representing one or more green space polygons.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: A list of integers (1 or 0) for each row in <code>df</code>. - <code>[1, 1, ..., 1]</code> if at least five points are within green space - <code>[0, 0, ..., 0]</code> otherwise</p> Example <p>df = pd.DataFrame({ ...     \"lat\": [12.9716, 12.9780, 12.9825, 12.9850, 12.9900], ...     \"lng\": [77.5946, 77.5949, 77.5953, 77.5960, 77.5965] ... }) green_spaces = gpd.read_file(\"greenspaces.shp\")  # Example file result = checkIfAtGrrenSpace(df, green_spaces) print(result) [1, 1, 1, 1, 1]   # indicates at least 5 points are inside a green space</p> Source code in <code>meowmotion/data_formatter.py</code> <pre><code>def checkIfAtGrrenSpace(df: pd.DataFrame, sdf: gpd.GeoDataFrame) -&gt; List[int]:\n    \"\"\"\n    Checks whether a trip has at least five data points that fall within the specified\n    green space polygons. If it does, the entire trip is marked with 1 for every row.\n    Otherwise, it returns 0 for each row.\n\n    Note:\n        - This function uses a threshold of five detections by default, but you can\n          customize this threshold as needed.\n\n    How It Works:\n        1. Iterates through all points in `df` (each representing a trajectory point in the trip).\n        2. For each point, checks if it intersects any of the polygons in `sdf`.\n        3. If at least five points from the trip are found in a green space, returns a list\n           of 1s (one per row in `df`).\n        4. Otherwise, returns a list of 0s.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing trip data with at least `lat` and `lng`\n            columns for each point in the trip.\n        sdf (gpd.GeoDataFrame): GeoDataFrame representing one or more green space polygons.\n\n    Returns:\n        List[int]: A list of integers (1 or 0) for each row in `df`.\n            - `[1, 1, ..., 1]` if at least five points are within green space\n            - `[0, 0, ..., 0]` otherwise\n\n    Example:\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"lat\": [12.9716, 12.9780, 12.9825, 12.9850, 12.9900],\n        ...     \"lng\": [77.5946, 77.5949, 77.5953, 77.5960, 77.5965]\n        ... })\n        &gt;&gt;&gt; green_spaces = gpd.read_file(\"greenspaces.shp\")  # Example file\n        &gt;&gt;&gt; result = checkIfAtGrrenSpace(df, green_spaces)\n        &gt;&gt;&gt; print(result)\n        [1, 1, 1, 1, 1]   # indicates at least 5 points are inside a green space\n    \"\"\"\n\n    count = 0\n    for i in range(df.shape[0]):\n        lat = df.iloc[0][\"lat\"]\n        lon = df.iloc[0][\"lng\"]\n        coord_point = Point(lon, lat)\n        point_found_at_gs = False\n        intersections = sdf.sindex.intersection(coord_point.bounds)\n        for index in intersections:\n            polygon = sdf.loc[index, \"geometry\"]\n            if coord_point.intersects(polygon):\n                point_found_at_gs = True\n                break\n        if point_found_at_gs is True:\n            count += 1\n            if count == 5:\n                break\n    if count &gt;= 5:\n        return [1] * df.shape[0]\n    else:\n        return [0] * df.shape[0]\n</code></pre>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.checkIfNearStop","title":"<code>checkIfNearStop(df, sdf)</code>","text":"<p>Determines whether the first and/or last point of a trip lies within a given polygon area (e.g., bus stop, train station, or metro station). It returns a list of length equal to <code>df.shape[0]</code>, with each element indicating whether:</p> <ul> <li>Both the first and last points intersect the polygon(s): 2</li> <li>Only one of the points intersects the polygon(s): 1</li> <li>Neither the first nor the last point intersects the polygon(s): 0</li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing trip data, including <code>lat</code> and <code>lng</code> columns for each point in the trip.</p> required <code>sdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame representing polygons for stops or stations (e.g., bus stops, train stations, metro stations).</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: A list of integers (2, 1, or 0) indicating the presence of the</p> <code>List[int]</code> <p>first/last point in the polygon(s).</p> Example <p>import pandas as pd import geopandas as gpd from shapely.geometry import Polygon</p> Source code in <code>meowmotion/data_formatter.py</code> <pre><code>def checkIfNearStop(df: pd.DataFrame, sdf: gpd.GeoDataFrame) -&gt; List[int]:\n    \"\"\"\n    Determines whether the first and/or last point of a trip lies within a given\n    polygon area (e.g., bus stop, train station, or metro station). It returns a list\n    of length equal to `df.shape[0]`, with each element indicating whether:\n\n    - Both the first and last points intersect the polygon(s): 2\n    - Only one of the points intersects the polygon(s): 1\n    - Neither the first nor the last point intersects the polygon(s): 0\n\n    Args:\n        df (pd.DataFrame): DataFrame containing trip data, including `lat` and `lng`\n            columns for each point in the trip.\n        sdf (gpd.GeoDataFrame): GeoDataFrame representing polygons for stops or stations\n            (e.g., bus stops, train stations, metro stations).\n\n    Returns:\n        List[int]: A list of integers (2, 1, or 0) indicating the presence of the\n        first/last point in the polygon(s).\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; from shapely.geometry import Polygon\n        &gt;&gt;&gt; # Example DataFrame with two points\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'lat': [12.9716, 12.9760],\n        ...     'lng': [77.5946, 77.5950]\n        ... })\n        &gt;&gt;&gt; # Example GeoDataFrame representing a stop\n        &gt;&gt;&gt; polygons = [Polygon([(77.5940, 12.9710), (77.5950, 12.9710),\n        ...                      (77.5950, 12.9720), (77.5940, 12.9720)])]\n        &gt;&gt;&gt; sdf = gpd.GeoDataFrame(geometry=polygons, crs=\"EPSG:4326\")\n        &gt;&gt;&gt; result = checkIfNearStop(df, sdf)\n        &gt;&gt;&gt; print(result)\n        [1, 1]  # Only the first point intersects the polygon\n    \"\"\"\n    total_points = df.shape[0] - 1\n    first_lat = df.iloc[0][\"lat\"]\n    first_lon = df.iloc[0][\"lng\"]\n    last_lat = df.iloc[total_points][\"lat\"]\n    last_lon = df.iloc[total_points][\"lng\"]\n    first_point = Point(first_lon, first_lat)\n    last_point = Point(last_lon, last_lat)\n    first_point_found = False\n    last_point_found = False\n\n    # checking for first point\n    intersections = sdf.sindex.intersection(first_point.bounds)\n    for index in intersections:\n        polygon = sdf.loc[index, \"geometry\"]\n        # Check for intersection\n        if first_point.intersects(polygon):\n            first_point_found = True\n            break\n\n    # checking for last point\n    intersections = sdf.sindex.intersection(last_point.bounds)\n    for index in intersections:\n        polygon = sdf.loc[index, \"geometry\"]\n        # Check for intersection\n        if last_point.intersects(polygon):\n            last_point_found = True\n            break\n\n    if first_point_found and last_point_found:\n        ar = [2] * df.shape[0]\n        return ar\n    elif first_point_found or last_point_found:\n        ar = [1] * df.shape[0]\n        return ar\n    else:\n        ar = [0] * df.shape[0]\n        return ar\n</code></pre>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.checkIfNearStop--example-dataframe-with-two-points","title":"Example DataFrame with two points","text":"<p>df = pd.DataFrame({ ...     'lat': [12.9716, 12.9760], ...     'lng': [77.5946, 77.5950] ... })</p>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.checkIfNearStop--example-geodataframe-representing-a-stop","title":"Example GeoDataFrame representing a stop","text":"<p>polygons = [Polygon([(77.5940, 12.9710), (77.5950, 12.9710), ...                      (77.5950, 12.9720), (77.5940, 12.9720)])] sdf = gpd.GeoDataFrame(geometry=polygons, crs=\"EPSG:4326\") result = checkIfNearStop(df, sdf) print(result) [1, 1]  # Only the first point intersects the polygon</p>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.featureEngineering","title":"<code>featureEngineering(trip_df, shape_files, cpu_cores=max(1, int(cpu_count() // 2)))</code>","text":"<p>Performs feature engineering on raw trip data by partitioning it and processing each partition in parallel. This includes calculating advanced trip features such as speed, acceleration, angular deviation, and straightness index, as well as identifying whether a trip starts or ends near transport stops or green spaces.</p> <p>This function distributes work across the specified number of CPU cores, calls the <code>processData</code> child function on each chunk, and then merges all processed chunks into a single DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>trip_df</code> <code>DataFrame</code> <p>A DataFrame containing raw trip information, including columns for user ID, trip ID, latitude (<code>lat</code>), longitude (<code>lng</code>), and timestamps (<code>datetime</code>).</p> required <code>shape_files</code> <code>List[GeoDataFrame]</code> <p>A list of GeoDataFrames representing various geographic layers (e.g., bus stops, train stops, metro stops, green spaces). These are used to check if trips start/end near these points or areas.</p> required <code>cpu_cores</code> <code>int</code> <p>Number of CPU cores to use for parallel processing. Defaults to half of the available cores.</p> <code>max(1, int(cpu_count() // 2))</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A concatenated DataFrame containing the enhanced feature set for all trips.</p> <code>DataFrame</code> <p>Features include: - Speed, acceleration, jerk, and angular deviation - Straightness index - Indicators for whether a trip begins or ends near transport stops or in green spaces - Filtered trips based on minimum impressions</p> Example Source code in <code>meowmotion/data_formatter.py</code> <pre><code>def featureEngineering(\n    trip_df: pd.DataFrame,\n    shape_files: List[gpd.GeoDataFrame],\n    cpu_cores: int = max(1, int(cpu_count() // 2)),\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs feature engineering on raw trip data by partitioning it and processing each partition\n    in parallel. This includes calculating advanced trip features such as speed, acceleration,\n    angular deviation, and straightness index, as well as identifying whether a trip starts or ends\n    near transport stops or green spaces.\n\n    This function distributes work across the specified number of CPU cores, calls the `processData`\n    child function on each chunk, and then merges all processed chunks into a single DataFrame.\n\n    Args:\n        trip_df (pd.DataFrame): A DataFrame containing raw trip information, including columns for\n            user ID, trip ID, latitude (`lat`), longitude (`lng`), and timestamps (`datetime`).\n        shape_files (List[gpd.GeoDataFrame]): A list of GeoDataFrames representing various geographic\n            layers (e.g., bus stops, train stops, metro stops, green spaces). These are used to check\n            if trips start/end near these points or areas.\n        cpu_cores (int, optional): Number of CPU cores to use for parallel processing. Defaults to half\n            of the available cores.\n\n    Returns:\n        pd.DataFrame: A concatenated DataFrame containing the enhanced feature set for all trips.\n        Features include:\n            - Speed, acceleration, jerk, and angular deviation\n            - Straightness index\n            - Indicators for whether a trip begins or ends near transport stops or in green spaces\n            - Filtered trips based on minimum impressions\n\n    Example:\n        &gt;&gt;&gt; # Suppose 'trip_df' is a DataFrame of trips and 'shapes' is a list of GeoDataFrames\n        &gt;&gt;&gt; from your_module import featureEngineering\n        &gt;&gt;&gt; enhanced_df = featureEngineering(trip_df, shapes, cores=4)\n        &gt;&gt;&gt; print(enhanced_df.head())\n    \"\"\"\n\n    print(f\"{datetime.now()}: Get Load Balanced Buckets\")\n    df_collection = getLoadBalancedBuckets(trip_df, cpu_cores)\n    args = [(df, shape_files) for df in df_collection]  # Wrap each df in a tuple\n    del df_collection\n    with Pool(cpu_cores) as p:\n        tdf = p.starmap(processData, args)\n    return pd.concat(tdf, ignore_index=True)\n</code></pre>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.featureEngineering--suppose-trip_df-is-a-dataframe-of-trips-and-shapes-is-a-list-of-geodataframes","title":"Suppose 'trip_df' is a DataFrame of trips and 'shapes' is a list of GeoDataFrames","text":"<p>from your_module import featureEngineering enhanced_df = featureEngineering(trip_df, shapes, cores=4) print(enhanced_df.head())</p>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.generateTrajStats","title":"<code>generateTrajStats(df)</code>","text":"<p>Aggregates and summarizes key trip-level statistics (e.g., median/percentile speeds, accelerations, jerk, angular deviation, distance) from enhanced trip data. This function operates on data that has already undergone feature engineering (e.g., via <code>featureEngineering</code>), and creates consolidated columns reflecting various trip metrics. A progress bar is displayed during calculation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame containing enhanced trip data, including columns such as <code>uid</code>, <code>trip_id</code>, <code>new_speed</code>, <code>accelaration</code>, <code>jerk</code>, and <code>angular_deviation</code>. It may also contain flags for stops and green spaces.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing aggregated statistics for each trip, including: - Speed median, 95th percentile, and standard deviation - Acceleration median, 95th percentile, and standard deviation - Jerk median, 95th percentile, and standard deviation - Angular deviation median, 95th percentile, and standard deviation - Straightness index - Total distance covered (km) - Indicators for whether the trip starts/ends near specific transport stops or green spaces - Weekend/hour categories - A placeholder for <code>transport_mode</code></p> Example <p>import pandas as pd data = { ...     \"uid\": [1, 1, 1, 2, 2], ...     \"trip_id\": [10, 10, 10, 20, 20], ...     \"new_speed\": [3.0, 5.5, 2.0, 4.0, 4.5], ...     \"accelaration\": [0.1, 0.2, 0.3, 0.1, 0.05], ...     \"jerk\": [0.01, 0.02, 0.03, 0.01, 0.02], ...     \"angular_deviation\": [5, 10, 15, 3, 4], ... } df = pd.DataFrame(data) result = generateTrajStats(df) result.head()    datetime  uid  trip_id  speed_median  ...  hour_category  transport_mode 0       NaT    1       10           3.5  ...             0             NaN 1       NaT    1       10           3.5  ...             0             NaN 2       NaT    1       10           3.5  ...             0             NaN 3       NaT    2       20           4.25 ...             0             NaN 4       NaT    2       20           4.25 ...             0             NaN</p> Source code in <code>meowmotion/data_formatter.py</code> <pre><code>def generateTrajStats(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates and summarizes key trip-level statistics (e.g., median/percentile speeds,\n    accelerations, jerk, angular deviation, distance) from enhanced trip data. This function\n    operates on data that has already undergone feature engineering (e.g., via `featureEngineering`),\n    and creates consolidated columns reflecting various trip metrics. A progress bar is displayed\n    during calculation.\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing enhanced trip data, including columns such as\n            `uid`, `trip_id`, `new_speed`, `accelaration`, `jerk`, and `angular_deviation`.\n            It may also contain flags for stops and green spaces.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing aggregated statistics for each trip, including:\n            - Speed median, 95th percentile, and standard deviation\n            - Acceleration median, 95th percentile, and standard deviation\n            - Jerk median, 95th percentile, and standard deviation\n            - Angular deviation median, 95th percentile, and standard deviation\n            - Straightness index\n            - Total distance covered (km)\n            - Indicators for whether the trip starts/ends near specific transport stops or green spaces\n            - Weekend/hour categories\n            - A placeholder for `transport_mode`\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = {\n        ...     \"uid\": [1, 1, 1, 2, 2],\n        ...     \"trip_id\": [10, 10, 10, 20, 20],\n        ...     \"new_speed\": [3.0, 5.5, 2.0, 4.0, 4.5],\n        ...     \"accelaration\": [0.1, 0.2, 0.3, 0.1, 0.05],\n        ...     \"jerk\": [0.01, 0.02, 0.03, 0.01, 0.02],\n        ...     \"angular_deviation\": [5, 10, 15, 3, 4],\n        ... }\n        &gt;&gt;&gt; df = pd.DataFrame(data)\n        &gt;&gt;&gt; result = generateTrajStats(df)\n        &gt;&gt;&gt; result.head()\n           datetime  uid  trip_id  speed_median  ...  hour_category  transport_mode\n        0       NaT    1       10           3.5  ...             0             NaN\n        1       NaT    1       10           3.5  ...             0             NaN\n        2       NaT    1       10           3.5  ...             0             NaN\n        3       NaT    2       20           4.25 ...             0             NaN\n        4       NaT    2       20           4.25 ...             0             NaN\n\n    \"\"\"\n\n    progress_bar = tqdm(total=25)\n\n    temp_df = df.copy()\n    if \"transport_mode\" not in temp_df.columns:\n        temp_df[\"transport_mode\"] = np.nan\n\n    progress_bar.update(1)\n    temp_df[\"speed_median\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"new_speed\"\n    ].transform(lambda x: x.median())\n    progress_bar.update(1)\n    temp_df[\"speed_pct_95\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"new_speed\"\n    ].transform(lambda x: np.percentile(x, 95))\n    progress_bar.update(1)\n    temp_df[\"speed_std\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\"new_speed\"].transform(\n        lambda x: np.std(x)\n    )\n    progress_bar.update(1)\n    temp_df[\"acceleration_median\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"accelaration\"\n    ].transform(lambda x: np.nanmedian(x))\n    progress_bar.update(1)\n    temp_df[\"acceleration_pct_95\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"accelaration\"\n    ].transform(lambda x: np.nanpercentile(x, 95))\n    progress_bar.update(1)\n    temp_df[\"acceleration_std\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"accelaration\"\n    ].transform(lambda x: np.nanstd(x))\n    progress_bar.update(1)\n    temp_df[\"jerk_median\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\"jerk\"].transform(\n        lambda x: np.nanmedian(x)\n    )\n    progress_bar.update(1)\n    temp_df[\"jerk_pct_95\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\"jerk\"].transform(\n        lambda x: np.nanpercentile(x, 95)\n    )\n    progress_bar.update(1)\n    temp_df[\"jerk_std\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\"jerk\"].transform(\n        lambda x: np.nanstd(x)\n    )\n    progress_bar.update(1)\n    temp_df[\"angular_dev_median\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"angular_deviation\"\n    ].transform(lambda x: np.nanmedian(x))\n    progress_bar.update(1)\n    temp_df[\"angular_dev_pct_95\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"angular_deviation\"\n    ].transform(lambda x: np.nanpercentile(x, 95))\n    progress_bar.update(1)\n    temp_df[\"angular_dev_std\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"angular_deviation\"\n    ].transform(lambda x: np.nanstd(x))\n    progress_bar.update(1)\n    temp_df[\"straightness_index\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"straightness_index\"\n    ].transform(lambda x: x.values[0])\n    progress_bar.update(1)\n    temp_df[\"distance_covered\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"distance_covered\"\n    ].transform(lambda x: sum(x) / 1000)\n    progress_bar.update(1)\n    temp_df[\"start_end_at_bus_stop\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"start_end_at_bus_stop\"\n    ].transform(lambda x: x.values[0])\n    progress_bar.update(1)\n    temp_df[\"start_end_at_train_stop\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"start_end_at_train_stop\"\n    ].transform(lambda x: x.values[0])\n    progress_bar.update(1)\n    temp_df[\"start_end_at_metro_stop\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"start_end_at_metro_stop\"\n    ].transform(lambda x: x.values[0])\n    progress_bar.update(1)\n    temp_df[\"found_at_green_space\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"found_at_green_space\"\n    ].transform(lambda x: x.values[0])\n    progress_bar.update(1)\n    # temp_df['temperature']=temp_df.groupby(['uid','trip_id'])['t'].transform(lambda x: x.mean())\n    progress_bar.update(1)\n    # temp_df['visibility']=temp_df.groupby(['uid','trip_id'])['v'].transform(lambda x: x.mean())\n    progress_bar.update(1)\n    # temp_df['wind_speed']=temp_df.groupby(['uid','trip_id'])['s'].transform(lambda x: x.mean())\n    progress_bar.update(1)\n    temp_df[\"is_weekend\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\"is_weekend\"].transform(\n        lambda x: x.values[0]\n    )\n    progress_bar.update(1)\n    temp_df[\"hour_category\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"hour_category\"\n    ].transform(lambda x: x.values[0])\n    progress_bar.update(1)\n    temp_df = temp_df[\n        [\n            \"datetime\",\n            \"uid\",\n            \"trip_id\",\n            \"speed_median\",\n            \"speed_pct_95\",\n            \"speed_std\",\n            \"acceleration_median\",\n            \"acceleration_pct_95\",\n            \"acceleration_std\",\n            \"jerk_median\",\n            \"jerk_pct_95\",\n            \"jerk_std\",\n            \"angular_dev_median\",\n            \"angular_dev_pct_95\",\n            \"angular_dev_std\",\n            \"straightness_index\",\n            \"distance_covered\",\n            \"start_end_at_bus_stop\",\n            \"start_end_at_train_stop\",\n            \"start_end_at_metro_stop\",\n            \"found_at_green_space\",\n            \"is_weekend\",\n            \"hour_category\",\n            \"transport_mode\",\n        ]\n    ]\n    progress_bar.update(1)\n    return temp_df\n</code></pre>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.processData","title":"<code>processData(df, shape_files)</code>","text":"<p>Cleans and enriches raw trip-point data with motion-related features (speed, acceleration, jerk, bearing, angular deviation, straightness index) and contextual flags indicating proximity to public-transport stops or green spaces.</p> <p>The function operates per trip (<code>uid</code>\u2013<code>trip_id</code>):   1. Removes duplicate timestamps and drops trips with fewer than      five distinct observations (<code>num_of_impressions</code> &lt; 5).   2. Computes time deltas, inter-point distance (haversine), speed,      speed z-scores, acceleration, and jerk, replacing extreme      speed outliers (|z| \u2265 3) with the median speed.   3. Derives temporal attributes\u2014calendar month, hour of day,      weekend flag, and a four-level <code>hour_category</code>      (0 Night, 1 Morning, 2 Afternoon, 3 Evening).   4. For each trip, determines whether the first and/or last point      lies within      \u2022 a bus stop (shape_files[0])      \u2022 a train station (shape_files[1])      \u2022 a metro station (shape_files[2])      and whether \u2265 5 points fall inside a green space      polygon (shape_files[3]).   5. Calculates a straightness index (straight-line \u00f7 actual path      length) and removes trips with an index &gt; 1 (spurious data).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Point-level trip data containing at least <code>[\"uid\", \"trip_id\", \"lat\", \"lng\", \"datetime\"]</code>.</p> required <code>shape_files</code> <code>List[GeoDataFrame]</code> <p>A list of four GeoDataFrames in this order: <code>[bus_stops_gdf, train_stops_gdf, metro_stops_gdf, green_space_gdf]</code>.  Each must use CRS EPSG 4326.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The cleaned and feature-rich DataFrame, one row</p> <code>DataFrame</code> <p>per retained point, including new columns such as</p> <code>DataFrame</code> <ul> <li><code>num_of_impressions</code> \u2022 <code>time_taken</code> \u2022 <code>distance_covered</code></li> </ul> <code>DataFrame</code> <ul> <li><code>speed</code> \u2022 <code>speed_z_score</code> \u2022 <code>new_speed</code></li> </ul> <code>DataFrame</code> <ul> <li><code>accelaration</code> \u2022 <code>jerk</code></li> </ul> <code>DataFrame</code> <ul> <li><code>bearing</code> \u2022 <code>angular_deviation</code></li> </ul> <code>DataFrame</code> <ul> <li><code>month</code> \u2022 <code>hour</code> \u2022 <code>is_weekend</code> \u2022 <code>hour_category</code></li> </ul> <code>DataFrame</code> <ul> <li><code>start_end_at_bus_stop</code> / <code>train_stop</code> / <code>metro_stop</code></li> </ul> <code>DataFrame</code> <ul> <li><code>found_at_green_space</code> \u2022 <code>straightness_index</code></li> </ul> Example <p>processed = processData(raw_trip_df, [ ...     bus_stops_gdf, train_stops_gdf, metro_stops_gdf, ...     green_space_gdf ... ]) processed.head()</p> Source code in <code>meowmotion/data_formatter.py</code> <pre><code>def processData(df: pd.DataFrame, shape_files: List[gpd.GeoDataFrame]) -&gt; pd.DataFrame:\n    \"\"\"\n    Cleans and enriches raw trip-point data with motion-related features\n    (speed, acceleration, jerk, bearing, angular deviation, straightness\n    index) and contextual flags indicating proximity to public-transport\n    stops or green spaces.\n\n    The function operates **per trip** (``uid``\u2013``trip_id``):\n      1. Removes duplicate timestamps and drops trips with fewer than\n         five distinct observations (``num_of_impressions`` &lt; 5).\n      2. Computes time deltas, inter-point distance (haversine), speed,\n         speed z-scores, acceleration, and jerk, replacing extreme\n         speed outliers (|z| \u2265 3) with the median speed.\n      3. Derives temporal attributes\u2014calendar month, hour of day,\n         weekend flag, and a four-level ``hour_category``\n         (0 Night, 1 Morning, 2 Afternoon, 3 Evening).\n      4. For each trip, determines whether the first and/or last point\n         lies within\n         \u2022 a **bus stop** (shape_files[0])\n         \u2022 a **train station** (shape_files[1])\n         \u2022 a **metro station** (shape_files[2])\n         and whether **\u2265 5 points** fall inside a **green space**\n         polygon (shape_files[3]).\n      5. Calculates a straightness index (straight-line \u00f7 actual path\n         length) and removes trips with an index &gt; 1 (spurious data).\n\n    Args:\n        df (pd.DataFrame): Point-level trip data containing at least\n            ``[\"uid\", \"trip_id\", \"lat\", \"lng\", \"datetime\"]``.\n        shape_files (List[gpd.GeoDataFrame]): A list of four\n            GeoDataFrames **in this order**:\n            ``[bus_stops_gdf, train_stops_gdf, metro_stops_gdf,\n            green_space_gdf]``.  Each must use CRS EPSG 4326.\n\n    Returns:\n        pd.DataFrame: The cleaned and feature-rich DataFrame, one row\n        per retained point, including new columns such as\n\n        * ``num_of_impressions`` \u2022 ``time_taken`` \u2022 ``distance_covered``\n        * ``speed`` \u2022 ``speed_z_score`` \u2022 ``new_speed``\n        * ``accelaration`` \u2022 ``jerk``\n        * ``bearing`` \u2022 ``angular_deviation``\n        * ``month`` \u2022 ``hour`` \u2022 ``is_weekend`` \u2022 ``hour_category``\n        * ``start_end_at_bus_stop`` / ``train_stop`` / ``metro_stop``\n        * ``found_at_green_space`` \u2022 ``straightness_index``\n\n    Example:\n        &gt;&gt;&gt; processed = processData(raw_trip_df, [\n        ...     bus_stops_gdf, train_stops_gdf, metro_stops_gdf,\n        ...     green_space_gdf\n        ... ])\n        &gt;&gt;&gt; processed.head()\n    \"\"\"\n    temp_df = df.copy()\n\n    # In some trips, for very same timestamp, we observed multiple datapoints. To deal with that, dropping all the duplicate timestamps and keeping the first one\n    temp_df = (\n        temp_df.groupby([\"uid\", \"trip_id\"], group_keys=True)\n        .apply(lambda x: x.drop_duplicates(subset=[\"datetime\"], keep=\"first\"))\n        .reset_index(drop=True)\n    )\n\n    # Counting number of impressions in each trip\n    temp_df[\"num_of_impressions\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        [\"datetime\"]\n    ].transform(lambda x: len(x))\n\n    # Filtering every trip with less than 5 impressions\n    temp_df = temp_df[temp_df.num_of_impressions &gt;= 5]\n\n    # Cacluting the time taken (in seconds) to move from point to the next one in a trip\n    temp_df[\"time_taken\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\"datetime\"].transform(\n        lambda x: x.diff().dt.total_seconds()\n    )\n\n    # Calculating Distance covered from previous point to current point\n    temp_df[\"prev_lat\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\"lat\"].transform(\n        lambda x: x.shift(1)\n    )\n    temp_df[\"prev_long\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\"lng\"].transform(\n        lambda x: x.shift(1)\n    )\n    temp_df.dropna(subset=[\"prev_lat\"], inplace=True)\n    temp_df[\"distance_covered\"] = temp_df.apply(\n        lambda row: haversine(\n            (row[\"lat\"], row[\"lng\"]),\n            (row[\"prev_lat\"], row[\"prev_long\"]),\n            unit=Unit.METERS,\n        ),\n        axis=1,\n    )\n\n    # Calculating Speed with which the distance was covered\n    temp_df[\"speed\"] = temp_df.distance_covered / temp_df.time_taken\n    temp_df[\"date\"] = temp_df[\"datetime\"].dt.date\n    temp_df[\"hour\"] = temp_df[\"datetime\"].dt.hour\n    temp_df = temp_df.astype({\"date\": \"datetime64[ns]\"})\n    assert temp_df[np.isinf(temp_df[\"speed\"])].shape[0] == 0\n    temp_df[\"speed_z_score\"] = temp_df.groupby([\"uid\", \"trip_id\"])[[\"speed\"]].transform(\n        lambda x: abs(stats.zscore(x))\n    )\n\n    # Calculate Acceleration\n    temp_df[\"new_speed\"] = temp_df.groupby([\"uid\", \"trip_id\"], group_keys=False)[\n        [\"speed\", \"speed_z_score\"]\n    ].apply(removeOutlier)[\"speed\"]\n    temp_df[\"accelaration\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"new_speed\"\n    ].transform(lambda x: x.shift(+1))\n    temp_df[\"accelaration\"] = (\n        temp_df[\"new_speed\"] - temp_df[\"accelaration\"]\n    ) / temp_df[\"time_taken\"]\n    temp_df[\"jerk\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\"accelaration\"].transform(\n        lambda x: x.diff()\n    )\n    temp_df[\"jerk\"] = temp_df[\"jerk\"] / temp_df[\"time_taken\"]\n    temp_df[\"bearing\"] = calculateBearing(\n        temp_df[\"prev_lat\"], temp_df[\"prev_long\"], temp_df[\"lat\"], temp_df[\"lng\"]\n    )\n    temp_df[\"angular_deviation\"] = temp_df.groupby([\"uid\", \"trip_id\"])[\n        \"bearing\"\n    ].transform(lambda x: np.abs(x.diff()))\n    temp_df[\"month\"] = temp_df[\"datetime\"].dt.month\n    temp_df[\"hour\"] = temp_df[\"datetime\"].dt.hour\n    temp_df[\"is_weekend\"] = temp_df[\"datetime\"].dt.dayofweek\n    temp_df[\"is_weekend\"] = temp_df.is_weekend.map({5: 1, 6: 1}).fillna(0)\n    temp_df = temp_df.astype({\"is_weekend\": \"int32\"})\n    conditions = [\n        (temp_df.hour &gt;= 0) &amp; (temp_df.hour &lt; 6),\n        (temp_df.hour &gt;= 6) &amp; (temp_df.hour &lt; 12),\n        (temp_df.hour &gt;= 12) &amp; (temp_df.hour &lt; 18),\n        (temp_df.hour &gt;= 18) &amp; (temp_df.hour &lt;= 23),\n    ]\n    category = [0, 1, 2, 3]  # Night, Morning, Afternoon, Evening\n    temp_df[\"hour_category\"] = np.select(conditions, category)\n\n    group = temp_df.groupby([\"uid\", \"trip_id\"])\n    temp_df[\"trip_group\"] = group.ngroup()\n    temp_df.sort_values(by=[\"trip_group\"], ascending=True, inplace=True)\n\n    ###########################################################################################\n\n    new_df = []\n    for i in tqdm(\n        range(temp_df[\"trip_group\"].max()), desc=\"Adding Stops and Green Space Features\"\n    ):\n        tdf = temp_df[temp_df[\"trip_group\"] == i]\n        tdf = tdf.copy()\n        tdf.sort_values(by=[\"datetime\"], ascending=True, inplace=True)\n        tdf[\"start_end_at_bus_stop\"] = checkIfNearStop(tdf, shape_files[0])\n        tdf[\"start_end_at_train_stop\"] = checkIfNearStop(tdf, shape_files[1])\n        tdf[\"start_end_at_metro_stop\"] = checkIfNearStop(tdf, shape_files[2])\n        tdf[\"found_at_green_space\"] = checkIfAtGrrenSpace(tdf, shape_files[3])\n        tdf[\"straightness_index\"] = calculateStraightnessIndex(tdf)\n        new_df.append(tdf)\n\n    temp_df = pd.concat(new_df)\n    del new_df\n    del tdf\n    temp_df = temp_df[\n        temp_df[\"straightness_index\"] &lt;= 1\n    ]  # Filtering out the trips with straightness index greater than 1\n    temp_df.drop(columns=[\"trip_group\"], inplace=True)\n\n    ###########################################################################################\n    return temp_df\n</code></pre>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.processTripData","title":"<code>processTripData(trip_point_df, na_flow_df, raw_df)</code>","text":"<p>Processes trip-level data by expanding stored trip-point coordinates, merging in origin-destination flows, and then attaching timestamps from a raw dataset. The result is a single DataFrame containing trip points (latitude, longitude, and timestamps) and corresponding origin/destination information.</p> Key Steps <ol> <li>Expands list-based trip points in <code>trip_point_df</code> into individual rows    for each (lat, lng) point.</li> <li>Joins the expanded trip points to <code>na_flow_df</code> to retrieve origin,    destination, and timing fields.</li> <li>Filters trips to ensure total travel time does not exceed 24 hours.</li> <li>Merges <code>raw_df</code> to add precise timestamps for each (lat, lng) point and    ensures each point is within the trip's time window.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>trip_point_df</code> <code>DataFrame</code> <p>Contains user IDs, trip IDs, and a column of list-based trip points. Must have columns <code>[\"uid\", \"trip_id\", \"trip_points\"]</code>.</p> required <code>na_flow_df</code> <code>DataFrame</code> <p>Non-aggregated OD flow data containing origin/destination coordinates and timestamps (<code>org_leaving_time</code>, <code>dest_arival_time</code>, etc.).</p> required <code>raw_df</code> <code>DataFrame</code> <p>The raw dataset with columns <code>[\"uid\", \"datetime\", \"lat\", \"lng\"]</code>, used to match each trip point to a specific timestamp.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A cleaned and merged DataFrame with columns for each trip's</p> <code>DataFrame</code> <p>user ID, trip ID, origin/destination coordinates, and per-point latitude,</p> <code>DataFrame</code> <p>longitude, and timestamps.</p> Example Source code in <code>meowmotion/data_formatter.py</code> <pre><code>def processTripData(\n    trip_point_df: pd.DataFrame, na_flow_df: pd.DataFrame, raw_df: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Processes trip-level data by expanding stored trip-point coordinates, merging\n    in origin-destination flows, and then attaching timestamps from a raw dataset.\n    The result is a single DataFrame containing trip points (latitude, longitude, and\n    timestamps) and corresponding origin/destination information.\n\n    Key Steps:\n        1. Expands list-based trip points in `trip_point_df` into individual rows\n           for each (lat, lng) point.\n        2. Joins the expanded trip points to `na_flow_df` to retrieve origin,\n           destination, and timing fields.\n        3. Filters trips to ensure total travel time does not exceed 24 hours.\n        4. Merges `raw_df` to add precise timestamps for each (lat, lng) point and\n           ensures each point is within the trip's time window.\n\n    Args:\n        trip_point_df (pd.DataFrame):\n            Contains user IDs, trip IDs, and a column of list-based trip points.\n            Must have columns `[\"uid\", \"trip_id\", \"trip_points\"]`.\n        na_flow_df (pd.DataFrame):\n            Non-aggregated OD flow data containing origin/destination coordinates\n            and timestamps (`org_leaving_time`, `dest_arival_time`, etc.).\n        raw_df (pd.DataFrame):\n            The raw dataset with columns `[\"uid\", \"datetime\", \"lat\", \"lng\"]`, used\n            to match each trip point to a specific timestamp.\n\n    Returns:\n        pd.DataFrame: A cleaned and merged DataFrame with columns for each trip's\n        user ID, trip ID, origin/destination coordinates, and per-point latitude,\n        longitude, and timestamps.\n\n    Example:\n        &gt;&gt;&gt; # Suppose you already have three DataFrames: trip_point_df, na_flow_df, raw_df\n        &gt;&gt;&gt; result_df = processTripData(trip_point_df, na_flow_df, raw_df)\n        &gt;&gt;&gt; print(result_df.head())\n          uid  trip_id       lat       lng       datetime  org_lat  org_lng  ...\n        0   1       10  12.9716  77.59460  2023-01-01 ...   12.970  77.5940  ...\n        1   1       10  12.9720  77.59470  2023-01-01 ...   12.970  77.5940  ...\n        ...\n\n    \"\"\"\n\n    # trip_file_path = f\"{data_dir}/{city}/{year}/trip_points\"\n    # trip_point_df = pd.read_csv(f\"{trip_file_path}/trip_points_500m_{year}.csv\")\n    trip_point_df[\"trip_points\"] = trip_point_df[\"trip_points\"].apply(ast.literal_eval)\n    trip_point_df = trip_point_df.explode(\"trip_points\")\n    trip_point_df.dropna(subset=[\"trip_points\"], inplace=True)\n    trip_point_df[[\"lat\", \"lng\"]] = pd.DataFrame(\n        trip_point_df[\"trip_points\"].tolist(), index=trip_point_df.index\n    )\n    trip_point_df.drop(columns=[\"trip_points\"], inplace=True)\n\n    # na_flows_file_path = f\"{data_dir}/{city}/{year}/na_flows\"\n    # tdf = pd.read_csv(na_flows_file_path + f\"/na_flows_500m_{year}.csv\")\n\n    na_flow_df[\"org_arival_time\"] = pd.to_datetime(na_flow_df[\"org_arival_time\"])\n    na_flow_df[\"org_leaving_time\"] = pd.to_datetime(na_flow_df[\"org_leaving_time\"])\n    na_flow_df[\"dest_arival_time\"] = pd.to_datetime(na_flow_df[\"dest_arival_time\"])\n    trip_point_df = trip_point_df.merge(\n        na_flow_df[\n            [\n                \"uid\",\n                \"trip_id\",\n                \"org_lat\",\n                \"org_lng\",\n                \"dest_lat\",\n                \"dest_lng\",\n                \"org_arival_time\",\n                \"org_leaving_time\",\n                \"dest_arival_time\",\n            ]\n        ],\n        on=[\"uid\", \"trip_id\"],\n        how=\"left\",\n    )\n    trip_point_df = trip_point_df[\n        (\n            trip_point_df[\"dest_arival_time\"] - trip_point_df[\"org_leaving_time\"]\n        ).dt.total_seconds()\n        / 3600\n        &lt;= 24\n    ]\n\n    print(f\"{datetime.now()}: Merging raw data with trip data to get datetime\")\n    trip_point_df = trip_point_df.merge(\n        raw_df[[\"uid\", \"datetime\", \"lat\", \"lng\"]],\n        on=[\"uid\", \"lat\", \"lng\"],\n        how=\"left\",\n    )\n\n    print(f\"{datetime.now()}: Converting datetime to datetime object\")\n    trip_point_df[\"datetime\"] = pd.to_datetime(trip_point_df[\"datetime\"])\n    trip_point_df = trip_point_df[\n        trip_point_df[\"datetime\"].between(\n            trip_point_df[\"org_leaving_time\"], trip_point_df[\"dest_arival_time\"]\n        )\n    ].reset_index(drop=True)\n    assert trip_point_df[\"datetime\"].isna().sum() == 0\n    print(f\"{datetime.now()}: Validation Done\")\n\n    return trip_point_df\n</code></pre>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.processTripData--suppose-you-already-have-three-dataframes-trip_point_df-na_flow_df-raw_df","title":"Suppose you already have three DataFrames: trip_point_df, na_flow_df, raw_df","text":"<p>result_df = processTripData(trip_point_df, na_flow_df, raw_df) print(result_df.head())   uid  trip_id       lat       lng       datetime  org_lat  org_lng  ... 0   1       10  12.9716  77.59460  2023-01-01 ...   12.970  77.5940  ... 1   1       10  12.9720  77.59470  2023-01-01 ...   12.970  77.5940  ... ...</p>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.readRawData","title":"<code>readRawData(data_dir, cpu_cores=max(1, cpu_count() // 2))</code>","text":"<p>Reads and compiles raw JSON data files for a given year and city by parallel processing multiple monthly files.</p> <p>Parameters:</p> Name Type Description Default <code>cpu_cores</code> <code>int</code> <p>The number of CPU cpu_cores to be used for parallel processing. By default, it uses half of the available cpu_cores.</p> <code>max(1, cpu_count() // 2)</code> <code>data_dir</code> <code>str</code> <p>The directory where the raw data files are stored.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing compiled raw data from all monthly files.</p> Example <p>df = readRawData(2023, \"path_to_root/city/year\") print(df.head())</p> Source code in <code>meowmotion/data_formatter.py</code> <pre><code>def readRawData(\n    data_dir: str, cpu_cores: int = max(1, cpu_count() // 2)\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads and compiles raw JSON data files for a given year and city by parallel processing\n    multiple monthly files.\n\n    Args:\n        cpu_cores (int): The number of CPU cpu_cores to be used for parallel processing. By default, it uses half of the available cpu_cores.\n        data_dir (str): The directory where the raw data files are stored.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing compiled raw data from all monthly files.\n\n    Example:\n        &gt;&gt;&gt; df = readRawData(2023, \"path_to_root/city/year\")\n        &gt;&gt;&gt; print(df.head())\n    \"\"\"\n    root = data_dir\n    month_files = os.listdir(root)\n    args = [(root, mf) for mf in month_files]\n    with Pool(cpu_cores) as p:\n        df = p.starmap(readJsonFiles, args)\n    return pd.concat(df, ignore_index=True)\n</code></pre>"},{"location":"reference/data_formatter/#meowmotion.data_formatter.removeOutlier","title":"<code>removeOutlier(group)</code>","text":"<p>Filters outliers in the <code>speed</code> column by replacing high z-score values (\u2265 3) with the median speed. This function is typically applied to each group within a larger grouped DataFrame (e.g., a single trip trajectory).</p> How It Works <ol> <li>Calculates the median speed within the group.</li> <li>Identifies rows where <code>speed_z_score</code> is \u2265 3.</li> <li>Replaces those outlier <code>speed</code> values with the median speed.</li> <li>Returns the modified group DataFrame.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>DataFrame</code> <p>Subset of a larger DataFrame, typically representing one trip. Must contain at least: - <code>speed</code>: The speed values to check. - <code>speed_z_score</code>: The corresponding z-score values for speed.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The same DataFrame with outlier speeds replaced by the median.</p> Example <p>import pandas as pd import numpy as np data = { ...     'speed': [5.0, 120.0, 6.0], ...     'speed_z_score': [0.2, 3.5, 0.3] ... } df = pd.DataFrame(data) print(df)      speed  speed_z_score 0     5.0            0.20 1   120.0            3.50 2     6.0            0.30</p> <p>cleaned = removeOutlier(df) print(cleaned)      speed  speed_z_score 0     5.0            0.20 1     5.5            3.50  # replaced with median (5.5) 2     6.0            0.30</p> Source code in <code>meowmotion/data_formatter.py</code> <pre><code>def removeOutlier(group: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters outliers in the `speed` column by replacing high z-score values (\u2265 3) with\n    the median speed. This function is typically applied to each group within a larger\n    grouped DataFrame (e.g., a single trip trajectory).\n\n    How It Works:\n        1. Calculates the median speed within the group.\n        2. Identifies rows where `speed_z_score` is \u2265 3.\n        3. Replaces those outlier `speed` values with the median speed.\n        4. Returns the modified group DataFrame.\n\n    Args:\n        group (pd.DataFrame): Subset of a larger DataFrame, typically representing\n            one trip. Must contain at least:\n            - `speed`: The speed values to check.\n            - `speed_z_score`: The corresponding z-score values for speed.\n\n    Returns:\n        pd.DataFrame: The same DataFrame with outlier speeds replaced by the median.\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; data = {\n        ...     'speed': [5.0, 120.0, 6.0],\n        ...     'speed_z_score': [0.2, 3.5, 0.3]\n        ... }\n        &gt;&gt;&gt; df = pd.DataFrame(data)\n        &gt;&gt;&gt; print(df)\n             speed  speed_z_score\n        0     5.0            0.20\n        1   120.0            3.50\n        2     6.0            0.30\n\n        &gt;&gt;&gt; cleaned = removeOutlier(df)\n        &gt;&gt;&gt; print(cleaned)\n             speed  speed_z_score\n        0     5.0            0.20\n        1     5.5            3.50  # replaced with median (5.5)\n        2     6.0            0.30\n    \"\"\"\n\n    group_speed = group[\"speed\"]\n    group_z_score = group[\"speed_z_score\"]\n    group_median_speed = np.median(group_speed)\n    group_speed[group_z_score &gt;= 3] = group_median_speed\n    group[\"speed\"] = group_speed\n    return group\n</code></pre>"},{"location":"reference/meowmob/","title":"Meowmob","text":""},{"location":"reference/meowmob/#meowmotion.meowmob.generateOD","title":"<code>generateOD(trip_df, shape, active_day_df, hldf, adult_population, output_dir, org_loc_cols=('org_lng', 'org_lat'), dest_loc_cols=('dest_lng', 'dest_lat'), cpu_cores=max(1, cpu_count() // 2), save_drived_products=True, od_type=['type3'])</code>","text":"<p>Generate weighted Origin-Destination (OD) matrices from trip-level data, using spatial joins, demographic weights, and user activity data. This function leverages multiprocessing to handle large datasets efficiently and can produce multiple types of OD matrices in a single pass.</p> <p>Key Steps: 1. Shape File Preparation:    - Ensures the provided <code>shape</code> GeoDataFrame uses EPSG:4326.    - Pre-builds a spatial index for quicker joins.</p> <ol> <li>Spatial Joins:</li> <li>Splits <code>trip_df</code> into load-balanced buckets (via <code>getLoadBalancedBuckets</code>)      for parallel processing.</li> <li> <p>Spatially joins origins and destinations against the <code>shape</code> to label each      trip with \"origin_geo_code\" and \"destination_geo_code\".</p> </li> <li> <p>Filtering:</p> </li> <li>Removes trips longer than 24 hours and stay durations over 3600 minutes.</li> <li> <p>Drops records without valid origin or destination geo-codes.</p> </li> <li> <p>Disclosure Analysis:</p> </li> <li>Aggregates trip counts by origin-destination pairs and user IDs to help      identify any potential risk of user-level data disclosure.</li> <li> <p>Saves results in \"disclosure_analysis_.csv\".</p> </li> <li> <p>Trip ID &amp; Metrics:</p> </li> <li>Assigns incremental <code>trip_id</code>s per user.</li> <li> <p>Computes total trips per user and merges with <code>active_day_df</code> to      calculate \"trips per active day\" (TPAD).</p> </li> <li> <p>Adding Demographic Data:</p> </li> <li>Merges each record with user-level IMD quintiles and council info      from <code>hldf</code>.</li> <li> <p>Adds placeholder columns for travel mode if needed.</p> </li> <li> <p>Optional Saving of Intermediate Products (if <code>save_drived_products=True</code>):</p> </li> <li> <p>Saves non-aggregated flows, aggregated flows, stay points, and trip points      in separate CSV files for further analysis.</p> </li> <li> <p>Final OD Matrix Generation:</p> </li> <li>Filters out infrequent or low-activity users based on active days and TPAD.</li> <li>For each OD type in <code>od_type</code> (e.g., \"type1\", \"type2\", \"type3\", \"type4\"),      selects trips matching the time-of-day/week criteria.</li> <li>Applies weighting (<code>getWeights</code>) to scale user trip counts to      population-level estimates.</li> <li>Aggregates trips, then calculates weighted trips with different weighting      factors (activity, council, IMD) for each origin-destination pair.</li> <li>Saves the resulting OD matrix as a CSV (e.g., \"type3_od.csv\") and collects      it in a list of OD DataFrames to be returned.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>trip_df</code> <code>DataFrame</code> <p>The main trip-level DataFrame. Must contain columns indicating user IDs, timestamps (arrivals/departures), plus the origin/destination lat-lng pairs (specified by <code>org_loc_cols</code> and <code>dest_loc_cols</code>).</p> required <code>shape</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing the geographic boundaries (e.g., MSOA or LSOA). Must have a valid geometry column. This is used for spatial joins.</p> required <code>active_day_df</code> <code>DataFrame</code> <p>DataFrame with columns [\"uid\", \"total_active_days\"], representing how many days each user was active.</p> required <code>hldf</code> <code>DataFrame</code> <p>DataFrame mapping user IDs to home council and IMD quintile info.</p> required <code>adult_population</code> <code>DataFrame</code> <p>Contains population counts broken down by council and IMD quintile.</p> required <code>output_dir</code> <code>str</code> <p>Directory path where all output files will be saved.</p> required <code>org_loc_cols</code> <code>Tuple[str, str]</code> <p>Column names for the origin's (longitude, latitude). Defaults to (\"org_lng\", \"org_lat\").</p> <code>('org_lng', 'org_lat')</code> <code>dest_loc_cols</code> <code>Tuple[str, str]</code> <p>Column names for the destination's (longitude, latitude). Defaults to (\"dest_lng\", \"dest_lat\").</p> <code>('dest_lng', 'dest_lat')</code> <code>cpu_cores</code> <code>int</code> <p>Number of CPU cores to use for parallel processing. Defaults to half of available cores (at least 1).</p> <code>max(1, cpu_count() // 2)</code> <code>save_drived_products</code> <code>bool</code> <p>Whether to save intermediate or \"derived\" datasets (e.g., stay points). Defaults to True.</p> <code>True</code> <code>od_type</code> <code>List[str]</code> <p>Which OD matrix types to produce. Recognized values: - \"type1\": AM Peak Weekdays (7am\u201310am) - \"type2\": PM Peak Weekdays (4pm\u20137pm) - \"type3\": All Trips (default) - \"type4\": All Trips excluding type1 + type2 Passing multiple values produces multiple OD DataFrames. Defaults to [\"type3\"].</p> <code>['type3']</code> <p>Returns:</p> Type Description <code>List[DataFrame]</code> <p>List[pd.DataFrame]: A list of OD matrix DataFrames, one for each type listed in <code>od_type</code>. Each DataFrame has columns like: - \"origin_geo_code\", \"destination_geo_code\" - \"trips\" (unweighted) - \"activity_weighted_trips\" - \"council_weighted_trips\" - \"act_cncl_weighted_trips\" (combined weighting) - \"percentage\" (percentage share of total trips)</p> Example <p>from meowmotion.meowmob import generateOD od_matrices = generateOD(         trip_df=trip_data,         shape=lsoa_shapes,         active_day_df=active_days,         hldf=home_locations,         adult_population=population_stats,         org_loc_cols=('org_lng', 'org_lat'),         dest_loc_cols=('dest_lng', 'dest_lat'),         output_dir='./output',         cpu_cores=4,         od_type=[\"type3\", \"type1\"]     ) print(od_matrices[0].head())  # OD matrix for \"type3\"</p> Source code in <code>meowmotion/meowmob.py</code> <pre><code>def generateOD(\n    trip_df: pd.DataFrame,\n    shape: gpd.GeoDataFrame,\n    active_day_df: pd.DataFrame,\n    hldf: pd.DataFrame,\n    adult_population: pd.DataFrame,\n    output_dir: str,\n    org_loc_cols: Optional[Tuple[str, str]] = (\"org_lng\", \"org_lat\"),  # (lng, lat)\n    dest_loc_cols: Optional[Tuple[str, str]] = (\"dest_lng\", \"dest_lat\"),  # (lng, lat)\n    cpu_cores: Optional[int] = max(1, cpu_count() // 2),\n    save_drived_products: Optional[bool] = True,\n    od_type: Optional[List[str]] = [\"type3\"],\n) -&gt; List[pd.DataFrame]:\n    \"\"\"\n    Generate weighted Origin-Destination (OD) matrices from trip-level data, using\n    spatial joins, demographic weights, and user activity data. This function\n    leverages multiprocessing to handle large datasets efficiently and can produce\n    multiple types of OD matrices in a single pass.\n\n    Key Steps:\n    1. **Shape File Preparation**:\n       - Ensures the provided `shape` GeoDataFrame uses EPSG:4326.\n       - Pre-builds a spatial index for quicker joins.\n\n    2. **Spatial Joins**:\n       - Splits `trip_df` into load-balanced buckets (via `getLoadBalancedBuckets`)\n         for parallel processing.\n       - Spatially joins origins and destinations against the `shape` to label each\n         trip with \"origin_geo_code\" and \"destination_geo_code\".\n\n    3. **Filtering**:\n       - Removes trips longer than 24 hours and stay durations over 3600 minutes.\n       - Drops records without valid origin or destination geo-codes.\n\n    4. **Disclosure Analysis**:\n       - Aggregates trip counts by origin-destination pairs and user IDs to help\n         identify any potential risk of user-level data disclosure.\n       - Saves results in \"disclosure_analysis_.csv\".\n\n    5. **Trip ID &amp; Metrics**:\n       - Assigns incremental `trip_id`s per user.\n       - Computes total trips per user and merges with `active_day_df` to\n         calculate \"trips per active day\" (TPAD).\n\n    6. **Adding Demographic Data**:\n       - Merges each record with user-level IMD quintiles and council info\n         from `hldf`.\n       - Adds placeholder columns for travel mode if needed.\n\n    7. **Optional Saving of Intermediate Products** (if `save_drived_products=True`):\n       - Saves non-aggregated flows, aggregated flows, stay points, and trip points\n         in separate CSV files for further analysis.\n\n    8. **Final OD Matrix Generation**:\n       - Filters out infrequent or low-activity users based on active days and TPAD.\n       - For each OD type in `od_type` (e.g., \"type1\", \"type2\", \"type3\", \"type4\"),\n         selects trips matching the time-of-day/week criteria.\n       - Applies weighting (`getWeights`) to scale user trip counts to\n         population-level estimates.\n       - Aggregates trips, then calculates weighted trips with different weighting\n         factors (activity, council, IMD) for each origin-destination pair.\n       - Saves the resulting OD matrix as a CSV (e.g., \"type3_od.csv\") and collects\n         it in a list of OD DataFrames to be returned.\n\n    Args:\n        trip_df (pd.DataFrame):\n            The main trip-level DataFrame. Must contain columns indicating user IDs,\n            timestamps (arrivals/departures), plus the origin/destination lat-lng\n            pairs (specified by `org_loc_cols` and `dest_loc_cols`).\n        shape (gpd.GeoDataFrame):\n            A GeoDataFrame containing the geographic boundaries (e.g., MSOA or LSOA).\n            Must have a valid geometry column. This is used for spatial joins.\n        active_day_df (pd.DataFrame):\n            DataFrame with columns [\"uid\", \"total_active_days\"], representing how\n            many days each user was active.\n        hldf (pd.DataFrame):\n            DataFrame mapping user IDs to home council and IMD quintile info.\n        adult_population (pd.DataFrame):\n            Contains population counts broken down by council and IMD quintile.\n        output_dir (str):\n            Directory path where all output files will be saved.\n        org_loc_cols (Tuple[str, str], optional):\n            Column names for the origin's (longitude, latitude).\n            Defaults to (\"org_lng\", \"org_lat\").\n        dest_loc_cols (Tuple[str, str], optional):\n            Column names for the destination's (longitude, latitude).\n            Defaults to (\"dest_lng\", \"dest_lat\").\n        cpu_cores (int, optional):\n            Number of CPU cores to use for parallel processing. Defaults to half\n            of available cores (at least 1).\n        save_drived_products (bool, optional):\n            Whether to save intermediate or \"derived\" datasets (e.g., stay points).\n            Defaults to True.\n        od_type (List[str], optional):\n            Which OD matrix types to produce. Recognized values:\n            - \"type1\": AM Peak Weekdays (7am\u201310am)\n            - \"type2\": PM Peak Weekdays (4pm\u20137pm)\n            - \"type3\": All Trips (default)\n            - \"type4\": All Trips excluding type1 + type2\n            Passing multiple values produces multiple OD DataFrames. Defaults to [\"type3\"].\n\n    Returns:\n        List[pd.DataFrame]:\n            A list of OD matrix DataFrames, one for each type listed in `od_type`.\n            Each DataFrame has columns like:\n            - \"origin_geo_code\", \"destination_geo_code\"\n            - \"trips\" (unweighted)\n            - \"activity_weighted_trips\"\n            - \"council_weighted_trips\"\n            - \"act_cncl_weighted_trips\" (combined weighting)\n            - \"percentage\" (percentage share of total trips)\n\n    Example:\n        &gt;&gt;&gt; from meowmotion.meowmob import generateOD\n        &gt;&gt;&gt; od_matrices = generateOD(\n                trip_df=trip_data,\n                shape=lsoa_shapes,\n                active_day_df=active_days,\n                hldf=home_locations,\n                adult_population=population_stats,\n                org_loc_cols=('org_lng', 'org_lat'),\n                dest_loc_cols=('dest_lng', 'dest_lat'),\n                output_dir='./output',\n                cpu_cores=4,\n                od_type=[\"type3\", \"type1\"]\n            )\n        &gt;&gt;&gt; print(od_matrices[0].head())  # OD matrix for \"type3\"\n    \"\"\"\n\n    print(f\"{datetime.now()}: Current CRS {shape.crs}\")\n    shape = shape.to_crs(\"EPSG:4326\")\n    print(f\"{datetime.now()}: CRS after Conversion: {shape.crs}\")\n    print(f\"{datetime.now()}: Indexing Shape File\")\n    shape.sindex\n\n    #############################################################\n    #                                                           #\n    #                   Spatial Join for Origin                 #\n    #                                                           #\n    #############################################################\n\n    df_collection = getLoadBalancedBuckets(trip_df, cpu_cores)\n    print(f\"{datetime.now()}: Spatial Join for Origin Started\")\n    # args=[(tdf, shape, 'org_lng', 'org_lat', 'origin') for tdf in df_collection]\n    args = [\n        (tdf, shape, org_loc_cols[0], org_loc_cols[1], \"origin\")\n        for tdf in df_collection\n    ]\n    with Pool(cpu_cores) as pool:\n        results = pool.starmap(spatialJoin, args)\n    df_collection = [*results]\n    print(f\"{datetime.now()}: Spatial Join for Origin Finished\")\n\n    #############################################################\n    #                                                           #\n    #                  Spatial Join for Destination             #\n    #                                                           #\n    #############################################################\n\n    print(f\"{datetime.now()}: Spatial Join for Destination Started\")\n    # args=[(tdf, shape, 'dest_lng', 'dest_lat', 'destination') for tdf in df_collection]\n    args = [\n        (tdf, shape, dest_loc_cols[0], dest_loc_cols[1], \"destination\")\n        for tdf in df_collection\n    ]\n    with Pool(cpu_cores) as pool:\n        results = pool.starmap(spatialJoin, args)\n    geo_df = pd.concat([*results])\n    del results\n    print(f\"{datetime.now()}: Spatial Join for Destination Finished\")\n    #############################################################\n    #                                                           #\n    # Filtering trips based on travel time and stay duration    #\n    #                                                           #\n    #############################################################\n\n    print(f\"{datetime.now()}: Filtering on Travel Time and Stay Duration\")\n    geo_df = geo_df[\n        (geo_df[\"dest_arival_time\"] - geo_df[\"org_leaving_time\"]).dt.total_seconds()\n        / 3600\n        &lt;= 24\n    ]\n    geo_df = geo_df[geo_df[\"stay_duration\"] &lt;= 3600]\n    nusers = geo_df[\"uid\"].nunique()\n    print(f\"{datetime.now()}: Total Unique Users: {nusers}\")\n    geo_df[\"origin_geo_code\"] = geo_df[\"origin_geo_code\"].fillna(\"Others\")\n    geo_df[\"destination_geo_code\"] = geo_df[\"destination_geo_code\"].fillna(\"Others\")\n    geo_df = geo_df[geo_df[\"origin_geo_code\"] != \"Others\"]\n    geo_df = geo_df[geo_df[\"destination_geo_code\"] != \"Others\"]\n    print(f\"{datetime.now()}: Filtering Completed\")\n\n    #############################################################\n    #                                                           #\n    #                   Disclosure Analysis                     #\n    #                                                           #\n    #############################################################\n\n    print(f\"{datetime.now()}: Generating file for disclosure analysis\")\n    analysis_df = (\n        geo_df.groupby([\"origin_geo_code\", \"destination_geo_code\"])\n        .agg(\n            total_trips=pd.NamedAgg(column=\"uid\", aggfunc=\"count\"),\n            num_users=pd.NamedAgg(column=\"uid\", aggfunc=\"nunique\"),\n        )\n        .reset_index()\n    )\n\n    print(f\"{datetime.now()}: Saving disclosure analysis file\")\n    saveFile(\n        path=f\"{output_dir}/disclosure_analysis\",\n        fname=\"disclosure_analysis_.csv\",\n        df=analysis_df,\n    )\n    print(f\"{datetime.now()}: Saved disclosure analysis file\")\n\n    ############################################################\n    #                                                          #\n    #                   Adding Trip ID                         #\n    #                                                          #\n    ############################################################\n\n    print(f\"{datetime.now()}: Adding Trip ID\")\n    geo_df = geo_df.assign(\n        trip_id=lambda df: df.groupby([\"uid\"])[\"trip_time\"].transform(\n            lambda x: [i for i in range(1, len(x) + 1)]\n        )\n    )\n\n    # first_cols = ['uid', 'trip_id']\n    # other_cols = [col for col in geo_df.columns if col not in first_cols]\n    # geo_df = geo_df[first_cols + other_cols]\n\n    geo_df = geo_df[\n        [\n            \"uid\",\n            \"trip_id\",\n            \"org_lat\",\n            \"org_lng\",\n            \"org_arival_time\",\n            \"org_leaving_time\",\n            \"dest_lat\",\n            \"dest_lng\",\n            \"dest_arival_time\",\n            \"stay_points\",\n            \"trip_points\",\n            \"trip_time\",\n            \"stay_duration\",\n            \"observed_stay_duration\",\n            \"origin_geo_code\",\n            \"origin_name\",\n            \"destination_geo_code\",\n            \"destination_name\",\n        ]\n    ]\n    print(f\"{datetime.now()}: Trip ID Added\")\n\n    #############################################################\n    #                                                           #\n    #                    Calculate Total Trips/User             #\n    #                                                           #\n    #############################################################\n\n    print(f\"{datetime.now()}: Calculating Total Trips/User\")\n    # geo_df[\"month\"] = geo_df[\"org_leaving_time\"].dt.month\n    geo_df = geo_df.assign(\n        total_trips=lambda df: df.groupby(\"uid\")[\"trip_id\"].transform(lambda x: len(x))\n    )\n    # geo_df = geo_df.drop(columns=[\"month\"])\n    print(f\"{datetime.now()}: Trips/User Calculated\")\n\n    #############################################################\n    #                                                           #\n    #                   Add Trips/Active Day                    #\n    #                                                           #\n    #############################################################\n\n    print(f\"{datetime.now()}: Calculating TPAD\")\n    geo_df = geo_df.merge(active_day_df, how=\"left\", on=\"uid\").assign(\n        tpad=lambda tdf: tdf[\"total_trips\"] / tdf[\"total_active_days\"]\n    )\n    print(f\"{datetime.now()}: TPAD Calculated\")\n\n    #############################################################\n    #                                                           #\n    #                       Add IMD Level                       #\n    #                                                           #\n    #############################################################\n\n    print(f\"{datetime.now()}: Adding IMD\")\n    geo_df = geo_df.merge(\n        hldf[[\"uid\", \"council_name\", \"imd_quintile\"]], on=\"uid\", how=\"left\"\n    )[\n        [\n            \"uid\",\n            \"council_name\",\n            \"imd_quintile\",\n            \"trip_id\",\n            \"org_lat\",\n            \"org_lng\",\n            \"org_arival_time\",\n            \"org_leaving_time\",\n            \"dest_lat\",\n            \"dest_lng\",\n            \"origin_geo_code\",\n            \"destination_geo_code\",\n            \"dest_arival_time\",\n            \"stay_points\",\n            \"trip_points\",\n            \"trip_time\",\n            \"stay_duration\",\n            \"observed_stay_duration\",\n            \"total_trips\",\n            \"total_active_days\",\n            \"tpad\",\n        ]\n    ]\n\n    #############################################################\n    #                                                           #\n    #                Add Travel Mode Placeholder                #\n    #                                                           #\n    #############################################################\n\n    geo_df = geo_df.assign(travel_mode=np.nan)\n\n    if save_drived_products:\n\n        #############################################################\n        #                                                           #\n        #              Save Aggregated Flow                         #\n        #                                                           #\n        #############################################################\n\n        print(f\"{datetime.now()}: Saving Non-Aggregated OD Flow\")\n        saveFile(\n            path=f\"{output_dir}/na_flows\",\n            fname=\"na_flows.csv\",\n            df=geo_df[\n                [\n                    \"uid\",\n                    \"imd_quintile\",\n                    \"trip_id\",\n                    \"org_lat\",\n                    \"org_lng\",\n                    \"org_arival_time\",\n                    \"org_leaving_time\",\n                    \"dest_lat\",\n                    \"dest_lng\",\n                    \"dest_arival_time\",\n                    \"total_trips\",\n                    \"total_active_days\",\n                    \"tpad\",\n                    \"travel_mode\",\n                ]\n            ],\n        )\n        print(f\"{datetime.now()}: Non-Aggregated OD Flow Saved\")\n\n        #############################################################\n        #                                                           #\n        #              Save Aggregated Flow                         #\n        #                                                           #\n        #############################################################\n\n        print(f\"{datetime.now()}: Saving Aggregated OD Flow\")\n\n        saveFile(\n            path=f\"{output_dir}/agg_stay_points\",\n            fname=\"agg_stay_points.csv\",\n            df=geo_df[\n                [\n                    \"origin_geo_code\",\n                    \"destination_geo_code\",\n                    \"org_arival_time\",\n                    \"org_leaving_time\",\n                    \"dest_arival_time\",\n                    \"travel_mode\",\n                ]\n            ],\n        )\n        print(f\"{datetime.now()}: Aggregated OD Flow Saved\")\n\n        #############################################################\n        #                                                           #\n        #              Save Non Aggregated Stay Points              #\n        #                                                           #\n        #############################################################\n\n        print(f\"{datetime.now()}: Saving Non-Aggragated Stay Points\")\n\n        saveFile(\n            path=f\"{output_dir}/non_agg_stay_points\",\n            fname=\"non_agg_stay_points.csv\",\n            df=geo_df[\n                [\n                    \"uid\",\n                    \"imd_quintile\",\n                    \"stay_points\",\n                    \"org_arival_time\",\n                    \"org_leaving_time\",\n                    \"stay_duration\",\n                    \"org_lat\",\n                    \"org_lng\",\n                    \"total_active_days\",\n                ]\n            ].rename(\n                columns={\n                    \"org_lat\": \"centroid_lat\",  # Changing the name to centroid because stay points don't have origin and destination\n                    \"org_lng\": \"centroid_lng\",\n                    \"org_arival_time\": \"stop_node_arival_time\",\n                    \"org_leaving_time\": \"stop_node_leaving_time\",\n                }\n            ),\n        )\n\n        print(f\"{datetime.now()}: Non-Aggragated Stay Points Saved\")\n\n        #############################################################\n        #                                                           #\n        #                  Save Aggregated Stay Points              #\n        #                                                           #\n        #############################################################\n\n        print(f\"{datetime.now()}: Saving Aggragated Stay Points\")\n\n        saveFile(\n            path=f\"{output_dir}/agg_stay_points\",\n            fname=\"agg_stay_points.csv\",\n            df=geo_df[\n                [\n                    \"imd_quintile\",\n                    \"origin_geo_code\",\n                    \"org_arival_time\",\n                    \"org_leaving_time\",\n                    \"stay_duration\",\n                ]\n            ].rename(\n                columns={\n                    \"org_arival_time\": \"stop_node_arival_time\",\n                    \"org_leaving_time\": \"stop_node_leaving_time\",\n                    \"origin_geo_code\": \"stop_node_geo_code\",\n                }\n            ),\n        )\n\n        print(f\"{datetime.now()}: Aggragated Stay Points Saved\")\n\n        #############################################################\n        #                                                           #\n        #                      Save Trip Points                     #\n        #                                                           #\n        #############################################################\n\n        print(f\"{datetime.now()}: Saving Trips Points\")\n\n        saveFile(\n            path=f\"{output_dir}/trip_points\",\n            fname=\"trip_points.csv\",\n            df=geo_df[\n                [\n                    \"uid\",\n                    \"imd_quintile\",\n                    \"trip_id\",\n                    \"trip_points\",\n                    \"total_active_days\",\n                    \"travel_mode\",\n                ]\n            ],\n        )\n\n        print(f\"{datetime.now()}: Trips Points Saved\")\n\n    ##################################################################################\n    #                                                                                #\n    #                           OD Generation                                        #\n    #                                                                                #\n    ##################################################################################\n\n    print(f\"{datetime.now()}: OD Calculation Started\")\n    geo_df = geo_df[\n        (geo_df[\"total_active_days\"] &gt;= 7) &amp; (geo_df[\"tpad\"] &gt;= 0.2)\n    ]  # Filtering based on number of active days and trips/active day\n\n    print(f\"{datetime.now()}: Total Trips: {len(geo_df)}\")\n    print(f'{datetime.now()}: Total Users: {len(geo_df[\"uid\"].unique())}')\n    print(f'{datetime.now()}: TPAD Stats:\\n{geo_df[\"tpad\"].describe()}')\n    od_trip_df = pd.DataFrame(\n        geo_df.groupby([\"uid\", \"origin_geo_code\", \"destination_geo_code\"]).apply(\n            lambda x: len(x)\n        ),\n        columns=[\"trips\"],\n    ).reset_index()  # Get number of Trips between orgins and destination for individual users\n    od_trip_df = od_trip_df.merge(\n        active_day_df, how=\"left\", left_on=\"uid\", right_on=\"uid\"\n    ).assign(tpad=lambda tdf: tdf[\"trips\"] / tdf[\"total_active_days\"])\n\n    print(f\"{datetime.now()}: Calculating Weights\")\n    weighted_trips = getWeights(\n        geo_df,\n        hldf,\n        adult_population,\n        \"origin_geo_code\",\n        \"destination_geo_code\",\n        active_day_df,\n    )\n    weighted_trips = weighted_trips[\n        [\"uid\", \"imd_weight\", \"council_weight\", \"activity_weight\"]\n    ]\n    weighted_trips = weighted_trips.drop_duplicates(subset=\"uid\", keep=\"first\")\n    print(f\"{datetime.now()}: Weights Calculated\")\n    data_population = len(geo_df[\"uid\"].unique())  # Total number of users in the data\n    adult_population = adult_population[\"Total\"].sum()  # Total population\n\n    # Producing 5 Type of OD Matrices\n    # Type 1: AM peak weekdays (7am-10am)\n    # Type 2: PM peak weekdays (4 pm-7 pm)\n    # Type 3: Everything\n    # Type 4: Type 3 - (Type 1 + Type 2)\n\n    type_meta = {\n        \"type1\": \"AM Peak Weekdays (7am-10am)\",\n        \"type2\": \"PM Peak Weekdays (4 pm-7 pm)\",\n        \"type3\": \"All (Everything)\",\n        \"type4\": \"All - (AM Peak + PM Peak)\",\n    }\n    return_ods = []\n    for typ in od_type:\n        print(f\"{datetime.now()}: Generating {type_meta[typ]} OD Matrix\")\n        if typ == \"type1\":\n            geo_df_filtered = geo_df[\n                (geo_df[\"org_leaving_time\"].dt.hour &gt;= 7)\n                &amp; (geo_df[\"org_leaving_time\"].dt.hour &lt;= 10)\n                &amp; (geo_df[\"org_leaving_time\"].dt.dayofweek &lt; 5)\n            ]\n        elif typ == \"type2\":\n            geo_df_filtered = geo_df[\n                (geo_df[\"org_leaving_time\"].dt.hour &gt;= 16)\n                &amp; (geo_df[\"org_leaving_time\"].dt.hour &lt;= 19)\n                &amp; (geo_df[\"org_leaving_time\"].dt.dayofweek &lt; 5)\n            ]\n        elif typ == \"type3\":\n            geo_df_filtered = geo_df.copy()  # No filtering for type3\n        elif typ == \"type4\":\n            geo_df_filtered = geo_df[\n                ~(\n                    (geo_df[\"org_leaving_time\"].dt.hour &gt;= 7)\n                    &amp; (geo_df[\"org_leaving_time\"].dt.hour &lt;= 10)\n                    &amp; (geo_df[\"org_leaving_time\"].dt.dayofweek &lt; 5)\n                )\n            ]\n            geo_df = geo_df[\n                ~(\n                    (geo_df[\"org_leaving_time\"].dt.hour &gt;= 16)\n                    &amp; (geo_df[\"org_leaving_time\"].dt.hour &lt;= 19)\n                    &amp; (geo_df[\"org_leaving_time\"].dt.dayofweek &lt; 5)\n                )\n            ]\n        else:\n            raise ValueError(f\"Invalid OD type: {typ}. Must be one of {type_meta}.\")\n\n        print(f\"{datetime.now()}: Generating OD trip DF\")\n        od_trip_df = pd.DataFrame(\n            geo_df_filtered.groupby(\n                [\"uid\", \"origin_geo_code\", \"destination_geo_code\"]\n            ).apply(lambda x: len(x)),\n            columns=[\"trips\"],\n        ).reset_index()  # Get number of Trips between orgins and destination for individual users\n        print(f\"{datetime.now()}: Adding weights to OD trips\")\n        od_trip_df = od_trip_df.merge(\n            weighted_trips[[\"uid\", \"activity_weight\", \"imd_weight\", \"council_weight\"]],\n            how=\"left\",\n            on=\"uid\",\n        )\n        od_trip_df[\"imd_weight\"] = od_trip_df[\"imd_weight\"].fillna(1)\n        od_trip_df[\"council_weight\"] = od_trip_df[\"council_weight\"].fillna(1)\n        od_trip_df.reset_index(drop=True, inplace=True)\n        print(f\"{datetime.now()}: Aggregating trips\")\n        agg_od_df = (\n            od_trip_df.groupby([\"origin_geo_code\", \"destination_geo_code\"])\n            .agg(\n                trips=(\"trips\", \"sum\"),\n                activity_weighted_trips=(\n                    \"trips\",\n                    lambda x: (\n                        (x * od_trip_df.loc[x.index, \"activity_weight\"]).sum()\n                        / data_population\n                    )\n                    * adult_population,\n                ),\n                council_weighted_trips=(\n                    \"trips\",\n                    lambda x: (\n                        (\n                            x\n                            * od_trip_df.loc[x.index, \"imd_weight\"]\n                            * od_trip_df.loc[x.index, \"council_weight\"]\n                        ).sum()\n                        / data_population\n                    )\n                    * adult_population,\n                ),\n                act_cncl_weighted_trips=(\n                    \"trips\",\n                    lambda x: (\n                        (\n                            x\n                            * od_trip_df.loc[x.index, \"activity_weight\"]\n                            * od_trip_df.loc[x.index, \"imd_weight\"]\n                            * od_trip_df.loc[x.index, \"council_weight\"]\n                        ).sum()\n                        / data_population\n                    )\n                    * adult_population,\n                ),\n            )\n            .reset_index()\n        )\n\n        agg_od_df = agg_od_df[agg_od_df[\"origin_geo_code\"] != \"Others\"]\n        agg_od_df = agg_od_df[agg_od_df[\"destination_geo_code\"] != \"Others\"]\n\n        print(f\"{datetime.now()}: OD Generation Completed\")\n        print(f\"{datetime.now()}: Saving OD\")\n        agg_od_df[\"percentage\"] = (\n            agg_od_df[\"act_cncl_weighted_trips\"]\n            / agg_od_df[\"act_cncl_weighted_trips\"].sum()\n        ) * 100\n\n        agg_od_df = agg_od_df.rename(\n            columns={\"act_cncl_weighted_trips\": \"trips_weighted\"}\n        )\n        agg_od_df = agg_od_df[\n            [\n                \"origin_geo_code\",\n                \"destination_geo_code\",\n                \"trips\",\n                \"trips_weighted\",\n                \"percentage\",\n            ]\n        ]\n\n        saveFile(path=f\"{output_dir}/od_matrix\", fname=f\"{typ}_od.csv\", df=agg_od_df)\n        return_ods.append(agg_od_df)\n\n    return return_ods\n</code></pre>"},{"location":"reference/meowmob/#meowmotion.meowmob.getActivityStats","title":"<code>getActivityStats(df, output_dir, cpu_cores=max(1, int(cpu_count() / 2)))</code>","text":"<p>Compute per-month user activity (number of active days) in parallel and save to disk.</p> <p>This function partitions the input DataFrame into load-balanced buckets (based on unique users), processes each bucket in parallel, and then combines the results. Each row in the returned DataFrame corresponds to a specific user and month, with a column indicating how many days that user was active during that month.</p> <p>Key Points: - Requires at least the columns \"uid\" and \"datetime\" in the input DataFrame. - Uses multiprocessing to handle large datasets efficiently, controlled by <code>cpu_cores</code>. - Saves the final aggregated statistics to \"activity_stats.csv\" in the provided output directory. - The returned DataFrame has columns:     * \"uid\"     * \"month\"     * \"total_active_days\"  (number of unique days in that month with at least one record) - Designed to produce monthly-level stats from typically yearly data. If you need   a yearly total, aggregate \"total_active_days\" across all months per user before   using these stats in any further steps (like OD generation).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing at least the columns \"uid\" and \"datetime\".</p> required <code>output_dir</code> <code>str</code> <p>Path where the resulting \"activity_stats.csv\" file will be saved.</p> required <code>cpu_cores</code> <code>int</code> <p>Number of CPU cores to use for multiprocessing. Defaults to half of the available cores (at least 1).</p> <code>max(1, int(cpu_count() / 2))</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame of monthly user activity counts, with columns [\"uid\", \"month\", \"total_active_days\"].</p> Example <p>from meowmotion.meowmob import getActivityStats</p> Source code in <code>meowmotion/meowmob.py</code> <pre><code>def getActivityStats(\n    df: pd.DataFrame, output_dir: str, cpu_cores: int = max(1, int(cpu_count() / 2))\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute per-month user activity (number of active days) in parallel and save to disk.\n\n    This function partitions the input DataFrame into load-balanced buckets (based on\n    unique users), processes each bucket in parallel, and then combines the results.\n    Each row in the returned DataFrame corresponds to a specific user and month, with\n    a column indicating how many days that user was active during that month.\n\n    Key Points:\n    - Requires at least the columns \"uid\" and \"datetime\" in the input DataFrame.\n    - Uses multiprocessing to handle large datasets efficiently, controlled by `cpu_cores`.\n    - Saves the final aggregated statistics to \"activity_stats.csv\" in the provided output directory.\n    - The returned DataFrame has columns:\n        * \"uid\"\n        * \"month\"\n        * \"total_active_days\"  (number of unique days in that month with at least one record)\n    - Designed to produce monthly-level stats from typically yearly data. If you need\n      a yearly total, aggregate \"total_active_days\" across all months per user before\n      using these stats in any further steps (like OD generation).\n\n    Args:\n        df (pd.DataFrame):\n            The input DataFrame containing at least the columns \"uid\" and \"datetime\".\n        output_dir (str):\n            Path where the resulting \"activity_stats.csv\" file will be saved.\n        cpu_cores (int, optional):\n            Number of CPU cores to use for multiprocessing. Defaults to half of the\n            available cores (at least 1).\n\n    Returns:\n        pd.DataFrame:\n            A DataFrame of monthly user activity counts, with columns [\"uid\", \"month\",\n            \"total_active_days\"].\n\n    Example:\n        &gt;&gt;&gt; from meowmotion.meowmob import getActivityStats\n        &gt;&gt;&gt; # Suppose df has columns: uid, datetime, lat, lng, etc.\n        &gt;&gt;&gt; activity_df = getActivityStats(df, output_dir=\"./stats\", cpu_cores=4)\n        &gt;&gt;&gt; activity_df.head()\n           uid  month  total_active_days\n        0    1      1                 10\n        1    1      2                 12\n        2    2      1                  8\n    \"\"\"\n    print(f\"{datetime.now()}: Generating Activity Stats\")\n    init_unique_users = df[\"uid\"].nunique()\n    tdf_collection = getLoadBalancedBuckets(df, cpu_cores)\n    with Pool(cpu_cores) as p:\n        df = p.map(activityStats, tdf_collection)\n\n    df = pd.concat(df, ignore_index=True)\n    df = df.reset_index(drop=True)\n    final_unique_users = df[\"uid\"].nunique()\n    assert (\n        init_unique_users == final_unique_users\n    ), \"Something is wrong..data Loss in Activity Stats Generation\"\n    print(f\"{datetime.now()}: Activity Stats generated.\")\n    print(f\"{datetime.now()}: Saving Activity Stats\")\n    saveFile(path=f\"{output_dir}/activity_stats\", fname=\"activity_stats.csv\", df=df)\n    return df\n</code></pre>"},{"location":"reference/meowmob/#meowmotion.meowmob.getActivityStats--suppose-df-has-columns-uid-datetime-lat-lng-etc","title":"Suppose df has columns: uid, datetime, lat, lng, etc.","text":"<p>activity_df = getActivityStats(df, output_dir=\"./stats\", cpu_cores=4) activity_df.head()    uid  month  total_active_days 0    1      1                 10 1    1      2                 12 2    2      1                  8</p>"},{"location":"reference/meowmob/#meowmotion.meowmob.getStopNodes","title":"<code>getStopNodes(tdf, time_th=5, radius=500, cpu_cores=max(1, int(cpu_count() / 2)))</code>","text":"<p>Detect stop nodes from trajectory data in parallel using scikit-mobility's stay_locations.</p> <p>This function splits the input TrajDataFrame across multiple CPU cores (via getLoadBalancedBuckets), detects stops on each chunk using the stay_locations function, then merges the results back together. After detection, latitude and longitude columns are renamed to \"org_lat\" and \"org_lng\" in the final returned DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>tdf</code> <code>TrajDataFrame</code> <p>Input trajectory data with columns at least [\"uid\", \"datetime\", \"lat\", \"lng\"].</p> required <code>time_th</code> <code>int</code> <p>Time threshold (in minutes) used by stay_locations to detect a stop. Defaults to 5.</p> <code>5</code> <code>radius</code> <code>int</code> <p>Spatial radius (in meters) within which points are considered part of the same stop. Defaults to 500.</p> <code>500</code> <code>cpu_cores</code> <code>int</code> <p>Number of CPU cores to use for parallel processing. Defaults to half the available cores (at least 1).</p> <code>max(1, int(cpu_count() / 2))</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame representing the detected stop nodes. The main columns include: \"uid\", \"org_lat\", \"org_lng\", \"datetime\" (representing arrival time), \"leaving_datetime\", and any additional columns returned by stay_locations.</p> Example <p>from meowmotion.meowmob import getStopNodes stops_df = getStopNodes(tdf, time_th=10, radius=1000, cpu_cores=4) print(stops_df.head())</p> Source code in <code>meowmotion/meowmob.py</code> <pre><code>def getStopNodes(\n    tdf: TrajDataFrame,\n    time_th: Optional[int] = 5,\n    radius: Optional[int] = 500,\n    cpu_cores: Optional[int] = max(1, int(cpu_count() / 2)),\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Detect stop nodes from trajectory data in parallel using scikit-mobility's stay_locations.\n\n    This function splits the input TrajDataFrame across multiple CPU cores (via getLoadBalancedBuckets),\n    detects stops on each chunk using the stay_locations function, then merges the results back together.\n    After detection, latitude and longitude columns are renamed to \"org_lat\" and \"org_lng\" in the final\n    returned DataFrame.\n\n    Args:\n        tdf (TrajDataFrame):\n            Input trajectory data with columns at least [\"uid\", \"datetime\", \"lat\", \"lng\"].\n        time_th (int, optional):\n            Time threshold (in minutes) used by stay_locations to detect a stop. Defaults to 5.\n        radius (int, optional):\n            Spatial radius (in meters) within which points are considered part of the same stop. Defaults to 500.\n        cpu_cores (int, optional):\n            Number of CPU cores to use for parallel processing. Defaults to half the available cores (at least 1).\n\n    Returns:\n        pd.DataFrame:\n            A DataFrame representing the detected stop nodes. The main columns include:\n            \"uid\", \"org_lat\", \"org_lng\", \"datetime\" (representing arrival time), \"leaving_datetime\",\n            and any additional columns returned by stay_locations.\n\n    Example:\n        &gt;&gt;&gt; from meowmotion.meowmob import getStopNodes\n        &gt;&gt;&gt; stops_df = getStopNodes(tdf, time_th=10, radius=1000, cpu_cores=4)\n        &gt;&gt;&gt; print(stops_df.head())\n    \"\"\"\n    tdf = tdf.reset_index(drop=True)\n    tdf_collection = getLoadBalancedBuckets(tdf, cpu_cores)\n    print(f\"{datetime.now()}: Stop Node Detection Started\")\n    args = [(df, time_th, radius) for df in tdf_collection if not df.empty]\n    with Pool(cpu_cores) as pool:\n        results = pool.starmap(stopNodes, args)\n\n    del tdf_collection  # Deleting the data to free up the memory\n    stdf = pd.concat([*results])\n    del results  # Deleting the results to free up the memory\n    stdf.rename(columns={\"lat\": \"org_lat\", \"lng\": \"org_lng\"}, inplace=True)\n    stdf = pd.DataFrame(stdf)\n    stdf = stdf.drop(columns=[\"impression_acc\"])\n    print(f\"{datetime.now()} Stop Node Detection Completed\\n\")\n    return stdf\n</code></pre>"},{"location":"reference/meowmob/#meowmotion.meowmob.getWeights","title":"<code>getWeights(geo_df, hldf, adult_population, origin_col, destination_col, active_day_df)</code>","text":"<p>Computes activity-based, IMD-level, and council-level weights for users to scale observed trips to population-level estimates.</p> <p>Parameters:</p> Name Type Description Default <code>geo_df</code> <code>DataFrame</code> <p>Geo-tagged trip DataFrame containing user ID and trip counts.</p> required <code>hldf</code> <code>DataFrame</code> <p>Home location and demographic info including IMD and council.</p> required <code>adult_population</code> <code>DataFrame</code> <p>Population statistics broken down by IMD and council.</p> required <code>origin_col</code> <code>str</code> <p>Name of the column containing origin geo code.</p> required <code>destination_col</code> <code>str</code> <p>Name of the column containing destination geo code.</p> required <code>active_day_df</code> <code>DataFrame</code> <p>DataFrame with total number of active days per user.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with user-level weights including: - <code>imd_weight</code> - <code>council_weight</code> - <code>activity_weight</code></p> Example <p>weighted_df = getWeights(         geo_df=geo_enriched_data,         hldf=home_locations,         adult_population=population_stats,         origin_col=\"origin_geo_code\",         destination_col=\"destination_geo_code\",         active_day_df=active_days     ) print(weighted_df[['uid', 'activity_weight', 'imd_weight']].head())</p> Source code in <code>meowmotion/meowmob.py</code> <pre><code>def getWeights(\n    geo_df: pd.DataFrame,\n    hldf: pd.DataFrame,\n    adult_population: pd.DataFrame,\n    origin_col: str,\n    destination_col: str,\n    active_day_df: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes activity-based, IMD-level, and council-level weights for users to scale\n    observed trips to population-level estimates.\n\n    Args:\n        geo_df (pd.DataFrame): Geo-tagged trip DataFrame containing user ID and trip counts.\n        hldf (pd.DataFrame): Home location and demographic info including IMD and council.\n        adult_population (pd.DataFrame): Population statistics broken down by IMD and council.\n        origin_col (str): Name of the column containing origin geo code.\n        destination_col (str): Name of the column containing destination geo code.\n        active_day_df (pd.DataFrame): DataFrame with total number of active days per user.\n\n    Returns:\n        pd.DataFrame: DataFrame with user-level weights including:\n            - `imd_weight`\n            - `council_weight`\n            - `activity_weight`\n\n    Example:\n        &gt;&gt;&gt; weighted_df = getWeights(\n                geo_df=geo_enriched_data,\n                hldf=home_locations,\n                adult_population=population_stats,\n                origin_col=\"origin_geo_code\",\n                destination_col=\"destination_geo_code\",\n                active_day_df=active_days\n            )\n        &gt;&gt;&gt; print(weighted_df[['uid', 'activity_weight', 'imd_weight']].head())\n    \"\"\"\n    od_trip_df = pd.DataFrame(\n        geo_df.groupby([\"uid\", origin_col, destination_col]).apply(lambda x: len(x)),\n        columns=[\"trips\"],\n    ).reset_index()  # Get number of Trips between orgins and destination for individual users\n    od_trip_df = od_trip_df.merge(active_day_df, how=\"left\", on=\"uid\").assign(\n        tpad=lambda tdf: tdf[\"trips\"] / tdf[\"total_active_days\"]\n    )\n    od_trip_df = pd.merge(\n        od_trip_df, hldf[[\"uid\", \"council_name\", \"imd_quintile\"]], how=\"left\", on=\"uid\"\n    )\n    od_trip_df = od_trip_df.rename(columns={\"council_name\": \"user_home_location\"})\n\n    # Calculating Weights Based in Adult Population and data Population\n\n    annual_users = (\n        od_trip_df.dropna(subset=[\"imd_quintile\"])\n        .groupby([\"user_home_location\", \"imd_quintile\"])\n        .agg(users=(\"uid\", \"nunique\"))\n        .reset_index()\n        .merge(\n            adult_population,\n            left_on=[\"user_home_location\", \"imd_quintile\"],\n            right_on=[\"council\", \"imd_quintile\"],\n            how=\"left\",\n        )\n        .groupby(\"user_home_location\", group_keys=True)\n        .apply(\n            lambda group: group.assign(\n                data_percent=group[\"users\"] / group[\"users\"].sum()\n            )\n        )\n        .reset_index(drop=True)\n        .assign(imd_weight=lambda df: df[\"Percentage\"] / df[\"data_percent\"])\n        .groupby(\"user_home_location\", group_keys=True)\n        .apply(\n            lambda group: group.assign(\n                total_pop=group[\"Total\"].sum(), data_pop=group[\"users\"].sum()\n            )\n        )\n        .reset_index(drop=True)\n        .assign(\n            council_weight=lambda df: (df[\"total_pop\"] / df[\"Total\"].sum())\n            / (df[\"data_pop\"] / df[\"users\"].sum())\n        )\n    )\n\n    annual_users = annual_users[  # Rearranging Columns\n        [\n            \"council\",\n            \"imd_quintile\",\n            \"users\",\n            \"Total\",\n            \"Percentage\",\n            \"data_percent\",\n            \"total_pop\",\n            \"data_pop\",\n            \"imd_weight\",\n            \"council_weight\",\n        ]\n    ]\n\n    annual_users = annual_users.rename(\n        columns={\n            \"users\": \"data_user_imd_level\",\n            \"Total\": \"adult_pop_imd_level\",\n            \"percentage\": \"adult_pop_percentage_imd_level\",\n            \"data_percent\": \"data_users_percentage_imd_level\",\n            \"total_pop\": \"adult_pop_council_level\",\n            \"data_pop\": \"data_users_council_level\",\n        }\n    )\n\n    od_trip_df = od_trip_df.merge(\n        annual_users[[\"council\", \"imd_quintile\", \"imd_weight\", \"council_weight\"]],\n        how=\"left\",\n        left_on=[\"user_home_location\", \"imd_quintile\"],\n        right_on=[\"council\", \"imd_quintile\"],\n        suffixes=[\"_od\", \"_anu\"],\n    )\n    od_trip_df[\"imd_weight\"] = od_trip_df[\"imd_weight\"].fillna(1)\n    od_trip_df[\"council_weight\"] = od_trip_df[\"council_weight\"].fillna(1)\n    od_trip_df[\"activity_weight\"] = (\n        365 / od_trip_df[\"total_active_days\"]\n    )  # Activity weight = 365 (total days in a year) / number of active days\n    return od_trip_df\n</code></pre>"},{"location":"reference/meowmob/#meowmotion.meowmob.processFlowGeneration","title":"<code>processFlowGeneration(stdf, raw_df, cpu_cores=max(1, int(cpu_count() / 2)))</code>","text":"<p>Generate flow data from stop nodes using parallel processing.</p> This function takes two data sources <ol> <li><code>stdf</code>: A DataFrame of stop nodes, which must contain columns such as     \"uid\", \"datetime\", \"org_lat\", \"org_lng\", and the \"dest_*\" fields added here.</li> <li><code>raw_df</code>: The underlying trajectory data (with columns like \"uid\", \"datetime\",     \"lat\", \"lng\") from which the detailed trip segments and stay points are extracted.</li> </ol> <p>The function first prepares <code>stdf</code> by assigning \"dest_at\", \"dest_lat\", and \"dest_lng\" (the next stop in sequence for each user), then uses <code>getLoadBalancedBuckets</code> to split the DataFrame for multiprocessing. For each split/bucket, it calls <code>flowGenration(...)</code> in parallel to build the trip segments and stay details from the raw data. Finally, it concatenates the partial results and returns a single DataFrame of flow data.</p> Columns in the final flow DataFrame typically include <ul> <li>\"uid\"</li> <li>\"org_lat\", \"org_lng\", \"org_arival_time\", \"org_leaving_time\"</li> <li>\"dest_lat\", \"dest_lng\", \"dest_arival_time\"</li> <li>\"stay_points\", \"trip_points\"</li> <li>\"trip_time\", \"stay_duration\", \"observed_stay_duration\" (and any other columns you choose to include in <code>flowGenration</code>)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>stdf</code> <code>DataFrame</code> <p>DataFrame containing stop nodes. Must have columns such as \"uid\", \"datetime\", \"org_lat\", \"org_lng\". Additional columns will be created or renamed (e.g., \"dest_lat\", \"dest_lng\", \"dest_at\").</p> required <code>raw_df</code> <code>DataFrame</code> <p>The raw trajectory data with columns like \"uid\", \"datetime\", \"lat\", \"lng\". Used to extract trip details.</p> required <code>cpu_cores</code> <code>int</code> <p>Number of CPU cores for multiprocessing. Defaults to half of available cores, at minimum 1.</p> <code>max(1, int(cpu_count() / 2))</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A concatenation of all flows generated in parallel.</p> <code>DataFrame</code> <p>Each row represents a trip between one stop node and the next.</p> Example <p>from meowmotion.meowmob import getStopNodes, processFlowGeneration stop_nodes_df = getStopNodes(traj_df) flow_data = processFlowGeneration(stop_nodes_df, raw_df, cpu_cores=4) print(flow_data.head())</p> Source code in <code>meowmotion/meowmob.py</code> <pre><code>def processFlowGeneration(\n    stdf: pd.DataFrame,\n    raw_df: pd.DataFrame,\n    cpu_cores: int = max(1, int(cpu_count() / 2)),\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate flow data from stop nodes using parallel processing.\n\n    This function takes two data sources:\n     1. `stdf`: A DataFrame of stop nodes, which must contain columns such as\n         \"uid\", \"datetime\", \"org_lat\", \"org_lng\", and the \"dest_*\" fields added here.\n     2. `raw_df`: The underlying trajectory data (with columns like \"uid\", \"datetime\",\n         \"lat\", \"lng\") from which the detailed trip segments and stay points are extracted.\n\n    The function first prepares `stdf` by assigning \"dest_at\", \"dest_lat\", and \"dest_lng\"\n    (the next stop in sequence for each user), then uses `getLoadBalancedBuckets` to split\n    the DataFrame for multiprocessing. For each split/bucket, it calls `flowGenration(...)`\n    in parallel to build the trip segments and stay details from the raw data. Finally,\n    it concatenates the partial results and returns a single DataFrame of flow data.\n\n    Columns in the final flow DataFrame typically include:\n      - \"uid\"\n      - \"org_lat\", \"org_lng\", \"org_arival_time\", \"org_leaving_time\"\n      - \"dest_lat\", \"dest_lng\", \"dest_arival_time\"\n      - \"stay_points\", \"trip_points\"\n      - \"trip_time\", \"stay_duration\", \"observed_stay_duration\"\n      (and any other columns you choose to include in `flowGenration`)\n\n    Args:\n        stdf (pd.DataFrame): DataFrame containing stop nodes. Must have columns such as\n            \"uid\", \"datetime\", \"org_lat\", \"org_lng\". Additional columns will be created\n            or renamed (e.g., \"dest_lat\", \"dest_lng\", \"dest_at\").\n        raw_df (pd.DataFrame): The raw trajectory data with columns like\n            \"uid\", \"datetime\", \"lat\", \"lng\". Used to extract trip details.\n        cpu_cores (int, optional): Number of CPU cores for multiprocessing.\n            Defaults to half of available cores, at minimum 1.\n\n    Returns:\n        pd.DataFrame: A concatenation of all flows generated in parallel.\n        Each row represents a trip between one stop node and the next.\n\n    Example:\n        &gt;&gt;&gt; from meowmotion.meowmob import getStopNodes, processFlowGeneration\n        &gt;&gt;&gt; stop_nodes_df = getStopNodes(traj_df)\n        &gt;&gt;&gt; flow_data = processFlowGeneration(stop_nodes_df, raw_df, cpu_cores=4)\n        &gt;&gt;&gt; print(flow_data.head())\n    \"\"\"\n\n    stdf[\"dest_at\"] = stdf.groupby(\"uid\")[\"datetime\"].transform(lambda x: x.shift(-1))\n    stdf[\"dest_lat\"] = stdf.groupby(\"uid\")[\"org_lat\"].transform(lambda x: x.shift(-1))\n    stdf[\"dest_lng\"] = stdf.groupby(\"uid\")[\"org_lng\"].transform(lambda x: x.shift(-1))\n    print(stdf.head())  # Printing the first 5 rows of the stop nodes data\n    stdf = stdf.dropna(subset=[\"dest_lat\"])\n    tdf_collection = getLoadBalancedBuckets(stdf, cpu_cores)\n    print(f\"{datetime.now()}: Generating args\")\n    args = []\n    for tdf in tdf_collection:\n        temp_raw_df = raw_df[raw_df[\"uid\"].isin(tdf[\"uid\"].unique())].copy()\n        temp_raw_df.set_index([\"uid\", \"datetime\"], inplace=True)\n        temp_raw_df.sort_index(inplace=True)\n        args.append((tdf, temp_raw_df))\n    del tdf_collection\n    print(f\"{datetime.now()}: args Generation Completed\")\n    print(f\"{datetime.now()}: Flow Generation Started\\n\\n\")\n    with Pool(cpu_cores) as pool:\n        results = pool.starmap(flowGenration, args)\n\n    flow_df = pd.concat(\n        [*results]\n    )  # Concatinating the flow data from all the processes\n    del results  # Deleting the results to free up the memory\n    print(f\"{datetime.now()} Flow Generation Completed\\n\")\n    return flow_df\n</code></pre>"},{"location":"reference/model_tmd/","title":"Model TMD","text":""},{"location":"reference/model_tmd/#meowmotion.model_tmd.modePredict","title":"<code>modePredict(artifacts_dir, model_file_name, le_file_name, processed_non_agg_data, stats_agg_data, shape_file, output_dir=None)</code>","text":"<p>Predict travel modes for every trip, attach predictions to point-level data, add origin/destination geographic codes, and (optionally) save the results.</p> Workflow <ol> <li>Load the trained classifier and its label encoder from artifacts_dir.</li> <li>Read the study-area shapefile and re-project to WGS 84 (EPSG 4326).</li> <li>Build the model-input feature set from stats_agg_data and predict the    travel mode for each trip.</li> <li>Merge predictions back into processed_non_agg_data (point level).</li> <li>Perform spatial joins to attach <code>origin_geo_code</code> and    <code>destination_geo_code</code>.</li> <li>Create a unique <code>trip_id</code> (<code>trip_num</code>) for downstream use.</li> <li>Produce an aggregated DataFrame of trip counts by origin, destination,    and travel mode.</li> <li>Optionally write both non-aggregated and aggregated CSV files to    <code>&lt;output_dir&gt;/predictions/</code>.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>artifacts_dir</code> <code>str</code> <p>Directory containing the trained model and label encoder files.</p> required <code>model_file_name</code> <code>str</code> <p>Filename of the saved classifier (e.g., <code>\"decision_tree.pkl\"</code>).</p> required <code>le_file_name</code> <code>str</code> <p>Filename of the fitted <code>LabelEncoder</code> (e.g., <code>\"label_encoder.joblib\"</code>).</p> required <code>processed_non_agg_data</code> <code>DataFrame</code> <p>Point-level dataset produced by the feature-engineering pipeline.</p> required <code>stats_agg_data</code> <code>DataFrame</code> <p>Trip-level statistics providing the predictor variables used by the model.</p> required <code>shape_file</code> <code>str</code> <p>Path to the polygon shapefile used for spatial joins (e.g., LSOAs or census tracts).</p> required <code>output_dir</code> <code>str</code> <p>If supplied, CSV outputs are written to <code>&lt;output_dir&gt;/predictions</code>. Defaults to <code>None</code> (no files saved).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[pd.DataFrame, pd.DataFrame]: * op_df \u2013 Point-level DataFrame with columns   <code>[\"trip_id\", \"origin_geo_code\", \"destination_geo_code\",      \"tp_lat\", \"tp_lng\", \"datetime\", \"travel_mode\"]</code>. * agg_op_df \u2013 Aggregated DataFrame of trip counts by   <code>origin_geo_code</code>, <code>destination_geo_code</code>, and   <code>travel_mode</code>.</p> Example <p>op_df, agg_df = modePredict( ...     artifacts_dir=\"artifacts\", ...     model_file_name=\"xgb_model.joblib\", ...     le_file_name=\"label_encoder.joblib\", ...     processed_non_agg_data=trip_points_df, ...     stats_agg_data=traj_stats_df, ...     shape_file=\"data/shapes/lsoa.shp\", ...     output_dir=\"outputs\" ... ) op_df.head() agg_df.head()</p> Source code in <code>meowmotion/model_tmd.py</code> <pre><code>def modePredict(\n    artifacts_dir: str,\n    model_file_name: str,\n    le_file_name: str,\n    processed_non_agg_data: pd.DataFrame,\n    stats_agg_data: pd.DataFrame,\n    shape_file: str,\n    output_dir: Optional[str] = None,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Predict travel modes for every trip, attach predictions to point-level data,\n    add origin/destination geographic codes, and (optionally) save the results.\n\n    Workflow:\n        1. Load the trained classifier and its label encoder from *artifacts_dir*.\n        2. Read the study-area shapefile and re-project to WGS 84 (EPSG 4326).\n        3. Build the model-input feature set from *stats_agg_data* and predict the\n           travel mode for each trip.\n        4. Merge predictions back into *processed_non_agg_data* (point level).\n        5. Perform spatial joins to attach `origin_geo_code` and\n           `destination_geo_code`.\n        6. Create a unique `trip_id` (``trip_num``) for downstream use.\n        7. Produce an aggregated DataFrame of trip counts by origin, destination,\n           and travel mode.\n        8. Optionally write both non-aggregated and aggregated CSV files to\n           ``&lt;output_dir&gt;/predictions/``.\n\n    Args:\n        artifacts_dir (str): Directory containing the trained model and label\n            encoder files.\n        model_file_name (str): Filename of the saved classifier\n            (e.g., ``\"decision_tree.pkl\"``).\n        le_file_name (str): Filename of the fitted ``LabelEncoder``\n            (e.g., ``\"label_encoder.joblib\"``).\n        processed_non_agg_data (pd.DataFrame): Point-level dataset produced by\n            the feature-engineering pipeline.\n        stats_agg_data (pd.DataFrame): Trip-level statistics providing the\n            predictor variables used by the model.\n        shape_file (str): Path to the polygon shapefile used for spatial joins\n            (e.g., LSOAs or census tracts).\n        output_dir (str, optional): If supplied, CSV outputs are written to\n            ``&lt;output_dir&gt;/predictions``. Defaults to ``None`` (no files saved).\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame]:\n            * **op_df** \u2013 Point-level DataFrame with columns\n              ``[\"trip_id\", \"origin_geo_code\", \"destination_geo_code\",\n                 \"tp_lat\", \"tp_lng\", \"datetime\", \"travel_mode\"]``.\n            * **agg_op_df** \u2013 Aggregated DataFrame of trip counts by\n              ``origin_geo_code``, ``destination_geo_code``, and\n              ``travel_mode``.\n\n    Example:\n        &gt;&gt;&gt; op_df, agg_df = modePredict(\n        ...     artifacts_dir=\"artifacts\",\n        ...     model_file_name=\"xgb_model.joblib\",\n        ...     le_file_name=\"label_encoder.joblib\",\n        ...     processed_non_agg_data=trip_points_df,\n        ...     stats_agg_data=traj_stats_df,\n        ...     shape_file=\"data/shapes/lsoa.shp\",\n        ...     output_dir=\"outputs\"\n        ... )\n        &gt;&gt;&gt; op_df.head()\n        &gt;&gt;&gt; agg_df.head()\n    \"\"\"\n\n    print(f\"{datetime.now()}: Loading Model and Encoder\")\n    model = joblib.load(f\"{artifacts_dir}/{model_file_name}\")\n    class_encoder = joblib.load(f\"{artifacts_dir}/{le_file_name}\")\n    print(f\"{datetime.now()}: Loading Shapefile\")\n    gdf = gpd.read_file(shape_file)\n    gdf = gdf.to_crs(epsg=4326)\n    gdf.index\n\n    print(f\"{datetime.now()}: Loading Processed Data\")\n    # processed_non_agg_data = pd.read_csv(processed_non_agg_data)\n    processed_non_agg_data = processed_non_agg_data[\n        [\n            \"uid\",\n            \"imd_quintile\",\n            \"trip_id\",\n            \"total_active_days\",\n            \"lat\",\n            \"lng\",\n            \"org_lat\",\n            \"org_lng\",\n            \"dest_lat\",\n            \"dest_lng\",\n            \"datetime\",\n            \"num_of_impressions\",\n            \"time_taken\",\n            \"prev_lat\",\n            \"prev_long\",\n            \"distance_covered\",\n            \"speed\",\n            \"date\",\n            \"hour\",\n            \"speed_z_score\",\n            \"new_speed\",\n            \"accelaration\",\n            \"jerk\",\n            \"bearing\",\n            \"angular_deviation\",\n            \"month\",\n            \"is_weekend\",\n            \"hour_category\",\n            \"start_end_at_bus_stop\",\n            \"start_end_at_train_stop\",\n            \"start_end_at_metro_stop\",\n            \"found_at_green_space\",\n            \"straightness_index\",\n        ]\n    ]\n\n    attributes = [\n        \"month\",\n        \"speed_median\",\n        \"speed_pct_95\",\n        \"speed_std\",\n        \"acceleration_median\",\n        \"acceleration_pct_95\",\n        \"acceleration_std\",\n        \"jerk_median\",\n        \"jerk_pct_95\",\n        \"jerk_std\",\n        \"angular_dev_median\",\n        \"angular_dev_pct_95\",\n        \"angular_dev_std\",\n        \"straightness_index\",\n        \"distance_covered\",\n        \"start_end_at_bus_stop\",\n        \"start_end_at_train_stop\",\n        \"start_end_at_metro_stop\",\n        \"found_at_green_space\",\n        \"is_weekend\",\n        \"hour_category\",\n    ]\n\n    print(f\"{datetime.now()}: Loading Stats Aggregated Data\")\n    # data = pd.read_csv(stats_agg_data, parse_dates=[\"datetime\"])\n    data = stats_agg_data.copy()\n    data[\"month\"] = data[\"datetime\"].dt.month\n    # keep the mode of month for each uid and trip_id\n    data[\"month\"] = data.groupby([\"uid\", \"trip_id\"])[\"month\"].transform(\n        lambda x: x.mode()[0]\n    )  # some night trips change the month. So, we keep the mode of month for each trip\n    data = data.drop_duplicates(subset=attributes)\n\n    print(f\"{datetime.now()}:Predicting Travel Mode\")\n    pred = model.predict(data[attributes])\n    pred = class_encoder.inverse_transform(pred)\n    data[\"travel_mode\"] = pred\n\n    print(f\"{datetime.now()}: Merging Travel Mode with Processed Data\")\n    processed_non_agg_data = processed_non_agg_data.merge(\n        data[[\"uid\", \"trip_id\", \"travel_mode\"]], on=[\"uid\", \"trip_id\"], how=\"left\"\n    )\n    del data\n    op_df = processed_non_agg_data[\n        [\n            \"uid\",\n            \"trip_id\",\n            \"org_lat\",\n            \"org_lng\",\n            \"dest_lat\",\n            \"dest_lng\",\n            \"lat\",\n            \"lng\",\n            \"datetime\",\n            \"travel_mode\",\n        ]\n    ]\n\n    # Add origin geo code\n    print(\n        f\"{datetime.now()}: Spatial Join for Origin Geo Codes (It may take a few minutes...)\"\n    )\n    op_df = spatialJoin(\n        op_df,\n        gdf,\n        \"org_lng\",\n        \"org_lat\",\n        loc_type=\"origin\",\n    )\n\n    # Add destination geo code\n    print(\n        f\"{datetime.now()}: Spatial Join for Destination Geo Codes (It may take a few minutes...)\"\n    )\n    op_df = spatialJoin(\n        op_df,\n        gdf,\n        \"dest_lng\",\n        \"dest_lat\",\n        loc_type=\"destination\",\n    )\n\n    print(\n        f\"{datetime.now()}: Getting Unique Trip Number (It may take a few minutes...)\"\n    )\n    op_df.loc[:, \"trip_num\"] = (\n        pd.factorize(op_df[[\"uid\", \"trip_id\"]].apply(tuple, axis=1))[0] + 1\n    )\n    op_df = op_df.drop(columns=[\"uid\", \"trip_id\"])\n    op_df = op_df[\n        [\n            \"trip_num\",\n            \"origin_geo_code\",\n            \"destination_geo_code\",\n            \"lat\",\n            \"lng\",\n            \"datetime\",\n            \"travel_mode\",\n        ]\n    ]\n    op_df = op_df.rename(\n        columns={\"trip_num\": \"trip_id\", \"lat\": \"tp_lat\", \"lng\": \"tp_lng\"}\n    )\n    op_df = op_df.dropna(subset=[\"travel_mode\"])\n    assert op_df[\"travel_mode\"].isna().sum() == 0\n\n    agg_op_df = op_df.copy()\n    agg_op_df = agg_op_df.drop_duplicates(subset=[\"trip_id\"])\n    agg_op_df = (\n        agg_op_df.groupby([\"origin_geo_code\", \"destination_geo_code\", \"travel_mode\"])\n        .size()\n        .unstack(fill_value=0)\n        .reset_index()\n    )\n\n    if output_dir is not None:\n        print(f\"{datetime.now()}: Saving Predictions\")\n        os.makedirs(f\"{output_dir}/predictions\", exist_ok=True)\n        op_df.to_csv(\n            f\"{output_dir}/predictions/predicted_travel_modes_non_agg.csv\", index=False\n        )\n        agg_op_df.to_csv(\n            f\"{output_dir}/predictions/predicted_travel_modes_agg.csv\", index=False\n        )\n        print(f\"{datetime.now()}: Predictions Saved\")\n\n    return op_df, agg_op_df\n</code></pre>"},{"location":"reference/model_tmd/#meowmotion.model_tmd.processTrainingData","title":"<code>processTrainingData(data)</code>","text":"<p>Cleans, filters, and splits labelled trajectory data into training and validation (testing) sets, then derives trip-level statistics for each set using :pyfunc:<code>generateTrajStats</code>.</p> <p>The procedure applies several rule-based steps:</p> <ol> <li>Confidence filtering \u2013 Drops records whose    <code>maximum_match_confidence</code> is below a mode-specific threshold    (<code>car</code>/<code>walk</code> &lt; 0.80, <code>bicycle</code>/<code>bus</code> &lt; 0.60, <code>train</code>/<code>metro</code> &lt; 0.40).</li> <li>Random split \u2013 Assigns \u2248 33 % of unique    <code>(installation_id, trip_id, leg_id)</code> groups to the validation set;    the remainder form the training set.</li> <li>Contextual consistency \u2013 Removes trips where a motorised mode    (<code>car</code>, <code>bus</code>, <code>train</code>) is flagged as found in green space.</li> <li>Speed outliers \u2013 Eliminates points with <code>new_speed</code> &gt; 40 m/s.</li> <li>NaN handling \u2013 Fills NaNs in <code>accelaration</code>, <code>angular_deviation</code>,    and <code>jerk</code> with 0.</li> <li>Physical-bounds filtering \u2013 Keeps only points where    <code>accelaration</code> \u2208 [\u20137 m/s\u00b2, 7 m/s\u00b2].</li> <li>Mode whitelist \u2013 Retains only the six canonical modes    (<code>walk</code>, <code>bicycle</code>, <code>car</code>, <code>bus</code>, <code>train</code>, <code>metro</code>).</li> <li>Trip-level feature generation \u2013 Calls    :pyfunc:<code>generateTrajStats</code> to compute statistics, then prunes trips    with implausible speed metrics (different rules per mode).</li> </ol> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Labelled point-level dataset produced by the feature-engineering pipeline. Must include, at minimum, the columns</p> <p><code>[\"installation_id\", \"trip_id\", \"leg_id\", \"timestamp\",    \"transport_mode\", \"maximum_match_confidence\",    \"new_speed\", \"accelaration\", \"jerk\", \"angular_deviation\",    \"found_at_green_space\"]</code></p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple[pd.DataFrame, pd.DataFrame]: stat_df \u2013 Cleaned training DataFrame of trip-level statistics.</p> <p>vald_stat_df \u2013 Cleaned validation DataFrame of trip-level statistics.</p> Example <p>train_stats, val_stats = processTrainingData(labelled_points_df) train_stats.head() val_stats.head()</p> Source code in <code>meowmotion/model_tmd.py</code> <pre><code>def processTrainingData(data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Cleans, filters, and splits labelled trajectory data into *training* and\n    *validation* (testing) sets, then derives trip-level statistics for each set\n    using :pyfunc:`generateTrajStats`.\n\n    The procedure applies several rule-based steps:\n\n    1. **Confidence filtering** \u2013 Drops records whose\n       ``maximum_match_confidence`` is below a mode-specific threshold\n       (`car`/`walk` &lt; 0.80, `bicycle`/`bus` &lt; 0.60, `train`/`metro` &lt; 0.40).\n    2. **Random split** \u2013 Assigns \u2248 33 % of unique\n       ``(installation_id, trip_id, leg_id)`` groups to the validation set;\n       the remainder form the training set.\n    3. **Contextual consistency** \u2013 Removes trips where a motorised mode\n       (`car`, `bus`, `train`) is flagged as *found in green space*.\n    4. **Speed outliers** \u2013 Eliminates points with ``new_speed`` &gt; 40 m/s.\n    5. **NaN handling** \u2013 Fills NaNs in ``accelaration``, ``angular_deviation``,\n       and ``jerk`` with 0.\n    6. **Physical-bounds filtering** \u2013 Keeps only points where\n       ``accelaration`` \u2208 [\u20137 m/s\u00b2, 7 m/s\u00b2].\n    7. **Mode whitelist** \u2013 Retains only the six canonical modes\n       (`walk`, `bicycle`, `car`, `bus`, `train`, `metro`).\n    8. **Trip-level feature generation** \u2013 Calls\n       :pyfunc:`generateTrajStats` to compute statistics, then prunes trips\n       with implausible speed metrics (different rules per mode).\n\n    Args:\n        data (pd.DataFrame):\n            Labelled point-level dataset produced by the feature-engineering\n            pipeline. Must include, at minimum, the columns\n\n            ``[\"installation_id\", \"trip_id\", \"leg_id\", \"timestamp\",\n               \"transport_mode\", \"maximum_match_confidence\",\n               \"new_speed\", \"accelaration\", \"jerk\", \"angular_deviation\",\n               \"found_at_green_space\"]``\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame]:\n            **stat_df** \u2013 Cleaned *training* DataFrame of trip-level\n            statistics.\n\n            **vald_stat_df** \u2013 Cleaned *validation* DataFrame of trip-level\n            statistics.\n\n    Example:\n        &gt;&gt;&gt; train_stats, val_stats = processTrainingData(labelled_points_df)\n        &gt;&gt;&gt; train_stats.head()\n        &gt;&gt;&gt; val_stats.head()\n    \"\"\"\n\n    proc_data = data.copy()\n    proc_data = proc_data.loc[\n        ~(\n            (proc_data[\"transport_mode\"] == \"car\")\n            &amp; (proc_data[\"maximum_match_confidence\"] &lt; 0.8)\n        )\n    ]\n\n    proc_data = proc_data.loc[\n        ~(\n            (proc_data[\"transport_mode\"] == \"walk\")\n            &amp; (proc_data[\"maximum_match_confidence\"] &lt; 0.8)\n        )\n    ]\n\n    proc_data = proc_data.loc[\n        ~(\n            (proc_data[\"transport_mode\"] == \"bicycle\")\n            &amp; (proc_data[\"maximum_match_confidence\"] &lt; 0.6)\n        )\n    ]\n\n    proc_data = proc_data.loc[\n        ~(\n            (proc_data[\"transport_mode\"] == \"bus\")\n            &amp; (proc_data[\"maximum_match_confidence\"] &lt; 0.6)\n        )\n    ]\n\n    proc_data = proc_data.loc[\n        ~(\n            (proc_data[\"transport_mode\"] == \"train\")\n            &amp; (proc_data[\"maximum_match_confidence\"] &lt; 0.4)\n        )\n    ]\n    proc_data = proc_data.loc[\n        ~(\n            (proc_data[\"transport_mode\"] == \"metro\")\n            &amp; (proc_data[\"maximum_match_confidence\"] &lt; 0.4)\n        )\n    ]\n\n    proc_data = proc_data.dropna(subset=[\"maximum_match_confidence\"])\n    trip_group = proc_data.groupby([\"installation_id\", \"trip_id\", \"leg_id\"])\n    proc_data[\"trip_group\"] = trip_group.grouper.group_info[0]\n\n    # Randomly Generating 33% of the validation/Testing data\n\n    # Define the range and the number of random numbers you want\n    lower_bound = 0\n    upper_bound = proc_data[\"trip_group\"].max()\n    num_of_random_numbers = int(0.33 * upper_bound)  # 4445\n    # Generate the distinct random numbers using random.sample\n    distinct_random_numbers = random.sample(\n        range(lower_bound, upper_bound), num_of_random_numbers\n    )\n    vald_proc_data = proc_data[proc_data[\"trip_group\"].isin(distinct_random_numbers)]\n    proc_data = proc_data.drop(vald_proc_data.index)\n\n    proc_data = proc_data.loc[\n        ~(\n            (proc_data[\"transport_mode\"] == \"car\")\n            &amp; (proc_data[\"found_at_green_space\"] == 1)\n        )\n    ]\n    proc_data = proc_data.loc[\n        ~(\n            (proc_data[\"transport_mode\"] == \"bus\")\n            &amp; (proc_data[\"found_at_green_space\"] == 1)\n        )\n    ]\n    proc_data = proc_data.loc[\n        ~(\n            (proc_data[\"transport_mode\"] == \"train\")\n            &amp; (proc_data[\"found_at_green_space\"] == 1)\n        )\n    ]\n    proc_data = proc_data[\n        proc_data.transport_mode.isin(\n            [\"walk\", \"bicycle\", \"car\", \"bus\", \"train\", \"metro\"]\n        )\n    ]\n\n    # Removing extreme outliers\n    print(f\"{datetime.now()}: Number of Records before Filtering: {proc_data.shape[0]}\")\n    proc_data = proc_data[proc_data.new_speed &lt;= 40]\n    print(f\"{datetime.now()} :Number of Records After Filtering: {proc_data.shape[0]}\")\n\n    vald_proc_data = vald_proc_data[\n        vald_proc_data.transport_mode.isin(\n            [\"walk\", \"bicycle\", \"car\", \"bus\", \"train\", \"metro\"]\n        )\n    ]\n    # Removing extreme outliers\n    print(\n        f\"{datetime.now()}: Number of Records before Filtering: {vald_proc_data.shape[0]}\"\n    )\n    vald_proc_data = vald_proc_data[vald_proc_data.new_speed &lt;= 40]\n    print(\n        f\"{datetime.now()}: Number of Records After Filtering: {vald_proc_data.shape[0]}\"\n    )\n    proc_data[\"accelaration\"].fillna(0, inplace=True)\n    proc_data[\"angular_deviation\"].fillna(0, inplace=True)\n    proc_data[\"jerk\"].fillna(0, inplace=True)\n    print(\n        f\"{datetime.now()}: Number of Records after Filling NaN: {proc_data.shape[0]}\"\n    )\n    print(f\"{datetime.now()}: NA Values Summary\\n\")\n    print(proc_data.isna().sum())\n    print(\"\\n\")\n\n    vald_proc_data[\"accelaration\"].fillna(0, inplace=True)\n    vald_proc_data[\"angular_deviation\"].fillna(0, inplace=True)\n    vald_proc_data[\"jerk\"].fillna(0, inplace=True)\n    print(\n        f\"{datetime.now()}: Number of Records after Filling NaN: {vald_proc_data.shape[0]}\"\n    )\n    print(f\"{datetime.now()}: NA Values Summary\\n\")\n    print(vald_proc_data.isna().sum())\n    print(\"\\n\")\n\n    # Removing extreme outliers acceleration, jerk and angular_deviation\n    proc_data = proc_data[\n        (proc_data[\"accelaration\"] &gt;= -7) &amp; (proc_data[\"accelaration\"] &lt;= 7)\n    ]\n    # https://pdf.sciencedirectassets.com/308315/1-s2.0-S2352146517X00070/1-s2.0-S2352146517307937/main.pdf? \\\n    # X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFoaCXVzLWVhc3QtMSJHMEUCIAU0wDBQETM6g4KbEu%2Bpf2UF00B6IxSgJenWpXUc65YoAi \\\n    # EAvgV1EU%2FJrUl6SYWYoXwVsCDZpo0KhNHk4m61VK9lxz8qvAUI0%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjU \\\n    # iDKzfI9kTW4eyOkQFlSqQBaIjoxj3k%2Bi4XxQtY7I32gLlYt7wMA5epgw03US0nzU1aj64wEtF0cjv7yMClisfXNknekhQpPGhWIQ1pMuF9Az \\\n    # mFIyTPCua2cT3S4p0pYKEkEwR43rTIXnCfMlb%2FGlmRN2kbMjvqKmugPgEqkM9jUlAqdRojhHFqBDF4ZmtowaTBkz%2FfEQS17SeKaDEwfwGbz \\\n    # LIv9LZ1%2BYFbQfutRa2%2F4dqtfb%2B0reaGqu5knx9nNvtAUOM5sOZHT8tJigW%2Bx6eXxL22%2B12aKviyc2Q2xOBKbRmstcYdFmErb7j0nJr \\\n    # FOd2r0tKWry1JQpSXs7z9kjhNNzUkz0YlQoUQTYfhPJV%2FG%2BHeZwNpDWdme%2BvouPU13IJrEKDiHMGJZ9q2k6XmeoMzr2Ce9atwdb9oQr7r3h \\\n    # cNRRtT2djXz%2FhHYaaMgdVxMdF1ExrnQ41wY8j6JIGJob2ZOu8dKlVr8NTrN%2Fc32HGn%2BoKgccMIjWEoIoZGLxEAW4cHB%2BDgn8xHL9xEwr4i \\\n    # L%2FYGCNocQuBwCSWjHaERKrBovhYA5EY%2BdcZ0Wza9hu8Al5GZr6IJ8u2bsWLofFv2XAvGBkr3qz1MB%2F%2FGSnvoqRchtwvm3L0B3ri7XmTY7WS \\\n    # xp8NKhbqniQ%2Bj9%2FlSrlhfTMm0SY%2Fhr%2Fdzej9wXuG9%2Bm1wwH4sRnO3AokQI8XyoyHCLovDAUaimH0jsn2bZviON8mdAN0MlAsuiiJXJDNXo \\\n    # YDMR%2BywgrsvTftka2xi7CRrd7YCERFnE752y%2BhB2XMTDZMNpXOplgXRSWzTzLDVXLs6N5P1Pk0L0NwmlfSwctGsfhsr5iJdB4Hmr%2FjvV2VTvut \\\n    # DnXSW%2Fhil67P2ukmt2pcxJ%2FIzi7TkDMVQJNQkWnVb5A7MO%2FpuaUGOrEB%2FkRFKZUN2u6suA1u2qIkQrN8yRaggRxm3I2142qtSk2xK2XXnx6yC \\\n    # ybkjte9EC0uXUVoFngEPePiverUnKaTnNR5reisksXGJ7HTylbDQ7MZTtBbBztdPqHFyDkoYPWwKJa%2BWEfmwr%2Fov3aIppu%2B7rwTxjwUhjZJP9JPCY \\\n    # %2FB4QWkqCYeW9IaJQ1%2FjoCnBMQQdXKIN7AC9SZd1sBIu1kXZ0LMaDDAfVu%2F%2B9X4NRpSpO%2FP&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz- \\\n    # Date=20230712T095527Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYUQKYG5EE%2F20230712%2Fus-ea \\\n    # st-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=76c7fc38436931a3d9c0192a069388486d9130d958e3b2994acb779c5ee74351&amp;hash=6a5ef828673 \\\n    # a2fa4b5e146070ce2a43e35de41966f8b027cb6f9d6490ba1c26e&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61 \\\n    # &amp;pii=S2352146517307937&amp;tid=spdf-6ea3e761-01b4-49d4-85a3-438ad5ad03b3&amp;sid=b8dd4ae7949db8401e9abfd-f11e02af27f2gxrqb&amp;type=clie \\\n    # nt&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=0204520a515a575c5003&amp;rr=7e58691aed2a075a&amp;cc=gb\n    vald_proc_data = vald_proc_data[\n        (vald_proc_data[\"accelaration\"] &gt;= -7) &amp; (vald_proc_data[\"accelaration\"] &lt;= 7)\n    ]\n\n    proc_data = proc_data.drop(columns=[\"trip_id\", \"leg_id\"])\n    proc_data = proc_data.rename(\n        columns={\n            \"trip_group\": \"trip_id\",\n            \"installation_id\": \"uid\",\n            \"timestamp\": \"datetime\",\n        }\n    )\n    proc_data[\"trip_id\"] = proc_data[\"trip_id\"].astype(\"int32\")\n\n    vald_proc_data = vald_proc_data.drop(columns=[\"trip_id\", \"leg_id\"])\n    vald_proc_data = vald_proc_data.rename(\n        columns={\n            \"trip_group\": \"trip_id\",\n            \"installation_id\": \"uid\",\n            \"timestamp\": \"datetime\",\n        }\n    )\n    vald_proc_data[\"trip_id\"] = vald_proc_data[\"trip_id\"].astype(\"int32\")\n    trip_points_df = proc_data.groupby([\"uid\", \"trip_id\", \"transport_mode\"])[\n        [\"uid\"]\n    ].apply(lambda x: x.count())\n    trip_points_df.rename(columns={\"uid\": \"total_points\"}, inplace=True)\n    trip_points_df.reset_index(inplace=True)\n    trip_points_df = trip_points_df.groupby([\"transport_mode\"])[[\"total_points\"]].apply(\n        lambda x: round(x.mean())\n    )\n\n    stat_df = generateTrajStats(proc_data)\n    stat_df = stat_df.drop_duplicates(\n        subset=[col for col in stat_df.columns if col != \"datetime\"], keep=\"first\"\n    )\n    stat_df[\"datetime\"] = pd.to_datetime(stat_df[\"datetime\"])\n    stat_df[\"month\"] = stat_df.datetime.dt.month\n    stat_df = stat_df.astype({\"is_weekend\": \"int32\"})\n\n    print(f\"{datetime.now()}: Generating Validation Stats\")\n    # Generating Stats for Validation Data\n    vald_stat_df = generateTrajStats(vald_proc_data)\n    vald_stat_df = vald_stat_df.drop_duplicates(\n        subset=[col for col in vald_stat_df.columns if col != \"datetime\"], keep=\"first\"\n    )\n    vald_stat_df[\"datetime\"] = pd.to_datetime(vald_stat_df[\"datetime\"])\n    vald_stat_df[\"month\"] = vald_stat_df.datetime.dt.month\n    vald_stat_df = vald_stat_df.astype({\"is_weekend\": \"int32\"})\n\n    ##########################################################################################################\n    #      For Training Data\n    ##########################################################################################################\n    stat_df = stat_df.loc[\n        ~((stat_df[\"transport_mode\"] == \"walk\") &amp; (stat_df[\"speed_median\"] &gt;= 6.9))\n    ]\n    stat_df = stat_df.loc[\n        ~((stat_df[\"transport_mode\"] == \"walk\") &amp; (stat_df[\"speed_pct_95\"] &gt;= 12.22))\n    ]\n    stat_df = stat_df.loc[\n        ~((stat_df[\"transport_mode\"] == \"bicycle\") &amp; (stat_df[\"speed_median\"] &gt;= 15))\n    ]\n    stat_df = stat_df.loc[\n        ~((stat_df[\"transport_mode\"] == \"bicycle\") &amp; (stat_df[\"speed_pct_95\"] &lt; 1))\n    ]\n    stat_df = stat_df.loc[\n        ~((stat_df[\"transport_mode\"] == \"bicycle\") &amp; (stat_df[\"speed_pct_95\"] &gt;= 22))\n    ]\n    stat_df = stat_df.loc[\n        ~((stat_df[\"transport_mode\"] == \"car\") &amp; (stat_df[\"speed_pct_95\"] &lt; 5.5))\n    ]\n    stat_df = stat_df.loc[\n        ~((stat_df[\"transport_mode\"] == \"bus\") &amp; (stat_df[\"speed_pct_95\"] &lt; 3))\n    ]\n    stat_df = stat_df.loc[\n        ~((stat_df[\"transport_mode\"] == \"train\") &amp; (stat_df[\"speed_pct_95\"] &lt; 5.5))\n    ]\n    stat_df = stat_df.loc[\n        ~((stat_df[\"transport_mode\"] == \"metro\") &amp; (stat_df[\"speed_pct_95\"] &lt; 5.5))\n    ]\n\n    ###################################################################################################\n    #         For Testing Data\n    ###################################################################################################\n\n    vald_stat_df = vald_stat_df.loc[\n        ~(\n            (vald_stat_df[\"transport_mode\"] == \"walk\")\n            &amp; (vald_stat_df[\"speed_median\"] &gt;= 6.9)\n        )\n    ]\n    vald_stat_df = vald_stat_df.loc[\n        ~(\n            (vald_stat_df[\"transport_mode\"] == \"walk\")\n            &amp; (vald_stat_df[\"speed_pct_95\"] &gt;= 12.22)\n        )\n    ]\n\n    vald_stat_df = vald_stat_df.loc[\n        ~(\n            (vald_stat_df[\"transport_mode\"] == \"bicycle\")\n            &amp; (vald_stat_df[\"speed_median\"] &gt;= 15)\n        )\n    ]\n    vald_stat_df = vald_stat_df.loc[\n        ~(\n            (vald_stat_df[\"transport_mode\"] == \"bicycle\")\n            &amp; (vald_stat_df[\"speed_pct_95\"] &lt; 1)\n        )\n    ]\n    vald_stat_df = vald_stat_df.loc[\n        ~(\n            (vald_stat_df[\"transport_mode\"] == \"bicycle\")\n            &amp; (vald_stat_df[\"speed_pct_95\"] &gt;= 22)\n        )\n    ]\n    vald_stat_df = vald_stat_df.loc[\n        ~(\n            (vald_stat_df[\"transport_mode\"] == \"car\")\n            &amp; (vald_stat_df[\"speed_pct_95\"] &lt; 5.5)\n        )\n    ]\n    vald_stat_df = vald_stat_df.loc[\n        ~(\n            (vald_stat_df[\"transport_mode\"] == \"bus\")\n            &amp; (vald_stat_df[\"speed_pct_95\"] &lt; 3)\n        )\n    ]\n    vald_stat_df = vald_stat_df.loc[\n        ~(\n            (vald_stat_df[\"transport_mode\"] == \"train\")\n            &amp; (vald_stat_df[\"speed_pct_95\"] &lt; 5.5)\n        )\n    ]\n    vald_stat_df = vald_stat_df.loc[\n        ~(\n            (vald_stat_df[\"transport_mode\"] == \"metro\")\n            &amp; (vald_stat_df[\"speed_pct_95\"] &lt; 5.5)\n        )\n    ]\n\n    return stat_df, vald_stat_df\n</code></pre>"},{"location":"reference/model_tmd/#meowmotion.model_tmd.trainDecisionTree","title":"<code>trainDecisionTree(x_train, y_train, val_x, val_y, le)</code>","text":"<p>This function trains a Decision Tree Classifier using the provided training data. It also evaluates the model on the validation data and prints the precision, recall, accuracy, and confusion matrix.</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>DataFrame</code> <p>The training features.</p> required <code>y_train</code> <code>array</code> <p>The training labels.</p> required <code>val_x</code> <code>DataFrame</code> <p>The validation features.</p> required <code>val_y</code> <code>array</code> <p>The validation labels.</p> required <p>Returns:</p> Name Type Description <code>dt</code> <code>DecisionTreeClassifier</code> <p>The trained Decision Tree model.</p> Example <p>dt_model = trainDecisionTree(x_train, y_train, val_x, val_y)</p> Source code in <code>meowmotion/model_tmd.py</code> <pre><code>def trainDecisionTree(\n    x_train: pd.DataFrame,\n    y_train: np.array,\n    val_x: pd.DataFrame,\n    val_y: np.array,\n    le: LabelEncoder,\n) -&gt; Tuple[DecisionTreeClassifier, float, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n\n    This function trains a Decision Tree Classifier using the provided training data.\n    It also evaluates the model on the validation data and prints the precision, recall, accuracy,\n    and confusion matrix.\n\n    Args:\n        x_train (pd.DataFrame): The training features.\n        y_train (np.array): The training labels.\n        val_x (pd.DataFrame): The validation features.\n        val_y (np.array): The validation labels.\n\n    Returns:\n        dt (DecisionTreeClassifier): The trained Decision Tree model.\n\n    Example:\n        dt_model = trainDecisionTree(x_train, y_train, val_x, val_y)\n    \"\"\"\n    dt = DecisionTreeClassifier(random_state=0)\n    dt.fit(x_train, y_train)\n    dt_pred = dt.predict(val_x)\n    dt_precision, dt_recall, dt_fscore, _ = precision_recall_fscore_support(\n        val_y, dt_pred\n    )\n    dt_acc = accuracy_score(val_y, dt_pred)\n    dt_precision = np.round(dt_precision * 100, 2)\n    dt_recall = np.round(dt_recall * 100, 2)\n    dt_acc = np.round(dt_acc * 100, 2)\n    cm = confusion_matrix(val_y, dt_pred, labels=dt.classes_)\n    print(f\"Precision:{dt_precision}\\nRecall:{dt_recall}\\nAcc:{dt_acc}\")\n    print(f\"Confusion Matrix:\\n{le.inverse_transform(dt.classes_)}\\n{cm}\")\n    return dt, dt_acc, dt_precision, dt_recall, cm\n</code></pre>"},{"location":"reference/model_tmd/#meowmotion.model_tmd.trainMLModel","title":"<code>trainMLModel(df_tr, df_val, model_name, output_dir=None)</code>","text":"<p>Trains a supervised classifier (decision-tree or random-forest) to predict travel mode from trip-level statistics. Applies SMOTE to balance classes, evaluates on a separate validation set, and optionally persists the model and label-encoder to disk.</p> <p>Parameters:</p> Name Type Description Default <code>df_tr</code> <code>DataFrame</code> <p>Training set returned by <code>processTrainingData</code>. Must include the feature columns listed in <code>tr_cols</code> plus a categorical <code>transport_mode</code> column.</p> required <code>df_val</code> <code>DataFrame</code> <p>Validation set with the same schema as <code>df_tr</code>.</p> required <code>model_name</code> <code>str</code> <p>Either \"decision_tree\" or \"random_forest\" (case-sensitive).</p> required <code>output_dir</code> <code>Optional[str]</code> <p>If supplied, the fitted model is saved to <code>&lt;output_dir&gt;/artifacts/{model_name}_model.joblib</code> and the LabelEncoder to <code>label_encoder.joblib</code> in the same folder.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[float, ndarray, ndarray, ndarray]</code> <p>Tuple[float, np.ndarray, np.ndarray, np.ndarray]: A tuple containing: - accuracy (float): Overall classification accuracy on the validation set. - precision (np.ndarray): Per-class precision scores. - recall (np.ndarray): Per-class recall scores. - cm (np.ndarray): Confusion matrix of shape (n_classes, n_classes).</p> Notes <p>The feature vector comprises 21 columns: [     \"month\", \"speed_median\", \"speed_pct_95\", \"speed_std\",     \"acceleration_median\", \"acceleration_pct_95\", \"acceleration_std\",     \"jerk_median\", \"jerk_pct_95\", \"jerk_std\",     \"angular_dev_median\", \"angular_dev_pct_95\", \"angular_dev_std\",     \"straightness_index\", \"distance_covered\",     \"start_end_at_bus_stop\", \"start_end_at_train_stop\",     \"start_end_at_metro_stop\", \"found_at_green_space\",     \"is_weekend\", \"hour_category\" ]</p> <p>SMOTE (Synthetic Minority Over-sampling Technique) is applied to the training set only.</p> Example <pre><code>acc, prec, rec, cm = trainMLModel(\n    df_tr=train_stats,\n    df_val=val_stats,\n    model_name=\"random_forest\",\n    output_dir=\"outputs\"\n)\n\nprint(f\"Accuracy: {acc:.3f}\")\nprint(\"Precision per class:\", prec)\nprint(\"Recall per class:\", rec)\nprint(\"Confusion matrix:\\n\", cm)\n</code></pre> Source code in <code>meowmotion/model_tmd.py</code> <pre><code>def trainMLModel(\n    df_tr: pd.DataFrame,\n    df_val: pd.DataFrame,\n    model_name: str,\n    output_dir: Optional[str] = None,\n) -&gt; Tuple[float, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Trains a supervised classifier (decision-tree or random-forest) to predict travel mode\n    from trip-level statistics. Applies SMOTE to balance classes, evaluates on a separate\n    validation set, and optionally persists the model and label-encoder to disk.\n\n    Args:\n        df_tr (pd.DataFrame):\n            Training set returned by `processTrainingData`. Must include the feature columns\n            listed in `tr_cols` plus a categorical `transport_mode` column.\n        df_val (pd.DataFrame):\n            Validation set with the same schema as `df_tr`.\n        model_name (str):\n            Either \"decision_tree\" or \"random_forest\" (case-sensitive).\n        output_dir (Optional[str], optional):\n            If supplied, the fitted model is saved to\n            `&lt;output_dir&gt;/artifacts/{model_name}_model.joblib` and the LabelEncoder to\n            `label_encoder.joblib` in the same folder.\n\n    Returns:\n        Tuple[float, np.ndarray, np.ndarray, np.ndarray]:\n            A tuple containing:\n            - accuracy (float): Overall classification accuracy on the validation set.\n            - precision (np.ndarray): Per-class precision scores.\n            - recall (np.ndarray): Per-class recall scores.\n            - cm (np.ndarray): Confusion matrix of shape (n_classes, n_classes).\n\n    Notes:\n        The feature vector comprises 21 columns:\n        [\n            \"month\", \"speed_median\", \"speed_pct_95\", \"speed_std\",\n            \"acceleration_median\", \"acceleration_pct_95\", \"acceleration_std\",\n            \"jerk_median\", \"jerk_pct_95\", \"jerk_std\",\n            \"angular_dev_median\", \"angular_dev_pct_95\", \"angular_dev_std\",\n            \"straightness_index\", \"distance_covered\",\n            \"start_end_at_bus_stop\", \"start_end_at_train_stop\",\n            \"start_end_at_metro_stop\", \"found_at_green_space\",\n            \"is_weekend\", \"hour_category\"\n        ]\n\n        SMOTE (Synthetic Minority Over-sampling Technique) is applied to the training set only.\n\n    Example:\n        ```python\n        acc, prec, rec, cm = trainMLModel(\n            df_tr=train_stats,\n            df_val=val_stats,\n            model_name=\"random_forest\",\n            output_dir=\"outputs\"\n        )\n\n        print(f\"Accuracy: {acc:.3f}\")\n        print(\"Precision per class:\", prec)\n        print(\"Recall per class:\", rec)\n        print(\"Confusion matrix:\\\\n\", cm)\n        ```\n    \"\"\"\n\n    if model_name not in [\"decision_tree\", \"random_forest\"]:\n        raise ValueError(\n            \"model_name should be either 'decision_tree' or 'random_forest'\"\n        )\n    print(f\"{datetime.now()}: Training ML Model\")\n    ml_df = df_tr.copy()\n    val_ml_df = df_val.copy()\n    print(f\"{datetime.now()}: Encoding Class Labels\")\n    le = LabelEncoder()\n    ml_df[\"class_label\"] = le.fit_transform(ml_df.transport_mode)\n    val_ml_df[\"class_label\"] = le.fit_transform(val_ml_df.transport_mode)\n    tr_cols = [\n        \"month\",\n        \"speed_median\",\n        \"speed_pct_95\",\n        \"speed_std\",\n        \"acceleration_median\",\n        \"acceleration_pct_95\",\n        \"acceleration_std\",\n        \"jerk_median\",\n        \"jerk_pct_95\",\n        \"jerk_std\",\n        \"angular_dev_median\",\n        \"angular_dev_pct_95\",\n        \"angular_dev_std\",\n        \"straightness_index\",\n        \"distance_covered\",\n        \"start_end_at_bus_stop\",\n        \"start_end_at_train_stop\",\n        \"start_end_at_metro_stop\",\n        \"found_at_green_space\",\n        \"is_weekend\",\n        \"hour_category\",\n    ]\n\n    x = ml_df[tr_cols]\n    y = ml_df[\"class_label\"].values\n    val_x = val_ml_df[tr_cols]\n    val_y = val_ml_df[\"class_label\"].values\n\n    print(f\"{datetime.now()}: Oversampling the data\")\n    oversample = SMOTE()\n    x_train, y_train = oversample.fit_resample(x, y)\n    print(f\"{datetime.now()}: Oversampling Completed\")\n\n    print(f\"{datetime.now()}: Training {model_name} Model\")\n    if model_name == \"decision_tree\":\n        model, acc, prec, recall, cm = trainDecisionTree(\n            x_train, y_train, val_x, val_y, le\n        )\n    elif model_name == \"random_forest\":\n        model, acc, prec, recall, cm = trainRandomForest(\n            x_train, y_train, val_x, val_y, le\n        )\n\n    if output_dir is not None:\n        print(f\"{datetime.now()}: Saving Model\")\n        os.makedirs(f\"{output_dir}/artifacts\", exist_ok=True)\n        joblib.dump(model, f\"{output_dir}/artifacts/{model_name}_model.joblib\")\n        print(f\"{datetime.now()}: Saving Label Encoder\")\n        joblib.dump(le, f\"{output_dir}/artifacts/label_encoder.joblib\")\n    return acc, prec, recall, cm\n</code></pre>"},{"location":"reference/model_tmd/#meowmotion.model_tmd.trainRandomForest","title":"<code>trainRandomForest(x_train, y_train, val_x, val_y, le)</code>","text":"<p>This function trains a Random Forest Classifier using the provided training data. It also evaluates the model on the validation data and prints the precision, recall, accuracy, and confusion matrix.</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>DataFrame</code> <p>The training features.</p> required <code>y_train</code> <code>array</code> <p>The training labels.</p> required <code>val_x</code> <code>DataFrame</code> <p>The validation features.</p> required <code>val_y</code> <code>array</code> <p>The validation labels.</p> required <p>Returns:</p> Name Type Description <code>rf</code> <code>RandomForestClassifier</code> <p>The trained Random Forest model.</p> Example <p>rf_model = trainRandomForest(x_train, y_train, val_x, val_y)</p> Source code in <code>meowmotion/model_tmd.py</code> <pre><code>def trainRandomForest(\n    x_train: pd.DataFrame,\n    y_train: np.array,\n    val_x: pd.DataFrame,\n    val_y: np.array,\n    le: LabelEncoder,\n) -&gt; Tuple[RandomForestClassifier, float, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n\n    This function trains a Random Forest Classifier using the provided training data.\n    It also evaluates the model on the validation data and prints the precision, recall, accuracy,\n    and confusion matrix.\n\n    Args:\n        x_train (pd.DataFrame): The training features.\n        y_train (np.array): The training labels.\n        val_x (pd.DataFrame): The validation features.\n        val_y (np.array): The validation labels.\n\n    Returns:\n        rf (RandomForestClassifier): The trained Random Forest model.\n\n    Example:\n        rf_model = trainRandomForest(x_train, y_train, val_x, val_y)\n    \"\"\"\n\n    rf = RandomForestClassifier(n_estimators=200, max_depth=200, max_features=None)\n    rf.fit(x_train, y_train)\n    rf_pred = rf.predict(val_x)\n    rf_precision, rf_recall, rf_fscore, _ = precision_recall_fscore_support(\n        val_y, rf_pred\n    )\n    rf_acc = accuracy_score(val_y, rf_pred)\n    rf_precision = np.round(rf_precision * 100, 2)\n    rf_recall = np.round(rf_recall * 100, 2)\n    rf_acc = np.round(rf_acc * 100, 2)\n    cm = confusion_matrix(val_y, rf_pred, labels=rf.classes_)\n    print(f\"Precision:{rf_precision}\\nRecall:{rf_recall}\\nAcc:{rf_acc}\")\n    print(f\"Confusion Matrix:\\n{le.inverse_transform(rf.classes_)}\\n{cm}\")\n    return rf, rf_acc, rf_precision, rf_recall, cm\n</code></pre>"},{"location":"reference/process_data/","title":"Process Data","text":""},{"location":"reference/process_data/#meowmotion.process_data.getFilteredData","title":"<code>getFilteredData(df, impr_acc=100, cpu_cores=max(1, int(cpu_count() / 2)))</code>","text":"<p>Parallel, two\u2013stage cleansing of raw impression data that</p> <ol> <li>drops points whose GNSS accuracy (<code>impression_acc</code>) exceeds the    user-specified threshold, and</li> <li>removes physically implausible jumps using scikit-mob\u2019s    :pyfunc:<code>skmob.preprocessing.filtering.filter</code>    (<code>max_speed_kmh=200</code> by default).</li> </ol> <p>The work is split into load-balanced buckets and processed concurrently with :pyclass:<code>multiprocessing.Pool</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Point-level impressions with at least the columns <code>[\"uid\", \"lat\", \"lng\", \"datetime\", \"impression_acc\"]</code> (plus any additional attributes you want to keep).</p> required <code>impr_acc</code> <code>int</code> <p>Maximum allowed GNSS accuracy in metres. Points with a larger <code>impression_acc</code> are discarded. Defaults to <code>100</code>.</p> <code>100</code> <code>cpu_cores</code> <code>int</code> <p>Number of CPU cores to devote to multiprocessing. By default, half of the available logical cores (but at least 1).</p> <code>max(1, int(cpu_count() / 2))</code> <p>Returns:</p> Name Type Description <code>TrajDataFrame</code> <code>TrajDataFrame</code> <p>A scikit-mob <code>TrajDataFrame</code> containing only points that pass both the accuracy and speed filters, with its original columns preserved.</p> Example <p>clean_traj = getFilteredData(raw_df, impr_acc=50, cpu_cores=8) print(clean_traj.shape)</p> Source code in <code>meowmotion/process_data.py</code> <pre><code>def getFilteredData(\n    df: pd.DataFrame,\n    impr_acc: Optional[int] = 100,\n    cpu_cores: Optional[int] = max(1, int(cpu_count() / 2)),\n) -&gt; TrajDataFrame:\n    \"\"\"\n    Parallel, two\u2013stage cleansing of raw impression data that\n\n    1. **drops points whose GNSS accuracy** (``impression_acc``) exceeds the\n       user-specified threshold, and\n    2. **removes physically implausible jumps** using scikit-mob\u2019s\n       :pyfunc:`skmob.preprocessing.filtering.filter`\n       (``max_speed_kmh=200`` by default).\n\n    The work is split into load-balanced buckets and processed concurrently\n    with :pyclass:`multiprocessing.Pool`.\n\n    Args:\n        df (pd.DataFrame):\n            Point-level impressions with *at least* the columns\n            ``[\"uid\", \"lat\", \"lng\", \"datetime\", \"impression_acc\"]``\n            (plus any additional attributes you want to keep).\n        impr_acc (int, optional):\n            Maximum allowed GNSS accuracy in **metres**. Points with a larger\n            ``impression_acc`` are discarded. Defaults to ``100``.\n        cpu_cores (int, optional):\n            Number of CPU cores to devote to multiprocessing. By default, half\n            of the available logical cores (but at least 1).\n\n    Returns:\n        TrajDataFrame:\n            A scikit-mob ``TrajDataFrame`` containing only points that pass\n            both the accuracy and speed filters, with its original columns\n            preserved.\n\n    Example:\n        &gt;&gt;&gt; clean_traj = getFilteredData(raw_df, impr_acc=50, cpu_cores=8)\n        &gt;&gt;&gt; print(clean_traj.shape)\n    \"\"\"\n\n    print(f\"{datetime.now()}: Filtering data based on impression accuracy={impr_acc}\")\n    print(f\"{datetime.now()}: Creating buckets for multiprocessing\")\n    tdf_collection = getLoadBalancedBuckets(df, cpu_cores)\n    args = [(tdf, impr_acc) for tdf in tdf_collection if not tdf.empty]\n    print(f\"{datetime.now()}: Filtering Started...\")\n    with Pool(cpu_cores) as pool:\n        results = pool.starmap(\n            filterData, args\n        )  # Filtering the data based on Impression Accuracy and Speed between GPS points\n\n    del tdf_collection  # Deleting the data to free up the memory\n    traj_df = pd.concat(\n        [*results]\n    )  # Concatinating the filtered data from all the processes\n    del results  # Deleting the results to free up the memory\n    print(f\"{datetime.now()}: Filtering Finished\\n\\n\\n\")\n\n    return traj_df\n</code></pre>"},{"location":"reference/process_data/#meowmotion.process_data.getLoadBalancedBuckets","title":"<code>getLoadBalancedBuckets(tdf, bucket_size)</code>","text":"<p>Partition a user-level DataFrame into bucket_size sub-DataFrames whose total row counts (i.e. number of \u201cimpressions\u201d) are as evenly balanced as possible.  Each bucket can then be processed in parallel on its own CPU core.</p>"},{"location":"reference/process_data/#meowmotion.process_data.getLoadBalancedBuckets--algorithm","title":"Algorithm","text":"<ol> <li>Count the number of rows (\u201cimpressions\u201d) for every unique <code>uid</code>.</li> <li>Sort users in descending order of impression count.</li> <li>Greedily assign each user to the bucket that currently has the    smallest total number of impressions (load-balancing heuristic).</li> <li>Build one DataFrame per bucket containing only the rows for the users    assigned to that bucket.</li> <li>Return the list of non-empty bucket DataFrames.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>tdf</code> <code>DataFrame</code> <p>A DataFrame that must contain a <code>\"uid\"</code> column plus any other fields.  Each row represents one GPS impression or point.</p> required <code>bucket_size</code> <code>int</code> <p>The desired number of buckets\u2014typically equal to the number of CPU cores you plan to use with :pyclass:<code>multiprocessing.Pool</code>.</p> required <p>Returns:</p> Type Description <code>list</code> <p>list[pd.DataFrame]: A list whose length is \u2264 bucket_size.  Each element is a DataFrame containing a disjoint subset of users such that the cumulative row counts across buckets are approximately balanced. Empty buckets are omitted.</p> Example <p>buckets = getLoadBalancedBuckets(raw_points_df, bucket_size=8) for i, bucket_df in enumerate(buckets, start=1): ...     print(f\"Bucket {i}: {len(bucket_df):,} rows \" ...           f\"({bucket_df['uid'].nunique()} users)\")</p> Note <p>The function is designed for embarrassingly parallel workloads where each user\u2019s data can be processed independently (e.g. feature extraction or filtering).</p> Source code in <code>meowmotion/process_data.py</code> <pre><code>def getLoadBalancedBuckets(tdf: pd.DataFrame, bucket_size: int) -&gt; list:\n    \"\"\"\n    Partition a user-level DataFrame into *bucket_size* sub-DataFrames whose\n    total row counts (i.e. number of \u201cimpressions\u201d) are as evenly balanced as\n    possible.  Each bucket can then be processed in parallel on its own CPU\n    core.\n\n    Algorithm\n    ---------\n    1. Count the number of rows (\u201cimpressions\u201d) for every unique ``uid``.\n    2. Sort users in descending order of impression count.\n    3. Greedily assign each user to the bucket that currently has the\n       **smallest** total number of impressions (*load-balancing heuristic*).\n    4. Build one DataFrame per bucket containing only the rows for the users\n       assigned to that bucket.\n    5. Return the list of non-empty bucket DataFrames.\n\n    Args:\n        tdf (pd.DataFrame):\n            A DataFrame that **must contain a ``\"uid\"`` column** plus any other\n            fields.  Each row represents one GPS impression or point.\n        bucket_size (int):\n            The desired number of buckets\u2014typically equal to the number of CPU\n            cores you plan to use with :pyclass:`multiprocessing.Pool`.\n\n    Returns:\n        list[pd.DataFrame]:\n            A list whose length is **\u2264 *bucket_size***.  Each element is a\n            DataFrame containing a disjoint subset of users such that the\n            cumulative row counts across buckets are approximately balanced.\n            Empty buckets are omitted.\n\n    Example:\n        &gt;&gt;&gt; buckets = getLoadBalancedBuckets(raw_points_df, bucket_size=8)\n        &gt;&gt;&gt; for i, bucket_df in enumerate(buckets, start=1):\n        ...     print(f\"Bucket {i}: {len(bucket_df):,} rows \"\n        ...           f\"({bucket_df['uid'].nunique()} users)\")\n\n    Note:\n        The function is designed for *embarrassingly parallel* workloads where\n        each user\u2019s data can be processed independently (e.g. feature\n        extraction or filtering).\n    \"\"\"\n\n    print(f\"{datetime.now()}: Getting unique users\")\n    unique_users = tdf[\"uid\"].unique()  # Getting Unique Users in the data\n    print(f\"{datetime.now()}: Number of unique users: {len(unique_users)}\")\n    print(f\"{datetime.now()}: Creating sets\")\n    num_impr_df = (\n        pd.DataFrame(tdf.groupby(\"uid\").size(), columns=[\"num_impressions\"])\n        .reset_index()\n        .sort_values(by=[\"num_impressions\"], ascending=False)\n    )  # Creating a DataFrame containing Unique UID and Total number of impressions that Unique UID has in the data.\n    buckets = (\n        {}\n    )  # A dictionary containing buckets of UIDs. Each bucket represent the CPU core. This dictionary tells how many users' data will be process on which core. For example, if bucket 1 contains 10 UIDs, data of those 10 UIDs will be processed on the core 1.\n    bucket_sums = {}  # A flag dictionary to keep the track of load on each bucket.\n\n    for i in range(1, bucket_size + 1):\n        buckets[i] = []  # Initializing empty buckets\n        bucket_sums[i] = 0  # Load in each bucket is zero initially\n\n    # Allocate users to buckets\n    for _, row in num_impr_df.iterrows():\n        user, impressions = (\n            row[\"uid\"],\n            row[\"num_impressions\"],\n        )  # Getting the UID and the number of impressions of that UID\n        # Find the bucket with the minimum sum of impressions\n        min_bucket = min(\n            bucket_sums, key=bucket_sums.get\n        )  # Getting the bucket with the minimum load. Initially, all the buckets have zero load.\n        # Add user to this bucket\n        buckets[min_bucket].append(user)  # Adding UID to the minimum bucket\n        # Update the sum of impressions for this bucket\n        bucket_sums[\n            min_bucket\n        ] += impressions  # Updating the load value of the bucket. For example, UID 1 was added to Bucket 1 and UID 1 had 1000 impressions (records). So, load of bucket 1 is 1000 now.\n\n    print(f\"{datetime.now()}: Creating seperate dataframes\")\n    tdf_collection = (\n        []\n    )  # List of collection of DataFrames. This list will contain the number of DataFrames=number of CPU Cores. Each DataFrame will be processed in a seperate core as a seperate process.\n    for i in range(1, bucket_size + 1):\n        # tdf_collection.append(tdf[tdf[\"uid\"].isin(buckets[i])].copy())\n        tdf[tdf[\"uid\"].isin(buckets[i])].copy()\n        if not tdf.empty:\n            tdf_collection.append(tdf[tdf[\"uid\"].isin(buckets[i])].copy())\n    return tdf_collection\n</code></pre>"},{"location":"reference/process_data/#meowmotion.process_data.readJsonFiles","title":"<code>readJsonFiles(root, month_file)</code>","text":"<p>Load a month-worth of impression records stored as gzipped JSON-Lines inside a ZIP archive and return them as a tidy DataFrame.</p>"},{"location":"reference/process_data/#meowmotion.process_data.readJsonFiles--data-at-rest-format","title":"Data-at-Rest Format","text":"<p>The function expects the following directory / file structure:</p> <p>``root/     2023-01.zip              # &lt;- month_file argument         2023-01-01-00.json.gz         2023-01-01-01.json.gz         ...         2023-01-31-23.json.gz ```</p> <ul> <li>Each <code>.json.gz</code> file is a JSON-Lines file (one JSON object per line).</li> <li> <p>Every JSON object is expected to contain at least these keys:</p> </li> <li> <p><code>impression_acc</code>  (float)\u2003\u2013 GNSS accuracy (metres)</p> </li> <li><code>device_iid_hash</code> (str)\u2003  \u2013 Anonymised user or device ID</li> <li><code>impression_lng</code>  (float)\u2003\u2013 Longitude in WGS-84</li> <li><code>impression_lat</code>  (float)\u2003\u2013 Latitude  in WGS-84</li> <li><code>timestamp</code>       (str/int) \u2013 ISO-8601 string or Unix epoch (ms)</li> </ul> <p>The loader iterates through each <code>.json.gz</code> in the archive, parses every line, and extracts the subset of fields listed above.</p> <p>Args:     root (str):         Path to the directory that contains month_file (e.g.         <code>\"/data/impressions\"</code>).     month_file (str):         Name of the ZIP archive to read         (e.g. <code>\"2023-01.zip\"</code> or <code>\"london_2024-06.zip\"</code>).</p> <p>Returns:     pandas.DataFrame:         Columns \u2192 <code>[\"impression_acc\", \"uid\", \"lng\", \"lat\", \"datetime\"]</code>         One row per JSON object across all <code>.json.gz</code> files in the archive.</p> <p>Example:     &gt;&gt;&gt; df = readJsonFiles(\"/data/impressions\", \"2023-01.zip\")     &gt;&gt;&gt; df.head()          impression_acc             uid        lng        lat             datetime     0              6.5  a1b2c3d4e5f6g7h8  -0.12776   51.50735   2023-01-01T00:00:10Z     1              4.8  h8g7f6e5d4c3b2a1  -0.12800   51.50720   2023-01-01T00:00:11Z     ...</p> Source code in <code>meowmotion/process_data.py</code> <pre><code>def readJsonFiles(root: str, month_file: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Load a month-worth of impression records stored as *gzipped JSON-Lines*\n    inside a ZIP archive and return them as a tidy DataFrame.\n\n    Data-at-Rest Format\n    -------------------\n    The function expects the following directory / file structure:\n\n    ``root/\n        2023-01.zip              # &lt;- month_file argument\n            2023-01-01-00.json.gz\n            2023-01-01-01.json.gz\n            ...\n            2023-01-31-23.json.gz\n    ```\n\n    * Each ``.json.gz`` file is a **JSON-Lines** file (one JSON object per line).\n    * Every JSON object is expected to contain at least these keys:\n\n      - ``impression_acc``  (float)\u2003\u2013 GNSS accuracy (metres)\n      - ``device_iid_hash`` (str)\u2003  \u2013 Anonymised user or device ID\n      - ``impression_lng``  (float)\u2003\u2013 Longitude in WGS-84\n      - ``impression_lat``  (float)\u2003\u2013 Latitude  in WGS-84\n      - ``timestamp``       (str/int) \u2013 ISO-8601 string *or* Unix epoch (ms)\n\n    The loader iterates through each ``.json.gz`` in the archive, parses every\n    line, and extracts the subset of fields listed above.\n\n    Args:\n        root (str):\n            Path to the directory that contains *month_file* (e.g.\n            ``\"/data/impressions\"``).\n        month_file (str):\n            Name of the ZIP archive to read\n            (e.g. ``\"2023-01.zip\"`` or ``\"london_2024-06.zip\"``).\n\n    Returns:\n        pandas.DataFrame:\n            Columns \u2192 ``[\"impression_acc\", \"uid\", \"lng\", \"lat\", \"datetime\"]``\n            One row per JSON object across all ``.json.gz`` files in the archive.\n\n    Example:\n        &gt;&gt;&gt; df = readJsonFiles(\"/data/impressions\", \"2023-01.zip\")\n        &gt;&gt;&gt; df.head()\n             impression_acc             uid        lng        lat             datetime\n        0              6.5  a1b2c3d4e5f6g7h8  -0.12776   51.50735   2023-01-01T00:00:10Z\n        1              4.8  h8g7f6e5d4c3b2a1  -0.12800   51.50720   2023-01-01T00:00:11Z\n        ...\n    \"\"\"\n\n    print(f\"{datetime.now()}: Processing {month_file}\")\n\n    data = []\n    month_zip_file = f\"{root}/{month_file}\"\n\n    with zipfile.ZipFile(month_zip_file, \"r\") as zf:\n        for gz_file in zf.namelist():\n            print(f\"{datetime.now()}: Processing {gz_file}\")\n            with zf.open(gz_file) as f, gzip.GzipFile(fileobj=f, mode=\"r\") as g:\n                for line in io.TextIOWrapper(g, encoding=\"utf-8\"):\n                    json_obj = json.loads(line.strip())\n                    data.append(\n                        {\n                            \"impression_acc\": json_obj.get(\"impression_acc\"),\n                            \"uid\": json_obj.get(\"device_iid_hash\"),\n                            \"lng\": json_obj.get(\"impression_lng\"),\n                            \"lat\": json_obj.get(\"impression_lat\"),\n                            \"datetime\": json_obj.get(\"timestamp\"),\n                        }\n                    )\n\n    df = pd.DataFrame(data)\n    print(f\"{datetime.now()}: {month_file} processed.\")\n\n    return df\n</code></pre>"},{"location":"reference/process_data/#meowmotion.process_data.saveFile","title":"<code>saveFile(path, fname, df)</code>","text":"<p>Write a pandas DataFrame to a CSV file, creating the target directory if it does not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Folder in which to store the file (e.g. <code>\"outputs/predictions\"</code>).</p> required <code>fname</code> <code>str</code> <p>Name of the CSV file to create (e.g. <code>\"trip_points.csv\"</code>).</p> required <code>df</code> <code>DataFrame</code> <p>The DataFrame to be saved.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>saveFile(\"outputs\", \"clean_points.csv\", clean_df)</p> Source code in <code>meowmotion/process_data.py</code> <pre><code>def saveFile(path: str, fname: str, df: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Write a pandas DataFrame to a **CSV** file, creating the target directory\n    if it does not already exist.\n\n    Args:\n        path (str): Folder in which to store the file\n            (e.g. ``\"outputs/predictions\"``).\n        fname (str): Name of the CSV file to create\n            (e.g. ``\"trip_points.csv\"``).\n        df (pd.DataFrame): The DataFrame to be saved.\n\n    Returns:\n        None\n\n    Example:\n        &gt;&gt;&gt; saveFile(\"outputs\", \"clean_points.csv\", clean_df)\n        # \u2192 file written to outputs/clean_points.csv\n    \"\"\"\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n    df.to_csv(join(path, fname), index=False)\n    return None\n</code></pre>"},{"location":"reference/process_data/#meowmotion.process_data.saveFile--file-written-to-outputsclean_pointscsv","title":"\u2192 file written to outputs/clean_points.csv","text":""},{"location":"reference/process_data/#meowmotion.process_data.spatialJoin","title":"<code>spatialJoin(df, shape, lng_col, lat_col, loc_type)</code>","text":"<p>Spatially joins point data (supplied in df) to polygon features (supplied in shape) and appends the polygon\u2019s code and name as new columns that are prefixed with the provided loc_type.</p> Workflow <ol> <li>Convert each <code>(lng_col, lat_col)</code> pair into a Shapely    :class:<code>Point</code> and wrap df into a GeoDataFrame (CRS = EPSG 4326).</li> <li>Perform a left, intersects-based spatial join with shape.</li> <li>Rename <code>\"geo_code\" \u2192 f\"{loc_type}_geo_code\"</code> and    <code>\"name\" \u2192 f\"{loc_type}_name\"</code>.</li> <li>Drop internal join artefacts (<code>index_right</code> and the point    <code>geometry</code>) and return a plain pandas DataFrame.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Point-level DataFrame containing longitude and latitude columns specified by lng_col and lat_col.</p> required <code>shape</code> <code>GeoDataFrame</code> <p>Polygon layer with at least the columns <code>[\"geo_code\", \"name\", \"geometry\"]</code> (e.g. LSOAs, census tracts). Must be in CRS WGS-84 (EPSG 4326) or convertible as such.</p> required <code>lng_col</code> <code>str</code> <p>Name of the longitude column in df.</p> required <code>lat_col</code> <code>str</code> <p>Name of the latitude column in df.</p> required <code>loc_type</code> <code>str</code> <p>Prefix for the new columns\u2014commonly <code>\"origin\"</code> or <code>\"destination\"</code>.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A copy of df with two new columns:</p> <ul> <li><code>f\"{loc_type}_geo_code\"</code></li> </ul> <ul> <li><code>f\"{loc_type}_name\"</code></li> </ul> <p>Rows that do not intersect any polygon will contain <code>NaN</code> in these</p> <p>columns.</p> Example <p>enriched_df = spatialJoin( ...     df=trip_points, ...     shape=lsoa_gdf, ...     lng_col=\"org_lng\", ...     lat_col=\"org_lat\", ...     loc_type=\"origin\" ... ) enriched_df[[\"origin_geo_code\", \"origin_name\"]].head()</p> Source code in <code>meowmotion/process_data.py</code> <pre><code>def spatialJoin(\n    df: pd.DataFrame,\n    shape: gpd.GeoDataFrame,\n    lng_col: str,\n    lat_col: str,\n    loc_type: str,  # 'origin' or 'destination'\n):\n    \"\"\"\n    Spatially joins point data (supplied in *df*) to polygon features\n    (supplied in *shape*) and appends the polygon\u2019s code and name as new\n    columns that are prefixed with the provided *loc_type*.\n\n    Workflow:\n        1. Convert each ``(lng_col, lat_col)`` pair into a Shapely\n           :class:`Point` and wrap *df* into a GeoDataFrame (CRS = EPSG 4326).\n        2. Perform a left, *intersects*-based spatial join with *shape*.\n        3. Rename ``\"geo_code\" \u2192 f\"{loc_type}_geo_code\"`` and\n           ``\"name\" \u2192 f\"{loc_type}_name\"``.\n        4. Drop internal join artefacts (``index_right`` and the point\n           ``geometry``) and return a plain pandas DataFrame.\n\n    Args:\n        df (pd.DataFrame):\n            Point-level DataFrame containing longitude and latitude columns\n            specified by *lng_col* and *lat_col*.\n        shape (gpd.GeoDataFrame):\n            Polygon layer with at least the columns\n            ``[\"geo_code\", \"name\", \"geometry\"]`` (e.g. LSOAs, census tracts).\n            Must be in CRS WGS-84 (EPSG 4326) or convertible as such.\n        lng_col (str): Name of the longitude column in *df*.\n        lat_col (str): Name of the latitude column in *df*.\n        loc_type (str): Prefix for the new columns\u2014commonly ``\"origin\"`` or\n            ``\"destination\"``.\n\n    Returns:\n        pd.DataFrame: A copy of *df* with two new columns:\n\n        * ``f\"{loc_type}_geo_code\"``\n        * ``f\"{loc_type}_name\"``\n\n        Rows that do not intersect any polygon will contain ``NaN`` in these\n        columns.\n\n    Example:\n        &gt;&gt;&gt; enriched_df = spatialJoin(\n        ...     df=trip_points,\n        ...     shape=lsoa_gdf,\n        ...     lng_col=\"org_lng\",\n        ...     lat_col=\"org_lat\",\n        ...     loc_type=\"origin\"\n        ... )\n        &gt;&gt;&gt; enriched_df[[\"origin_geo_code\", \"origin_name\"]].head()\n    \"\"\"\n\n    geometry = [Point(xy) for xy in zip(df[lng_col], df[lat_col])]\n    geo_df = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n    geo_df.sindex\n    geo_df = gpd.sjoin(\n        geo_df,\n        shape[[\"geo_code\", \"name\", \"geometry\"]],\n        how=\"left\",\n        predicate=\"intersects\",\n    )\n    col_rename_dict = {\n        \"geo_code\": f\"{loc_type}_geo_code\",\n        \"name\": f\"{loc_type}_name\",\n    }\n    geo_df = geo_df.rename(columns=col_rename_dict)\n    geo_df = geo_df.drop(columns=[\"index_right\", \"geometry\"])\n    geo_df = geo_df.reset_index(drop=True)\n    geo_df = pd.DataFrame(geo_df)\n    return geo_df\n</code></pre>"}]}